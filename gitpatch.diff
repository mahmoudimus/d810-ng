diff --git a/.gitignore b/.gitignore
index 473f1fd..c8cdfdb 100644
--- a/.gitignore
+++ b/.gitignore
@@ -1,3 +1,4 @@
+# Python
 **/__pycache__/
 **/.idea/
 **/.vscode/
@@ -6,13 +7,6 @@
 **.pyc
 **.log
 
-<<<<<<< Updated upstream
-samples/src/*.d
-samples/src/*.o
-samples/bins/*
-!samples/bins/**/*.dll
-!samples/bins/**/*.so
-=======
 # AI tools
 .cursorindexignore
 .cursorignore
@@ -53,5 +47,4 @@ src/d810/**/*.html
 src/d810/**/*.so.dSYM
 
 .env
-_indexable
->>>>>>> Stashed changes
+_indexable
\ No newline at end of file
diff --git a/docs/symbolic-microcode-plan.md b/docs/symbolic-microcode-plan.md
new file mode 100644
index 0000000..7f5e738
--- /dev/null
+++ b/docs/symbolic-microcode-plan.md
@@ -0,0 +1,169 @@
+## Symbolic microcode evaluator plan (Z3/Triton-AST), preserving Hex-Rays semantics
+
+This document proposes a symbolic variant of the existing MicroCodeInterpreter that builds and solves symbolic expressions for dispatcher evaluation. It retains Hex-Rays microcode semantics and API while enabling SMT solving of conditional/indirect branches and unknown reads.
+
+Rationale:
+- Avoid the native-vs-microcode semantic gap by staying within microcode opcodes and `mop_t` abstractions.
+- Leverage Z3 (preferred, already present in repo) or Triton ASTs to reason about unknowns and control-flow feasibility.
+
+Relevant code today:
+- `src/d810/expr/emulator.py`: concrete `MicroCodeInterpreter` and `MicroCodeEnvironment`.
+- `src/d810/optimizers/microcode/flow/flattening/generic.py`: dispatcher orchestration; calls `GenericDispatcherInfo.emulate_dispatcher_with_father_history()`.
+- `src/d810/hexrays/tracker.py`: `MopHistory` produces initial mop values; used to seed environment.
+- `src/d810/expr/z3_utils.py`: existing helpers for Z3 integration that we can reuse.
+
+
+## Design overview
+
+Two compatible evaluators:
+- Keep the current concrete `MicroCodeInterpreter` as-is (fast path).
+- Introduce `SymbolicMicroCodeInterpreter` with the same public surface (or nearly):
+  - `eval_instruction(blk, ins, env, ...) -> bool`
+  - `eval_mop(mop, env, ...) -> ValueLike | None`
+  - Internally, operations produce Z3 bit-vectors instead of Python ints when needed.
+
+Backend abstraction (optional but recommended for clarity):
+- Define a small adapter that implements primitive arithmetic and logical ops over either ints or Z3 bit-vectors. Example methods: `bv(size, int)`, `add`, `sub`, `mul`, `udiv`, `sdiv`, `and_`, `or_`, `xor`, `shl`, `lshr`, `ashr`, `eq`, `ult`, `ule`, `ugt`, `uge`, `slt`, `sle`, `sgt`, `sge`, `ite`.
+- Concrete backend maps to Python ints with masking; symbolic backend maps to Z3 `BitVecRef` with the same bit-width.
+
+Value representation:
+- Use bit-width-accurate values everywhere. Ensure masking with `AND_TABLE[size]` semantics is preserved in the symbolic domain via explicit `Extract/ZeroExt/SignExt` as needed.
+
+Environment model:
+- `MicroCodeEnvironment` keeps separate maps for `mop_r` and `mop_S`. For the symbolic interpreter, store `ValueLike` (int or Z3 BitVec) instead of ints.
+- For `mop_v` (absolute memory):
+  - If segment non-writable: concretize via IDA read (fast, precise constant).
+  - If writable or segment unknown: return fresh Z3 symbol (bit-vector) keyed by EA and size, cached per environment to stay consistent.
+
+Unknowns and symbols:
+- When a mop’s value is not found in the environment, create a fresh Z3 symbol (e.g., `sym_r_rax_8`, `sym_S_off_0x30_8`, `sym_mem_0x401000_4`).
+- For reads with helper-like semantics already special-cased in the concrete evaluator (`__readfsqword`, `__readgsqword`, PEB/TEB, etc.), either:
+  - direct concrete non-zero (mirrors current behavior), or
+  - symbolic value with constraint `!= 0` if that is more robust; both are acceptable.
+
+
+## Opcode coverage
+
+Mirror the existing coverage in `MicroCodeInterpreter._eval_instruction` and `eval`:
+- Unary: `m_neg`, `m_lnot`, `m_bnot`, `m_low`, `m_high`, `m_xds`, `m_xdu`.
+- Binary: `m_add`, `m_sub`, `m_mul`, `m_udiv`, `m_sdiv`, `m_umod`, `m_smod`, `m_or`, `m_and`, `m_xor`, `m_shl`, `m_shr`, `m_sar`.
+- Flag ops: `m_cfadd`, `m_ofadd`, `m_set*` (implement using Z3 helpers; reuse `get_add_cf`, `get_add_of`, `get_parity_flag` by re-implementing them over Z3 or writing Z3 equivalents—bit-precise definitions).
+- Loads/stores: `m_ldx`/`m_stx` symbolic handling as above; stack loads (`ss.2`) remain as environment lookups; other segments → RO concretize or symbolize.
+- Control flow:
+  - For conditional jumps, evaluate the predicate as a Z3 Bool and fork potential successors.
+  - For `m_jtbl`, evaluate key expression (symbolic) and compare against cases: build disjunctions of `(key == case_value)` then map to block targets.
+  - For `m_ijmp`, evaluate destination address expression; check equality against candidate target block entry EAs.
+
+
+## Branch feasibility and next-block selection
+
+Strategy:
+- When a branch is encountered inside the dispatcher region, construct solver queries to determine successor feasibility.
+- If exactly one in-region successor is feasible, take it.
+- If multiple are feasible, apply a deterministic policy (lowest serial), and optionally record the condition for cache.
+- If none feasible or solver times out, abort symbolic mode and propagate “unresolvable”.
+
+Solving infrastructure:
+- Use `src/d810/expr/z3_utils.py` helpers to manage contexts and timeouts.
+- Per-branch solve timeout: 50–100ms; overall per-dispatcher budget: 300–500ms (configurable).
+
+
+## Integration with the unflattener
+
+`GenericDispatcherInfo.emulate_dispatcher_with_father_history(...)`:
+1. Keep current concrete run with `MicroCodeInterpreter`.
+2. If that fails to exit the dispatcher region or meets unresolved evaluation:
+   - If `use_symbolic_microcode=true`, instantiate `SymbolicMicroCodeInterpreter` and re-run from entry.
+   - Use the same initial seeding from father history into the symbolic environment.
+   - Step within dispatcher region, deciding successors via Z3 feasibility.
+3. Return `(target_blk, instructions_executed)` with the same shape as today.
+
+Minimal API alignment:
+- `SymbolicMicroCodeInterpreter.eval_instruction(...)` should update an environment’s `next_blk` and `next_ins` akin to today (internally it will select the feasible successor). If both successors are feasible, it chooses deterministically and records a note (for logs or caching).
+
+
+## Caching and determinism
+
+Cache key:
+- `(entry_block_serial, compared_mop signature, initial_value_digest, maturity)`
+
+Cache content:
+- `(resolved_successor_serial, path_conditions_digest)`
+
+Determinism:
+- Always choose the lowest-serial feasible successor when multiple are possible; log all feasible ones for debugging.
+
+
+## Configuration
+
+Add optimizer options (e.g., in `conf/options.json` or relevant unflattening config):
+- `use_symbolic_microcode`: bool (default false)
+- `z3_timeout_ms`: int
+- `z3_total_budget_ms`: int
+- `symbolize_writable_reads`: bool (default true)
+- `concretize_readonly_reads`: bool (default true)
+
+
+## Error handling and fallbacks
+
+- If symbolic interpreter encounters an unsupported opcode, fall back to concrete interpreter for that step when safe; otherwise bail out of symbolic mode.
+- On timeout/unknown, abort symbolic mode and bubble up.
+
+
+## Deliverables and code edits
+
+New module(s):
+- `src/d810/expr/symbolic_interpreter.py`:
+  - `class SymbolicMicroCodeInterpreter:` mirrors `MicroCodeInterpreter` public API
+  - Small Z3 backend or reuse/adapt from `z3_utils.py`
+  - Helpers for symbolic flag computation
+
+Edits:
+- `src/d810/optimizers/microcode/flow/flattening/generic.py`:
+  - In `GenericDispatcherInfo.emulate_dispatcher_with_father_history`, add optional symbolic fallback.
+- `src/d810/conf/options.json` (or specific rule config) to include the new flags.
+
+
+## Testing strategy
+
+Unit tests (symbolic ops):
+- Arithmetic/bitwise ops produce correct Z3 AST widths and masks.
+- `m_high`/`m_low` and `xds/xdu` correctly transform bit-widths.
+- Flag computations (`cf`, `of`, parity) verified via small satisfiability checks.
+
+Branch tests:
+- Conditional branch with symbolic predicate: verify solver picks the feasible successor.
+- Jump table with few cases: verify feasibility selection and default handling.
+- Indirect jump: equality to candidate block addresses decides successor.
+
+System tests:
+- Reuse existing dispatcher fixtures; compare outcomes (target block + CFG rewrites) with and without the symbolic fallback on known-resolvable flows.
+- Add cases that require symbolic reasoning (writable memory load, unknown register) to validate improved robustness.
+
+Performance:
+- Enforce per-branch and per-dispatcher time budgets; ensure overall optimizer runtime stays within acceptable bounds.
+
+
+## Risks and mitigations
+
+- Incomplete opcode coverage: start with the subset exercised by dispatcher paths; log and add as needed.
+- Z3 path explosion: we only need the next successor, not full path exploration; solve locally at each branch with short timeouts.
+- Masking/width mistakes: centralize bit-width handling and add unit tests.
+
+
+## Rollout plan
+
+1. Land behind `use_symbolic_microcode=false` and run tests.
+2. Dogfood on representative binaries; grow opcode coverage guided by logs.
+3. Consider enabling by default if success rate and perf are good; otherwise keep as opt-in for tough samples.
+
+
+## Appendix: Implementation notes
+
+- Bit-widths:
+  - Use `size` (bytes) → `bits = size * 8`; ensure every operation returns correctly masked values (`Extract(bits-1, 0, expr)` as needed).
+- Memory symbols:
+  - Key by `(ea, size)` and cache in the environment so repeated loads from the same address are the same symbol.
+- Dispatcher boundaries:
+  - Only perform symbolic branching while `cur_blk.serial` belongs to the dispatcher internal set to avoid wandering beyond intended scope.
+
diff --git a/docs/triton-fallback-plan.md b/docs/triton-fallback-plan.md
new file mode 100644
index 0000000..e0ed656
--- /dev/null
+++ b/docs/triton-fallback-plan.md
@@ -0,0 +1,197 @@
+## Triton-backed dispatcher emulation: hybrid fast-path + symbolic fallback
+
+This document describes how to augment the current dispatcher evaluation in the control-flow unflattener with a Triton-powered symbolic fallback, while preserving the existing MicroCodeInterpreter fast path. The goal is to improve robustness on unknown inputs, indirect jumps, and jump-tables, without paying solver costs for common cases.
+
+References:
+
+- Triton repository: `https://github.com/JonathanSalwan/Triton`
+- Triton Python API docs: `https://triton-library.github.io/documentation/doxygen/py_triton_page.html`
+- Triton examples (Python): `https://github.com/JonathanSalwan/Triton/tree/master/src/examples/python`
+
+### Current architecture snapshot (integration points)
+
+- Dispatcher discovery and unflattening live under `src/d810/optimizers/microcode/flow/flattening/`.
+- Emulation entry point used by unflattener:
+  - `GenericDispatcherInfo.emulate_dispatcher_with_father_history()` in `generic.py` builds a `MicroCodeEnvironment`, seeds variables from `MopHistory`, and steps through blocks with `MicroCodeInterpreter.eval_instruction(...)` to find the first out-of-dispatcher block and collect side effects.
+  - Call site example:
+    - `GenericDispatcherUnflatteningRule.resolve_dispatcher_father()` calls `dispatcher_info.emulate_dispatcher_with_father_history(...)` and rewrites CFG accordingly.
+- The concrete evaluator is `d810.expr.emulator.MicroCodeInterpreter` with `MicroCodeEnvironment`.
+
+Key file references:
+
+- `src/d810/optimizers/microcode/flow/flattening/generic.py` (dispatcher logic)
+- `src/d810/hexrays/tracker.py` (`MopHistory`, provides initial values)
+- `src/d810/expr/emulator.py` (current concrete microcode interpreter)
+
+### Problem statement
+
+Concrete microcode evaluation can stall when:
+
+- Values for state variables or memory reads are unknown.
+- Control flow uses `m_jtbl`/`m_ijmp` or computed branches with data-dependent targets.
+- Helpers or FS/GS/PEB/TEB-derived values require modeling.
+
+We want a symbolic fallback to answer feasibility and compute the next dispatcher successor, without fully replacing the fast path.
+
+## Proposed approach
+
+Add a Triton-based fallback in `GenericDispatcherInfo.emulate_dispatcher_with_father_history()` that is invoked only when the concrete path becomes unknown or ambiguous.
+
+High-level flow:
+
+1. Attempt current concrete emulation via `MicroCodeInterpreter` (unchanged).
+2. If it returns unknown/halts before exiting dispatcher region, invoke Triton fallback.
+3. Triton executes native instructions from the current block head EA, with a seeded state derived from `MopHistory`, until the execution exits the dispatcher region. On conditional/indirect branches, solver queries determine feasible successors.
+4. Return the same outputs as today: the first out-of-dispatcher `mblock_t` plus any non-CF side effects to be copied.
+
+### Dispatcher region identification
+
+Use the existing `GenericDispatcherInfo` fields to constrain the Triton run:
+
+- `entry_block` identifies the entry `mblock_t` and its `use_before_def_list`.
+- `dispatcher_internal_blocks` and `dispatcher_exit_blocks` delimit the region. Continue stepping while current block serial is in the internal set and stop once not in it.
+
+### Triton environment construction
+
+Triton context configuration (example for x86_64):
+
+- `ARCH.X86_64`
+- Modes: `ALIGNED_MEMORY = True`, others as needed
+- Solver timeout: 50–100ms per query (configurable)
+
+State seeding from father history:
+
+- For each mop in `entry_block.use_before_def_list`, obtain value from `father_history.get_mop_constant_value(mop)`.
+  - If known: set concrete Triton register or memory value.
+  - If unknown: symbolize the register or memory cell.
+
+Register mapping:
+
+- Map IDA/Hex-Rays register names to Triton registers (e.g., `rax`, `rbx`, `rcx`, `rdx`, `rsi`, `rdi`, `rsp`, `rbp`, XMM/YMM if needed). Provide a small adapter table.
+
+Stack and memory model:
+
+- Choose a synthetic stack base (e.g., `0x7fff00000000`) for stack variables (`mop_S`) and model `off` as an address offset from that base. Size according to mop size.
+- For `mop_v` and other absolute address reads:
+  - If the target segment is non-writable, concretize from IDA (`idaapi.get_qword/get_dword/...`).
+  - If writable or not found: symbolize the memory cell.
+
+Helpers and special reads:
+
+- For helpers like `__readfsqword`, `__readgsqword`, and known PEB/TEB accessors, inject stable non-zero concrete values (consistent with today’s `_get_synthetic_call_ret`).
+- Optionally for unknown helpers, stub with a fresh symbol constrained to be non-zero.
+
+### Stepping and control-flow resolution
+
+Instruction fetch/decoding:
+
+- Fetch bytes at current EA from IDA, decode with IDA to get size and next EA; feed bytes to Triton `Instruction` and call `ctx.processing(inst)`.
+- Maintain `EA → block_serial` mapping via `get_block_serials_by_address(mba, ea)` when needed.
+
+Branches:
+
+- For conditional branches, retrieve the branch condition AST and query feasibility for both taken/not-taken. Pick the successor block that remains within the dispatcher region if unique; if multiple feasible successors remain inside, pick a deterministic one (e.g., lowest serial) and log.
+- For `jtbl`/indirect jumps: obtain the computed target AST; for each candidate case target block EA, assert equality and query; select a feasible target in the region. If default or multiple, use a deterministic tie-breaker.
+
+Exit condition:
+
+- Stop when the next block serial is not in the internal dispatcher set; return that `mblock_t`.
+- Collect non-control-flow side-effect instructions during traversal for later copying (mirrors current behavior).
+
+### Caching and performance
+
+Cache key:
+
+- `(entry_block_entry_ea, dispatcher_id, compared_mop signature, initial_value_digest)`
+
+Cache value:
+
+- `(out_block_serial, side_effects_fingerprint)` or a small structure with resolved branch sequence.
+
+Timeouts and limits:
+
+- Per-solve timeout: 50–100ms; per-dispatcher budget: 300–500ms (configurable).
+- If exceeded, abort fallback and bubble up “unresolvable” so the outer logic can keep current behavior.
+
+### Configuration and flags
+
+- Add config knobs (with sensible defaults) to `conf/options.json` or the relevant optimizer config:
+  - `use_triton_fallback`: true/false
+  - `triton_timeout_ms`: int
+  - `triton_max_total_ms`: int
+  - `triton_concretize_readonly`: bool
+  - `triton_symbolize_writable`: bool
+
+Import guard:
+
+- Attempt to import Triton at runtime; if unavailable, keep current behavior silently.
+
+### Error handling
+
+- If decode fails at an EA, try stepping to block tail and transition to successor by IDA CFG; otherwise abort fallback.
+- If solver returns unknown/timeout for all options, abort fallback.
+- Always keep concrete fast path as the first attempt; never regress current success cases.
+
+## Deliverables and code edits
+
+1. New module: `src/d810/optimizers/microcode/flow/flattening/triton_fallback.py`
+   - `build_triton_context(arch_config) -> TritonContext`
+   - `seed_state_from_mops(ctx, mba, mops, father_history) -> SeedReport`
+   - `step_until_exit(ctx, mba, entry_blk, internal_serials, exit_serials, helpers_policy) -> (target_blk, side_effects)`
+   - `dispatch_with_triton(...)` convenience function used by `GenericDispatcherInfo`.
+
+2. Edit `GenericDispatcherInfo.emulate_dispatcher_with_father_history(...)`:
+   - Wrap existing concrete run.
+   - On failure/unknown inside dispatcher, call the fallback when `use_triton_fallback` and Triton available.
+   - Return consistent outputs.
+
+3. Config: extend relevant config file(s) with flags/timeouts and thread through `GenericDispatcherUnflatteningRule.configure`.
+
+4. Telemetry/logging: emit timing and solver stats under `unflat_logger` at info/debug.
+
+## Testing strategy
+
+Unit tests:
+
+- Seed-only tests for `seed_state_from_mops` mapping register and stack variables (including unknowns → symbols).
+- Branch feasibility unit tests with small synthetic blocks: conditional, `jtbl`, and `ijmp`.
+
+System tests:
+
+- Reuse existing dispatcher fixtures; run with fallback disabled (baseline) vs enabled and assert identical target block selections on known solvable cases.
+- Add cases where the concrete path previously failed due to unknowns (e.g., writable memory read); verify fallback resolves to a stable out-of-dispatcher block and unflattening proceeds.
+
+Performance:
+
+- Record dispatcher-resolution latency and cache hit rate. Ensure typical budgets stay under configured thresholds.
+
+## Risks and mitigations
+
+- Semantic gap native vs microcode: we drive by native instructions while the rest of pipeline uses microcode. Mitigate by only using Triton to decide the next block; do not mutate microcode based on Triton state.
+- Over-constraining vs under-constraining: prefer symbolization for writable/unknown reads. Concretize only RO segments.
+- Solver churn: use timeouts and caching; short-circuit when only one in-region successor is feasible.
+
+## Rollout plan
+
+1. Land behind `use_triton_fallback=false` by default; CI green.
+2. Enable in developer testing, collect logs, tune timeouts.
+3. Enable by default once stability and perf are confirmed.
+
+## Appendix: Pseudocode sketch
+
+```python
+def emulate_dispatcher_with_father_history(...):
+    # 1) Try fast path
+    if concrete_run_succeeds:
+        return target_blk, side_effects
+
+    # 2) Optional fallback
+    if not config.use_triton_fallback or not triton_available:
+        return cur_blk, executed  # existing behavior
+
+    # 3) Triton path
+    ctx = build_triton_context()
+    seed_state_from_mops(ctx, mba, entry_block.use_before_def_list, father_history)
+    target_blk, side_effects = step_until_exit(ctx, mba, entry_block, internal_blocks, exit_blocks)
+    return target_blk, side_effects
+```
diff --git a/docs/triton-hybrid.md b/docs/triton-hybrid.md
new file mode 100644
index 0000000..16a3d41
--- /dev/null
+++ b/docs/triton-hybrid.md
@@ -0,0 +1,488 @@
+## Triton Hybrid Integration Plan (Selective Symbolic Oracle)
+
+This document describes how to integrate Triton as an optional, on‑demand symbolic oracle alongside the existing `MicroCodeInterpreter`. The goal is to keep the current fast path intact, and query Triton only when we hit unknowns that matter (branches, indirect jumps, or key memory loads), avoiding fabricated values like synthetic call returns.
+
+### Why integrate a Triton hybrid
+
+- • Avoid wrong concretization: Your microcode emulator sometimes must pick a number (0 or a sentinel) for unknowns (e.g., call results, `%var_*` bases). That cascades into bad states like `MEMORY[0]` and misfolded branches.
+- • Keep unknowns symbolic: Triton models call results and FS/GS/PEB reads as symbolic with constraints (e.g., “!= 0”), so we don’t fabricate addresses or segments.
+- • Prove only when safe: Use SMT to prove/negate conditions. If undecidable quickly, leave unknown. This preserves correctness without sacrificing performance.
+
+### Where it’s helpful
+
+- • PE header probe branch
+  - Example: `jnz xdu.4([ds:%var_658].2), #0x5A4D`
+  - Current issue: `%var_658` is unknown → risky to fold; sometimes collapses to `$0x0`.
+  - With Triton: treat `[ds:%var_658]` as a symbolic load; ask “is it always 0x5A4D?” If provable, fold; if refutable, invert; else leave unknown. No fake zeros.
+
+- • PEB/TEB-based derefs
+  - Example: `ldx ds, (NtCurrentPeb()+0x18), %var_...`
+  - Current issue: forcing 0/None leads to `MEMORY[0]` or “no segment”.
+  - With Triton: `NtCurrentPeb()` is a non-zero symbolic pointer; deref stays symbolic unless needed for a proof. Avoids null/segment artifacts.
+
+- • Dispatcher next-target resolution (flattening)
+  - Example: indirect jump via `state = tbl[(state ^ key) & mask]`.
+  - With Triton: enumerate a small, feasible set of ijmp targets under constraints; feed them to CFG expansion to recover real successors.
+
+- • Guarded constant folds with unknown inputs
+  - Example: `((sym & 0xFF00) == 0x4D00)` for MZ/PE checks.
+  - With Triton: prove bitmask relations directly; fold only when the solver guarantees it.
+
+- • Eliminating MEMORY[0] regressions
+  - Null derefs and synthetic call addresses aren’t manufactured; symbolic memory + constraints prevents spurious `[0x0]` reads and keeps conditions correct.
+
+- • Lightweight, on-demand oracle
+  - Triton runs only on small “micro-slices” at the moment you need a decision (branch fold, target set, or specific load), with tight timeouts and fast fallback to your emulator.
+
+In short: the hybrid keeps your emulator fast and deterministic for the easy cases, while using Triton to safely reason about the hard unknowns that currently force incorrect constants or missed optimizations.
+
+### What we keep vs. add
+
+- Keep: `MicroCodeInterpreter` concrete fast path and all existing passes.
+- Add: a tiny `SymbolicOracle` wrapper (Triton-backed) that can:
+  - prove/negate a branch condition,
+  - evaluate a memory load when the base is symbolic,
+  - enumerate a small set of targets for an indirect jump/state variable.
+
+### Design principles
+
+- Optional and safe: if Triton is missing or times out, fall back to current behavior.
+- Narrow scope: map only micro-ops needed for dispatcher reasoning (mov, add/sub, and/or/xor, shifts, zero/sign extend, `ldx`, and compare). Calls become symbolic values, not numbers.
+- No synthetic constants: calls and FS/GS/PEB helpers are modeled as symbolic with non-zero constraints. We do not fabricate segment-backed addresses.
+- Tight limits: per-query solver timeout (50–100 ms), step cap (~100), and path cap (~64). Abort quickly if not decisive.
+
+---
+
+## Files to add
+
+- `src/d810/expr/sym_oracle_triton.py` — Triton wrapper (optional import, minimal API)
+- (Optional) `src/d810/expr/sym_ir.py` — small adapters to convert a micro-slice to Triton ops
+
+No changes are required in other modules to land the scaffolding. Hooks in the emulator and unflattener are guarded and off by default.
+
+---
+
+## Public API of the oracle
+
+```python
+class SymbolicOracle:
+    def __init__(self, arch: str = "auto", timeout_ms: int = 80,
+                 step_cap: int = 100, path_cap: int = 64): ...
+
+    def is_enabled(self) -> bool:
+        """Return True if Triton is available and the context is initialized."""
+
+    def prove(self, micro_slice, condition) -> tuple[bool|None, dict]:
+        """Return (True, model) if condition is valid; (False, model) if unsat; (None, {}) if unknown.
+        `micro_slice` is a minimal list of defining micro-ops for operands in `condition`."""
+
+    def eval_load(self, micro_slice, addr_expr, size: int) -> int|None:
+        """Attempt to concretize a memory load at `addr_expr` under current constraints.
+        Return int on singleton model; else None."""
+
+    def enumerate_ijmp_targets(self, micro_slice, max_targets: int = 8) -> list[int]|None:
+        """Enumerate a small set of feasible indirect jump targets; None if not decisive."""
+```
+
+---
+
+## Example implementation skeleton
+
+```python
+# src/d810/expr/sym_oracle_triton.py
+from __future__ import annotations
+
+try:
+    from triton import TritonContext, ARCH, Instruction, AST_REPRESENTATION
+    from triton import MemoryAccess, CPUSIZE
+except Exception:  # Triton not available
+    TritonContext = None
+
+
+class SymbolicOracle:
+    def __init__(self, arch: str = "auto", timeout_ms: int = 80,
+                 step_cap: int = 100, path_cap: int = 64):
+        self.timeout_ms = timeout_ms
+        self.step_cap = step_cap
+        self.path_cap = path_cap
+        self.ctx = None
+        if TritonContext is None:
+            return
+        self.ctx = TritonContext()
+        # Choose architecture dynamically or from config
+        if arch == "x64":
+            self.ctx.setArchitecture(ARCH.X86_64)
+        elif arch == "x86":
+            self.ctx.setArchitecture(ARCH.X86)
+        elif arch == "aarch64":
+            self.ctx.setArchitecture(ARCH.AARCH64)
+        else:
+            # Heuristic or config-driven selection could go here
+            self.ctx.setArchitecture(ARCH.X86_64)
+        self.ctx.setAstRepresentation(AST_REPRESENTATION.PYTHON)
+
+    def is_enabled(self) -> bool:
+        return self.ctx is not None
+
+    # --- helpers ---
+    def _fresh_symbolic(self, name: str, size_bits: int, non_zero: bool = False):
+        var = self.ctx.newSymbolicVariable(size_bits, name)
+        sym = self.ctx.getAstContext().variable(var)
+        if non_zero:
+            self.ctx.pushPathConstraint(sym != 0)
+        return sym
+
+    def _build_slice(self, micro_slice):
+        """Translate a minimal subset of micro-ops into Triton AST.
+        Intentionally partial: mov, add/sub, and/or/xor, shifts, zext/sext, ldx (symbolic), cmps.
+        Calls: treated as fresh symbolic with optional non-zero constraints for FS/GS/PEB.
+        """
+        # Implementation intentionally left minimal for the first iteration.
+        # Map only the operations required by the dispatcher condition/targets.
+        pass
+
+    # --- API ---
+    def prove(self, micro_slice, condition):
+        if not self.is_enabled():
+            return (None, {})
+        try:
+            self._build_slice(micro_slice)
+            cond_ast = condition.to_triton_ast(self.ctx)  # adapter method you provide
+            # Check validity: cond is valid iff not (¬cond) is satisfiable
+            is_invalid = self.ctx.isSat(~cond_ast)
+            if not is_invalid:  # ¬cond unsat => cond valid
+                return (True, {})
+            # Check refutation: cond is unsat?
+            if not self.ctx.isSat(cond_ast):
+                return (False, {})
+            return (None, {})
+        except Exception:
+            return (None, {})
+
+    def eval_load(self, micro_slice, addr_expr, size: int):
+        if not self.is_enabled():
+            return None
+        try:
+            self._build_slice(micro_slice)
+            addr_ast = addr_expr.to_triton_ast(self.ctx)
+            # Try to model a singleton value: ask solver for a model, re-check uniqueness if needed
+            if not self.ctx.isSat(addr_ast == addr_ast):  # trivial satisfiable check
+                return None
+            model = self.ctx.getModel(addr_ast)
+            if not model:
+                return None
+            # For first iteration, only handle fully concrete address
+            addr_val = addr_ast.evaluate()
+            if addr_val is None:
+                return None
+            # We do not read real process memory; only constant tables you pre-seed can be read.
+            return None
+        except Exception:
+            return None
+
+    def enumerate_ijmp_targets(self, micro_slice, max_targets: int = 8):
+        if not self.is_enabled():
+            return None
+        try:
+            self._build_slice(micro_slice)
+            # Build target expr and enumerate up to max_targets models (skipped in skeleton)
+            return None
+        except Exception:
+            return None
+```
+
+Notes:
+
+- The above skeleton intentionally defers the micro-slice to Triton mapping. Start with the few ops used by your dispatcher and grow as needed.
+- For helpers like `__readfsqword`/`__readgsqword`/`NtCurrentPeb`, create a fresh symbolic pointer with a `!= 0` constraint. Do not fabricate numbers.
+
+---
+
+## Emulator hooks (guarded, optional)
+
+Add a nullable `oracle` field to `MicroCodeInterpreter` and delegate only when unknowns matter:
+
+```python
+# in __init__
+self.oracle = None  # type: Optional[SymbolicOracle]
+
+# on evaluating call (m_call/m_icall)
+if self.oracle and self.oracle.is_enabled():
+    # return a placeholder object representing a symbolic value in your env,
+    # instead of a fabricated integer
+    return environment.define_symbolic_call_result(ins, non_zero_for_helpers=True)
+
+# on evaluating load (mop_d) when base is unknown/symbolic
+if self.oracle and self.oracle.is_enabled() and self.symbolic_mode:
+    oracle_val = self.oracle.eval_load(micro_slice, addr_expr, size)
+    if oracle_val is not None:
+        return oracle_val & AND_TABLE[size]
+    return None  # keep unknown
+
+# on folding branch conditions
+if self.oracle and self.oracle.is_enabled():
+    verdict, _ = self.oracle.prove(micro_slice, cond_expr)
+    if verdict is True:
+        return True  # taken
+    if verdict is False:
+        return False  # not taken
+    # else keep unknown
+```
+
+These hooks are no-ops unless `oracle` is assigned and enabled.
+
+---
+
+## Using the oracle in the unflattener
+
+- In `tracker.py` / `optimizers/microcode/flow/flattening/generic.py`, when the dispatcher’s state variable or an indirect jump target set is unknown:
+
+```python
+if interpreter.oracle and interpreter.oracle.is_enabled():
+    targets = interpreter.oracle.enumerate_ijmp_targets(micro_slice, max_targets=8)
+    if targets:
+        # feed targets into the CFG expansion logic
+        ...
+```
+
+---
+
+## Configuration and defaults
+
+- Add a config option (e.g., `conf/options.json`):
+  - `symbolic_oracle.enabled: false`
+  - `symbolic_oracle.timeout_ms: 80`
+  - `symbolic_oracle.step_cap: 100`
+  - `symbolic_oracle.path_cap: 64`
+
+When disabled or Triton is unavailable, behavior remains identical to today.
+
+---
+
+## Installation notes (optional)
+
+- Triton often requires system deps (Capstone, Z3). On macOS:
+  - `brew install capstone z3`
+  - `pip install triton-library`
+- Keep this optional. Ship with try/except guards so the package works without Triton.
+
+---
+
+## Milestones
+
+1) Scaffolding (this doc + `sym_oracle_triton.py` skeleton)
+2) Minimal mapping: mov/add/sub/xor/and/shifts, zero/sign-extend, compares; calls as symbolic; FS/GS/PEB non-zero.
+3) Hook branch folding: `prove` integration (timeouts enforced).
+4) Hook indirect jump: `enumerate_ijmp_targets` with small bound.
+5) Caching: memoize per-EA micro-slice results.
+
+Each milestone yields value and can be merged independently.
+
+## Extra Consideration
+
+Triton is a better fit for this use-case than angr.
+
+- When Triton is better
+  - Lightweight, embeddable “oracle” for small micro-slices.
+  - Easy to introduce symbolic vars for calls and prove/negate branch conditions.
+  - Low overhead, keeps your Hex-Rays microcode context and mappings intact.
+
+- When angr is better
+  - Whole-function/path exploration, richer OS/library models (SimProcedures), full calling convention handling.
+  - You want to symbolically execute from bytes with its own IR and memory model, not inline with Hex-Rays.
+
+- Integration cost
+  - Triton: map a small subset of micro-ops; add non-zero constraints for FS/GS/PEB helpers; query with tight timeouts.
+  - angr: heavy; requires lifting to VEX or feeding raw bytes, syncing memory/state with IDA, and you lose the neat 1:1 link to Hex-Rays microcode.
+
+Recommendation: keep your fast MicroCodeInterpreter and add a selective Triton-backed oracle for unknowns at branches/ijmp/critical loads. Consider angr only if you later need broader path exploration with complex library/OS semantics.
+
+---
+
+## Extra Notes on Architecture and Implementation
+
+### How the Emulator Cascades Through Both Passes
+
+This section documents the actual architecture discovered through code analysis:
+
+#### Pass 1: `generic.py` (Dispatcher Emulation) — Direct Emulator Use
+
+```
+GenericDispatcherInfo.emulate_dispatcher_with_father_history()
+    ↓
+Creates: MicroCodeInterpreter(symbolic_mode=True)
+         MicroCodeEnvironment()
+    ↓
+Iterates through dispatcher blocks:
+    MicroCodeInterpreter.eval_instruction(blk, ins, environment)
+    ↓ Executes each instruction, updates environment
+    ↓ When hit unknown → synthetic value or fails
+```
+
+**Key insight:** This pass uses `MicroCodeInterpreter` **directly** with `symbolic_mode=True`, allowing it to continue emulation even with unknowns (by fabricating synthetic values). Triton oracle here provides **proven values instead of guesses**.
+
+#### Pass 2: `fix_pred_cond_jump_block.py` (Branch Prediction) — Indirect Emulator Use via Backward Tracing
+
+```
+sort_predecessors(blk)
+    ↓
+MopTracker.search_backward(pred_blk, pred_blk.tail)
+    ↓ Traces backward, builds list of MopHistory objects (one per path)
+    ↓
+get_all_possibles_values(mop_histories, searched_mop_list)
+    ↓ For each history, calls: mop_history.get_mop_constant_value(searched_mop)
+    ↓
+MopHistory._execute_microcode()
+    ↓ Line 113: Creates MicroCodeInterpreter(symbolic_mode=False)
+    ↓ Lines 190-201: Iterates through all instructions on that path
+    ↓ Each instruction: microcode_interpreter.eval_instruction(blk, ins, environment)
+    ↓ Result: Returns None if any instruction can't be evaluated
+```
+
+**Key insight:** This pass uses `MicroCodeInterpreter` **indirectly through `MopHistory`** with `symbolic_mode=False`, meaning **any unknown stops the backward trace** (returns `None`). Triton oracle here provides **fallback values that prevent trace failure**.
+
+### The Critical Difference: symbolic_mode
+
+| Context | Mode | Behavior | Triton Benefit |
+|---------|------|----------|---|
+| **generic.py** | `symbolic_mode=True` | Hits unknown → uses synthetic value, continues | Oracle avoids synthetic by proving real value |
+| **MopHistory** | `symbolic_mode=False` | Hits unknown → returns None, trace fails | Oracle provides concrete value so trace succeeds |
+
+### Implementation Considerations
+
+#### For `generic.py` (Immediate Priority)
+
+Add oracle to `MicroCodeInterpreter` as documented in the triton-hybrid.md skeleton. The `symbolic_mode=True` path already handles unknowns gracefully (just replaces them with synthetics). Triton improves this by:
+
+1. **Proving branch conditions:** Instead of taking both paths symbolically, Triton can prove only one is feasible
+2. **Modeling system helpers:** `__readfsqword`, `__readgsqword`, `NtCurrentPeb` become symbolic with `!= 0` constraint (never fabricates 0)
+3. **Resolving dispatcher state:** When initial state is unknown, Triton can symbolically execute dispatcher to collect feasible exit blocks
+
+#### For `fix_pred_cond_jump_block.py` (Secondary, Optional Enhancement)
+
+To leverage Triton in this pass, you have two options:
+
+**Option A: Enable oracle fallback in `MopHistory` (Recommended)**
+
+```python
+# In tracker.py, line 113:
+self._mc_interpreter = MicroCodeInterpreter(
+    symbolic_mode=False,
+    oracle=triton_oracle  # NEW
+)
+
+# In emulator.py eval() method, add:
+if value is None and self.oracle and self.oracle.is_enabled():
+    # Try oracle as last resort before raising
+    value = self.oracle.query_register_value(mop)
+```
+
+**Benefit:** `MopHistory` can complete backward traces even with unknowns, improving branch prediction confidence.
+
+**Risk:** Minimal — oracle is only queried on unknowns, so no regression.
+
+**Option B: Use `symbolic_mode=True` in `MopHistory` (Advanced)**
+
+```python
+# In tracker.py, line 113:
+self._mc_interpreter = MicroCodeInterpreter(
+    symbolic_mode=True,  # NEW
+    oracle=triton_oracle  # NEW
+)
+```
+
+**Benefit:** Backward traces naturally handle unknowns via Z3 bitvectors.
+
+**Risk:** Code expecting concrete `int` values might break; requires careful value handling.
+
+**Recommendation:** Start with Option A (minimal change). If you find that backward traces still fail frequently, revisit Option B.
+
+### Phase-Based Rollout Strategy
+
+1. **Phase 1 (Unflattening):** Triton in `generic.py` only
+   - Adds oracle to `MicroCodeInterpreter.__init__()` with `oracle=None` default
+   - Improves dispatcher emulation for cases with unknowns
+   - `fix_pred_cond_jump_block.py` behavior unchanged (still uses `MopHistory` without oracle)
+
+2. **Phase 2 (Branch Prediction, Optional):** Triton fallback in `MopHistory`
+   - Add oracle field to `MopHistory`
+   - In `eval()` method, query oracle as fallback on unknowns
+   - Improves backward traces that currently fail at unknowns
+
+3. **Phase 3 (Optimization, Optional):** Full symbolic mode in `MopHistory`
+   - Switch `MopHistory` to `symbolic_mode=True` if Phase 2 shows diminishing returns
+   - Requires testing and validation
+
+### Key Data Flow Paths for Triton
+
+**Dispatcher Emulation (forward execution):**
+```
+Unknown register/memory
+  ↓ (symbolic_mode=True)
+  ↓ MicroCodeInterpreter.eval()
+  ↓ (no value in environment)
+  ↓ [NEW] Query oracle.prove_value(mop) if enabled
+  ↓ If oracle returns value: use it + cache in environment
+  ↓ Else: use synthetic (current behavior)
+```
+
+**Backward Tracing (backward path analysis):**
+```
+MopHistory._execute_microcode() hits unknown
+  ↓ (symbolic_mode=False)
+  ↓ MicroCodeInterpreter.eval_instruction() returns False
+  ↓ _execute_microcode() returns False
+  ↓ get_mop_constant_value() returns None
+  ↓ [NEW, Phase 2] Query oracle.get_concrete_value(mop) in MopHistory
+  ↓ If oracle returns value: use it + cache in environment
+  ↓ Else: still None (current behavior)
+```
+
+### Testing Strategy
+
+**Unit tests to add:**
+
+1. **Oracle availability:** Verify graceful degradation when Triton unavailable
+2. **Dispatcher emulation:** Validate that dispatcher with symbolic inputs correctly identifies exit blocks
+3. **Backward tracing:** Verify that MopHistory can complete paths with oracle fallback
+4. **Branch prediction:** Confirm that fix_pred_cond_jump_block improves with oracle (Phase 2+)
+
+**Integration tests:**
+
+1. Real binaries with obfuscated dispatchers
+2. Binaries with unknown call results in dispatcher paths
+3. Binaries with FS/GS/PEB-based guards in branch conditions
+4. Regression testing: ensure Phase 1 doesn't break existing passes
+
+### Performance Considerations
+
+**Oracle query costs:**
+
+- Per-branch solve timeout: 50–100ms (configurable)
+- Typical benefit: 1–10 queries per dispatcher (for branch proofs)
+- Expected overhead: <500ms per dispatcher emulation
+- Expected benefit: 2–5× more dispatchers successfully unflattened
+
+**Mitigation strategies:**
+
+- Cache results by dispatcher signature
+- Skip oracle on fast paths (concrete-only evaluation)
+- Use tight timeouts and step caps as documented
+- Monitor in logs; adjust timeouts if needed
+
+### Debugging and Logging
+
+Add logging at oracle query points:
+
+```python
+# In MicroCodeInterpreter.eval()
+if self.oracle and self.oracle.is_enabled():
+    logger.debug("Querying oracle for %s (unknown in env)", format_mop_t(mop))
+    value = self.oracle.query_value(...)
+    if value is not None:
+        logger.info("Oracle proved value for %s: %x", format_mop_t(mop), value)
+    else:
+        logger.debug("Oracle inconclusive for %s", format_mop_t(mop))
+```
+
+This makes it easy to identify which unknowns Triton helps with and which still need heuristics.
diff --git a/include/pyptr.h b/include/pyptr.h
new file mode 100644
index 0000000..34ede4a
--- /dev/null
+++ b/include/pyptr.h
@@ -0,0 +1,287 @@
+/**
+ * @file pyptr.h
+ * @author D810
+ * @brief A smart pointer for Python objects, using Python's reference counting
+ * @version 0.1
+ * @date 2025-08-11
+ *
+ * @copyright Copyright (c) 2025
+ *
+ * =============================================================================
+ *  D810::pyptr<T> - A C++ smart pointer for PyObject* and friends
+ * =============================================================================
+ *
+ *  Motivation:
+ *  -----------
+ *  - When writing C++ code that interoperates with Python (e.g., via the Python C API,
+ *    Cython, or SWIG), you must manually manage reference counts for PyObject*.
+ *  - Forgetting to INCREF/DECREF leads to leaks or crashes.
+ *  - std::shared_ptr cannot be used directly, because PyObject* has its own refcounting.
+ *
+ *  What is pyptr<T>?
+ *  -----------------
+ *  - A minimal, header-only smart pointer that wraps a PyObject* (or any Python C API object)
+ *    and manages its reference count using Py_XINCREF/Py_XDECREF.
+ *  - Semantics are similar to std::shared_ptr<T>, but the "sharedness" is always correct
+ *    because Python itself tracks the refcount.
+ *  - Copying a pyptr<T> increments the refcount; destruction decrements it.
+ *  - Move semantics are supported (move leaves the source null).
+ *  - Null pointers are handled safely (Py_XINCREF/DECREF are null-safe).
+ *
+ *  Why not use std::shared_ptr<PyObject>?
+ *  --------------------------------------
+ *  - std::shared_ptr would double-manage the lifetime, which is wrong for PyObject*.
+ *  - pyptr<T> is a "shim" that delegates all ownership to Python's refcounting.
+ *  - This avoids double-free, and is always correct even if the object is also owned by Python.
+ *
+ *  Usability:
+ *  ----------
+ *  - Use pyptr<PyObject> (or pyptr<PyListObject>, etc) as a drop-in replacement for raw PyObject*.
+ *  - You can pass pyptr<T> to any C API expecting T* (implicit conversion).
+ *  - Use .get() to get the raw pointer, or operator->/operator* for pointer-like access.
+ *  - Use .reset(), .swap(), assignment, etc, as with std::shared_ptr.
+ *  - Use .use_count() to get the current Python refcount (for debugging).
+ *  - Use .unique() to check if this is the only reference (rarely useful in Python, but provided).
+ *  - Use new_shared(f, args...) to construct a pyptr from a factory function returning a T*.
+ *
+ *  Example:
+ *  --------
+ *      pyptr<PyObject> obj(PyList_New(0)); // refcount=1
+ *      {
+ *          pyptr<PyObject> obj2 = obj;     // refcount=2
+ *      }                                   // obj2 destroyed, refcount=1
+ *      obj.reset();                        // refcount=0, Py_DECREF called
+ *
+ *  Caveats:
+ *  --------
+ *  - pyptr<T> does NOT manage Python's GIL. You must ensure the GIL is held when manipulating Python objects.
+ *  - pyptr<T> is not thread-safe (but neither is Python's refcounting).
+ *  - Do not use pyptr<T> for objects not managed by Python's refcounting.
+ *  - pyptr<T> is not polymorphic: pyptr<PyObject> and pyptr<PyListObject> are unrelated types.
+ *
+ *  Advanced:
+ *  ---------
+ *  - The new_shared() helper allows you to wrap factory functions (e.g., PyList_New) in a type-safe way.
+ *  - Comparison operators and swap are provided for convenience.
+ *  - object_ptr is an alias for pyptr<PyObject>.
+ *
+ *  This header is self-contained and has no dependencies except Python.h and <utility>.
+ */
+
+#ifndef _D810_PYPTR_H
+#define _D810_PYPTR_H
+
+#include <Python.h>
+#include <utility>
+
+namespace D810
+{
+
+    /**
+     * @brief Smart pointer for Python objects (PyObject* or similar).
+     *
+     * pyptr<T> manages the reference count of a Python object.
+     * Copying increments the refcount, destruction decrements it.
+     * Move semantics are supported.
+     *
+     * @tparam T The Python C API type (usually PyObject or a subclass).
+     */
+    template <typename T>
+    struct pyptr
+    {
+        using element_type = T;
+
+    public:
+        /// Default constructor: null pointer.
+        pyptr() noexcept : p_(nullptr)
+        {
+        }
+
+        /// Construct from nullptr.
+        pyptr(std::nullptr_t) noexcept : pyptr()
+        {
+        }
+
+        /// Construct from a raw pointer, takes ownership (INCREF).
+        explicit pyptr(element_type *p) : p_(p)
+        {
+            Py_XINCREF(p_);
+        }
+
+        /// Move constructor: transfers ownership, source is set to null.
+        pyptr(pyptr &&r) noexcept : p_(r.p_)
+        {
+            r.p_ = nullptr;
+        }
+
+        /// Copy constructor: increments refcount.
+        pyptr(pyptr const &r) noexcept : p_(r.p_)
+        {
+            Py_XINCREF(p_);
+        }
+
+        /// Destructor: decrements refcount.
+        ~pyptr() noexcept
+        {
+            Py_XDECREF(p_);
+        }
+
+        /// Copy/move assignment: copy-and-swap idiom.
+        pyptr &operator=(pyptr r) noexcept
+        {
+            swap(r);
+            return *this;
+        }
+
+        /// Reset to null (decrements refcount if not null).
+        void reset() noexcept
+        {
+            pyptr().swap(*this);
+        }
+
+        /// Reset to a new pointer (decrements old, increments new).
+        void reset(element_type *p)
+        {
+            pyptr(p).swap(*this);
+        }
+
+        /// Swap with another pyptr.
+        void swap(pyptr &r) noexcept
+        {
+            std::swap(p_, r.p_);
+        }
+
+        /// Implicit conversion to raw pointer.
+        operator element_type *() const noexcept
+        {
+            return get();
+        }
+
+        /// Get the raw pointer.
+        element_type *get() const noexcept
+        {
+            return p_;
+        }
+
+        /// Dereference operator.
+        element_type &operator*() const noexcept
+        {
+            return *get();
+        }
+
+        /// Member access operator.
+        element_type *operator->() const noexcept
+        {
+            return get();
+        }
+
+        /// Get the current Python refcount (for debugging).
+        Py_ssize_t use_count() const noexcept
+        {
+            return Py_REFCNT(p_);
+        }
+
+        /// True if this is the only reference (rarely useful).
+        bool unique() const noexcept
+        {
+            return use_count() == 1;
+        }
+
+        /// Boolean conversion: true if not null.
+        explicit operator bool() const noexcept
+        {
+            return get() != nullptr;
+        }
+
+    private:
+        element_type *p_; ///< The managed pointer.
+    };
+
+    namespace detail
+    {
+        /**
+         * @brief Helper to deduce the return type of new_shared().
+         *
+         * Given a function F and arguments Args..., deduces the type returned by F(Args...),
+         * removes pointer, and wraps in pyptr<>.
+         */
+        template <typename F, typename... Args>
+        struct shared_result_of
+        {
+            using type = pyptr<
+                typename std::remove_pointer<
+                    typename std::result_of<F(Args...)>::type>::type>;
+        };
+
+    }
+
+    /**
+     * @brief Helper to construct a pyptr from a factory function.
+     *
+     * Example: auto p = new_shared(PyList_New, 10);
+     *
+     * @tparam F Factory function type.
+     * @tparam Args Argument types.
+     * @param f The factory function.
+     * @param args Arguments to pass to f.
+     * @return pyptr<T> where T is deduced from f(args...).
+     */
+    template <typename F, typename... Args>
+    auto new_shared(F f, Args... args)
+        -> typename detail::shared_result_of<F, Args...>::type
+    {
+        using P = typename detail::shared_result_of<F, Args...>::type;
+
+        return P(f(args...));
+    }
+
+    // Comparison operators for pyptr<T>
+    template <typename T, typename U>
+    inline bool operator==(pyptr<T> const &x, pyptr<U> const &y) noexcept
+    {
+        return x.get() == y.get();
+    }
+
+    template <typename T, typename U>
+    inline bool operator!=(pyptr<T> const &x, pyptr<U> const &y) noexcept
+    {
+        return !(x == y);
+    }
+
+    template <typename T>
+    inline bool operator==(std::nullptr_t, pyptr<T> const &x) noexcept
+    {
+        return !x;
+    }
+
+    template <typename T>
+    inline bool operator==(pyptr<T> const &x, std::nullptr_t) noexcept
+    {
+        return !x;
+    }
+
+    template <typename T>
+    inline bool operator!=(std::nullptr_t, pyptr<T> const &x) noexcept
+    {
+        return bool(x);
+    }
+
+    template <typename T>
+    inline bool operator!=(pyptr<T> const &x, std::nullptr_t) noexcept
+    {
+        return bool(x);
+    }
+
+    /// Swap two pyptr<T> instances.
+    template <typename T>
+    inline void swap(pyptr<T> &x, pyptr<T> &y) noexcept
+    {
+        x.swap(y);
+    }
+
+    /// Alias for pyptr<PyObject>
+    using object_ptr = pyptr<PyObject>;
+
+}
+
+#endif
\ No newline at end of file
diff --git a/include/swigpyobject.h b/include/swigpyobject.h
new file mode 100644
index 0000000..3377aad
--- /dev/null
+++ b/include/swigpyobject.h
@@ -0,0 +1,30 @@
+#ifndef _D810_SWIGPYOBJECT_H
+#define _D810_SWIGPYOBJECT_H
+
+#include <Python.h>
+
+// @fmt: off
+// clang-format off
+typedef struct
+{
+  PyObject_HEAD  // PyObject ob_base
+  void *ptr;     // This is the pointer to the actual C instance
+  void *ty;      // swig_type_info originally, but shouldn't matter
+  int own;
+  PyObject *next;
+} SwigPyObject;
+// clang-format on
+// @fmt: on
+
+template <typename T>
+T swigtocpp(PyObject *obj)
+{
+  // unwraps python object to get the cpp pointer
+  // from the swig bindings
+  auto swigpointer = reinterpret_cast<SwigPyObject *>(obj);
+  auto objpointervoid = swigpointer->ptr;
+  auto objpointer = reinterpret_cast<T>(objpointervoid);
+  return objpointer;
+}
+
+#endif
\ No newline at end of file
diff --git a/pyproject.toml b/pyproject.toml
index 5fc22fe..1341180 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -1,5 +1,5 @@
 [build-system]
-requires = ["setuptools", "wheel"]
+requires = ["setuptools", "wheel", "Cython"]
 build-backend = "setuptools.build_meta"
 
 [project]
@@ -14,10 +14,28 @@ description = "This is a next generation version of D-810, an IDA Pro plugin whi
 dependencies = ["z3-solver", "typing-extensions"]
 
 [project.optional-dependencies]
-dev = ["pyinstrument"]
+dev = ["pyinstrument>=5.0.0,<6.0.0"]
 
 [tool.setuptools]
 package-dir = { "" = "src" }
 
 [tool.setuptools.packages.find]
 where = ["src"]
+
+[tool.cyright]
+include = ["*.py", "src/", "include/"]
+exclude = ["build/", "docs/", "scripts/", "samples/", "tests/"]
+
+reportGeneralTypeIssues = true
+reportOptionalSubscript = true
+reportOptionalMemberAccess = true
+reportOptionalCall = true
+reportOptionalIterable = true
+reportOptionalContextManager = true
+reportOptionalOperand = true
+reportMissingModuleSource = true
+
+[tool.cython-lint]
+max-line-length = 88
+ignore = ['E503', 'E504']
+exclude = []
diff --git a/samples/bins/libobfuscated_darwin_x86_64.dll b/samples/bins/libobfuscated_darwin_x86_64.dll
index 1472d23..8194088 100755
Binary files a/samples/bins/libobfuscated_darwin_x86_64.dll and b/samples/bins/libobfuscated_darwin_x86_64.dll differ
diff --git a/samples/src/asm/tiny_elf64.asm b/samples/src/asm/tiny_elf64.asm
new file mode 100644
index 0000000..9cad579
--- /dev/null
+++ b/samples/src/asm/tiny_elf64.asm
@@ -0,0 +1,374 @@
+; Taken from: ida-domain/blob/225410d33a4c3c636c15b985ce8ab86c9944cc3d/tests/resources/tiny.asm
+; This is a test file for the IDA Pro assembler.
+; It is used to test the assembler's ability to handle all types of operands.
+; It is not used for any other purpose.
+; nasm -f elf64 tiny.asm -o tiny.bin
+
+BITS 64
+
+section .text
+    global _main
+
+_main:
+    ; Print "Hello, IDA!"
+    mov     rax, 1              ; syscall: sys_write
+    mov     rdi, 1              ; file descriptor (stdout)
+    lea     rsi, [rel hello]    ; Load string address
+    mov     rdx, hello_len      ; String length
+    syscall
+
+    ; =================================================================
+    ; COMPREHENSIVE OPERAND TYPE TESTING SECTION
+    ; =================================================================
+    call    test_all_operand_types
+
+    ; Perform addition: add_numbers(5, 10)
+    mov     rdi, 5
+    mov     rsi, 10
+    call    add_numbers
+    mov     r12, rax            ; Store sum result
+
+    ; Print "Sum: "
+    mov     rax, 1
+    mov     rdi, 1
+    lea     rsi, [rel sum_str]
+    mov     rdx, sum_len
+    syscall
+
+    ; Print sum result
+    mov     rdi, r12
+    call    print_number
+
+    ; Print newline
+    mov     rax, 1
+    mov     rdi, 1
+    lea     rsi, [rel newline]
+    mov     rdx, newline_len
+    syscall
+
+    ; Perform multiplication: multiply_numbers(5, 10)
+    mov     rdi, 5
+    mov     rsi, 10
+    call    multiply_numbers
+    mov     r12, rax            ; Store product result
+
+    ; Print "Product: "
+    mov     rax, 1
+    mov     rdi, 1
+    lea     rsi, [rel product_str]
+    mov     rdx, product_len
+    syscall
+
+    ; Print product result
+    mov     rdi, r12
+    call    print_number
+
+    ; Print newline
+    mov     rax, 1
+    mov     rdi, 1
+    lea     rsi, [rel newline]
+    mov     rdx, newline_len
+    syscall
+
+    ; Test call hierarchy
+    call    level1_func
+
+    ; Exit
+    mov     rax, 60             ; syscall: exit
+    xor     rdi, rdi
+    syscall
+
+; =================================================================
+; COMPREHENSIVE OPERAND TESTING FUNCTION
+; =================================================================
+test_all_operand_types:
+    push    rbp
+    mov     rbp, rsp
+    push    rbx
+    push    r12
+    push    r13
+    push    r14
+    push    r15
+
+    ; === 1. REGISTER TO REGISTER OPERANDS ===
+    mov     rax, rbx            ; reg64 to reg64
+    mov     eax, ebx            ; reg32 to reg32
+    mov     ax, bx              ; reg16 to reg16
+    mov     al, bl              ; reg8 to reg8
+    movzx   rax, ax             ; zero extend reg16 to reg64
+    movsx   rax, eax            ; sign extend reg32 to reg64
+
+    ; === 2. IMMEDIATE OPERANDS ===
+    mov     rax, 0x1234567890ABCDEF    ; 64-bit immediate
+    mov     eax, 0x12345678            ; 32-bit immediate
+    mov     ax, 0x1234                 ; 16-bit immediate
+    mov     al, 0x12                   ; 8-bit immediate
+    add     rax, 42                    ; immediate arithmetic
+    cmp     rax, -1                    ; signed immediate
+
+    ; === 3. DIRECT MEMORY OPERANDS ===
+    mov     rax, [rel test_data]           ; direct memory access
+    mov     eax, [rel test_data]           ; 32-bit direct memory
+    mov     ax, [rel test_data]            ; 16-bit direct memory
+    mov     al, [rel test_data]            ; 8-bit direct memory
+
+    ; === 4. REGISTER INDIRECT OPERANDS ===
+    lea     rbx, [rel test_data]
+    mov     rax, [rbx]                 ; simple register indirect
+    mov     eax, [rbx]                 ; 32-bit register indirect
+    mov     [rbx], rax                 ; write to register indirect
+
+    ; === 5. DISPLACEMENT OPERANDS (register + offset) ===
+    mov     rax, [rbp + 8]             ; positive displacement
+    mov     rax, [rbp - 8]             ; negative displacement
+    mov     rax, [rsp + 16]            ; stack with displacement
+    mov     [rbp - 16], rax            ; write with displacement
+    mov     eax, [rbx + 4]             ; 32-bit with displacement
+
+    ; === 6. SIB OPERANDS (Scale-Index-Base) ===
+    lea     rsi, [rel test_array]
+    mov     rdi, 0                     ; index = 0
+
+    ; Simple SIB: [base + index]
+    mov     rax, [rsi + rdi]           ; [base + index], scale=1
+    mov     rax, [rsi + rdi * 1]       ; explicit scale=1
+
+    ; SIB with scale factors
+    mov     rax, [rsi + rdi * 2]       ; [base + index*2]
+    mov     rax, [rsi + rdi * 4]       ; [base + index*4]
+    mov     rax, [rsi + rdi * 8]       ; [base + index*8]
+
+    ; SIB with displacement
+    mov     rax, [rsi + rdi * 2 + 8]   ; [base + index*scale + disp]
+    mov     rax, [rsi + rdi * 4 - 4]   ; [base + index*scale - disp]
+
+    ; SIB without base (index only)
+    mov     rax, [rdi * 2]             ; [index*scale]
+    mov     rax, [rdi * 4 + 16]        ; [index*scale + disp]
+
+    ; Complex SIB addressing
+    mov     rax, [rbp + rdi * 8 + 0x20] ; [base + index*8 + 32]
+    mov     eax, [rbx + rcx * 2 - 8]    ; 32-bit with complex SIB
+
+    ; === 7. DIFFERENT DATA SIZES ===
+    ; 8-bit operations
+    mov     byte [rel test_data], 0x42
+    mov     bl, byte [rel test_data]
+    add     byte [rsi + rdi], 1
+
+    ; 16-bit operations
+    mov     word [rel test_data], 0x1234
+    mov     bx, word [rel test_data]
+    add     word [rsi + rdi * 2], 100
+
+    ; 32-bit operations
+    mov     dword [rel test_data], 0x12345678
+    mov     ebx, dword [rel test_data]
+    add     dword [rsi + rdi * 4], 1000
+
+    ; 64-bit operations
+    mov     rax, 0x123456789ABCDEF0
+    mov     qword [rel test_data], rax
+    mov     rbx, qword [rel test_data]
+    add     qword [rsi + rdi * 8], 10000
+
+    ; === 8. FLOATING POINT OPERANDS ===
+    ; SSE operations
+    movss   xmm0, [rel float_val]        ; load 32-bit float
+    movsd   xmm1, [rel double_val]       ; load 64-bit double
+    movss   [rel temp_float], xmm0       ; store float
+    movsd   [rel temp_double], xmm1      ; store double
+
+    ; Packed operations
+    movaps  xmm0, [rel vector_data]     ; aligned packed single
+    movups  xmm1, [rel vector_data]     ; unaligned packed single
+
+    ; === 9. STRING OPERATIONS ===
+    lea     rsi, [rel src_string]
+    lea     rdi, [rel dst_string]
+    mov     rcx, 16
+    rep     movsb                      ; string copy with rep prefix
+
+    ; === 10. CONDITIONAL MOVES ===
+    cmp     rax, rbx
+    cmove   rcx, rdx                 ; conditional move if equal
+    cmovg   rcx, rdx                 ; conditional move if greater
+
+    ; === 11. BIT OPERATIONS ===
+    bt      rax, 5                      ; bit test (immediate bit index)
+    bt      rax, rcx                    ; bit test (register bit index)
+    bts     qword [rel test_data], 3       ; bit test and set
+
+    ; === 12. SEGMENT OVERRIDES (x64 mostly ignores, but valid) ===
+    mov     rax, gs:[0x30]             ; segment override
+
+    ; === 13. RIP-RELATIVE ADDRESSING (x64 specific) ===
+    mov     rax, [rel test_data]       ; RIP-relative
+    lea     rbx, [rel hello]           ; RIP-relative LEA
+
+    ; === 14. JUMP/CALL OPERANDS ===
+    ; Note: These won't execute in normal flow, but show operand types
+    jmp     skip_jumps
+
+    call    rax                       ; indirect call (register)
+    call    [rbx]                     ; indirect call (memory)
+    call    [rbx + rcx * 4]           ; indirect call (SIB)
+    jmp     rax                        ; indirect jump (register)
+    jmp     [rel test_data]                ; indirect jump (memory)
+
+skip_jumps:
+
+    ; === 15. STACK OPERATIONS ===
+    push    rax                       ; push register
+    push    qword [rel test_data]         ; push memory
+    push    42                        ; push immediate
+    pop     rbx                        ; pop to register
+    pop     qword [rel test_data]          ; pop to memory
+
+    ; === 16. ATOMIC OPERATIONS ===
+    lock add qword [rel test_data], 1  ; locked memory operation
+    xchg    rax, rbx                  ; exchange registers
+    xchg    rax, [rel test_data]          ; exchange register and memory
+
+    ; === 17. VECTOR INSTRUCTIONS (AVX examples) ===
+    ; Note: These require CPU support
+    ; vaddps ymm0, ymm1, ymm2      ; 256-bit vector add
+    ; vmovdqa ymm0, [vector_data]  ; aligned vector move
+
+    ; Restore registers and return
+    pop     r15
+    pop     r14
+    pop     r13
+    pop     r12
+    pop     rbx
+    pop     rbp
+    ret
+
+; ------------------------------------------------------------------
+; Function: add_numbers(int a, int b) -> int
+; Adds two numbers and returns the result in RAX.
+; ------------------------------------------------------------------
+add_numbers:
+    push    rbp
+    mov     rbp, rsp
+    mov     rax, rdi
+    add     rax, rsi
+    pop     rbp
+    ret
+
+; ------------------------------------------------------------------
+; Function: multiply_numbers(int a, int b) -> int
+; Multiplies two numbers and returns the result in RAX.
+; ------------------------------------------------------------------
+multiply_numbers:
+    push    rbp
+    mov     rbp, rsp
+    mov     rax, rdi
+    imul    rax, rsi
+    pop     rbp
+    ret
+
+; ------------------------------------------------------------------
+; Function: print_number(int num)
+; Converts a number to ASCII and prints it to stdout.
+; ------------------------------------------------------------------
+print_number:
+    mov     rbx, rsp
+    sub     rsp, 20             ; Reserve stack space
+    mov     rsi, rsp
+    mov     rcx, 10             ; Base 10
+    xor     rdx, rdx            ; Clear remainder
+
+    .print_digit:
+        div     rcx                 ; RAX /= 10, remainder in RDX
+        add     dl, '0'             ; Convert remainder to ASCII
+        dec     rsi                 ; Move buffer pointer
+        mov     [rsi], dl           ; Store digit
+        test    rax, rax
+        jnz     .print_digit        ; Continue if RAX != 0
+
+    mov     rax, 1              ; syscall: sys_write
+    mov     rdi, 1
+    mov     rdx, rbx
+    sub     rdx, rsi            ; Calculate printed string length
+    mov     rsi, rsi            ; rsi already points to buffer
+    syscall
+
+    add     rsp, 20             ; Restore stack
+    ret
+
+; =================================================================
+; CALL HIERARCHY FOR TESTING CALLERS/CALLEES
+; =================================================================
+
+; Level 1: Called by _start, calls level2_func_a and level2_func_b
+level1_func:
+    push    rbp
+    mov     rbp, rsp
+
+    call    level2_func_a
+    call    level2_func_b
+
+    pop     rbp
+    ret
+
+; Level 2a: Called by level1_func, calls level3_func
+level2_func_a:
+    push    rbp
+    mov     rbp, rsp
+
+    call    level3_func
+
+    pop     rbp
+    ret
+
+; Level 2b: Called by level1_func, calls level3_func
+level2_func_b:
+    push    rbp
+    mov     rbp, rsp
+
+    call    level3_func
+
+    pop     rbp
+    ret
+
+; Level 3: Called by both level2_func_a and level2_func_b
+level3_func:
+    push    rbp
+    mov     rbp, rsp
+
+    ; Leaf function - no calls
+
+    pop     rbp
+    ret
+
+section .data
+    ; Test data for various operand types
+    test_data       dq 0x1234567890ABCDEF
+    test_array      dq 1, 2, 3, 4, 5, 6, 7, 8
+    temp_float      dd 0.0
+    temp_double     dq 0.0
+
+    ; Vector data (128-bit aligned)
+    align 16
+    vector_data     dd 1.0, 2.0, 3.0, 4.0
+
+    ; String data for testing
+    src_string      db "Source string data", 0
+    dst_string      times 32 db 0
+
+section .rodata
+    hello       db "Hello, IDA!", 10, 0
+    hello_len   equ $ - hello
+
+    sum_str     db "Sum: "
+    sum_len     equ $ - sum_str
+
+    product_str db "Product: "
+    product_len equ $ - product_str
+
+    newline     db 10
+    newline_len equ 1
+
+    float_val   dd 0x4048F5C3      ; 3.14 as 32-bit IEEE float
+    double_val  dq 0x40191EB851EB851F ; 6.28 as 64-bit IEEE double
\ No newline at end of file
diff --git a/samples/src/asm/tiny_x64.asm b/samples/src/asm/tiny_x64.asm
new file mode 100644
index 0000000..913513c
--- /dev/null
+++ b/samples/src/asm/tiny_x64.asm
@@ -0,0 +1,26 @@
+; ----------------------------------------------------------------------------------------
+;
+; To assemble:
+;
+;     nasm -fmacho64 tiny_x64.asm; clang -arch x86_64 tiny_x64.o -o ../../bins/tiny_x64 && rm -f tiny_x64.o
+;
+; To run:
+;
+;     ../../bins/tiny_x64
+;
+; On macOS, this will emit a warning ld: warning: no platform load command ... which can be
+; ignored since it is the correct output for the macOS platform.
+; ----------------------------------------------------------------------------------------
+
+          global    _main
+          extern    _puts
+
+          section   .text
+_main:    push      rbx                     ; Call stack must be aligned
+          lea       rdi, [rel message]      ; First argument is address of message
+          call      _puts                   ; puts(message)
+          pop       rbx                     ; Fix up stack before returning
+          ret                          ; invoke operating system to exit
+
+          section   .data
+message:  db        "Hello, World", 10      ; note the newline at the end
\ No newline at end of file
diff --git a/samples/src/c/unwrap_loops.c b/samples/src/c/unwrap_loops.c
index 7131ee8..042a74e 100644
--- a/samples/src/c/unwrap_loops.c
+++ b/samples/src/c/unwrap_loops.c
@@ -6,9 +6,65 @@ extern long _InterlockedCompareExchange(volatile signed __int32 *Destination, si
 extern RTL_CRITICAL_SECTION g_mutex;
 
 extern void unk_1802CCC58(int);
+extern int lolclose(unsigned __int64 hObject);
 extern void sub_1800D3BF0(int, int, int, int, __int64);
 extern void sub_180221640(unsigned __int64, int, int, unsigned __int64, int, int);
 
+/**
+ * @brief
+ *
+ * @param n0x59
+ * @param n0xA
+ * @param n0x48
+ * @param a4
+ *
+ */
+__int64 __fastcall SafeCloseHandle(
+    int unusedParam1,
+    int unusedParam2,
+    int unusedParam3,
+    unsigned __int64 hHandleToClose)
+{
+    int cleanupState;
+    int finalState;
+    unsigned __int64 hObject;
+
+    for (cleanupState = 0;; cleanupState = 2)
+    {
+        while (1)
+        {
+            finalState = cleanupState;
+            if (cleanupState)
+                break;
+
+            hObject = hHandleToClose;
+            if (hHandleToClose == (unsigned __int64)0xFFFFFFFFFFFFFFFFULL)
+                cleanupState = 2;
+            else
+                cleanupState = 1;
+        }
+
+        if (cleanupState != 1)
+            break;
+
+        lolclose(hObject);
+        hHandleToClose = (unsigned __int64)0xFFFFFFFFFFFFFFFFULL;
+    }
+
+    return finalState;
+}
+
+void bogus_loops(int n0x59, int n0xA, int n0x48, unsigned int *a4)
+{
+    int i;           // [rsp+Ch] [rbp-Ch]
+    unsigned int v5; // [rsp+14h] [rbp-4h]
+
+    for (i = 0; !i; i = 1)
+        v5 = *a4 ^ (*a4 << 0xD) ^ ((*a4 ^ (*a4 << 0xD)) >> 0x11);
+
+    *a4 = v5 ^ (0x20 * v5);
+}
+
 __int64 unwrap_loops()
 {
     int i;
diff --git a/scripts/lldb-it.sh b/scripts/lldb-it.sh
new file mode 100755
index 0000000..49e07cc
--- /dev/null
+++ b/scripts/lldb-it.sh
@@ -0,0 +1,44 @@
+#!/bin/bash
+
+# Find process and line number
+IDA_PID=$(pgrep -x ida)
+if [ -z "$IDA_PID" ]; then
+    echo "Error: IDA process not found"
+    exit 1
+fi
+
+LINE_NUM=$(rg -n "static PyObject \*__pyx_f_.*?_fast_ast_fast_minsn_to_ast.*?\)\s*\{" -i --glob '*.{c,cpp,cxx}' --no-heading | awk -F':' '{print $2}')
+if [ -z "$LINE_NUM" ]; then
+    echo "Error: Could not find target function"
+    exit 1
+fi
+
+echo "Found IDA process: $IDA_PID"
+echo "Found function at line: $LINE_NUM"
+# Find all .dSYM files under ./src/d810 (including hidden and ignored)
+DSYM_FILES=$(fd -u .dSYM ./src/d810)
+
+# For each .dSYM file, add a target symbols add command (strip trailing / if present)
+SYMBOL_COMMANDS=""
+while IFS= read -r line; do
+    # Remove trailing slash if present
+    CLEANED_LINE="${line%/}"
+    echo "target symbols add ${CLEANED_LINE}"
+done <<< "$DSYM_FILES" > /tmp/lldb_symbols.txt
+
+# Create command file
+cat > /tmp/lldb_commands.txt << EOF
+# breakpoint set --file _fast_ast.cpp --line ${LINE_NUM}
+$(cat /tmp/lldb_symbols.txt)
+breakpoint list
+target list
+continue
+EOF
+
+rm -f /tmp/lldb_symbols.txt
+
+# Run LLDB with commands - this will execute the commands and then drop you into interactive mode
+lldb -p "$IDA_PID" -s "/tmp/lldb_commands.txt"
+
+# Clean up
+rm -f /tmp/lldb_commands.txt
\ No newline at end of file
diff --git a/scripts/ununicode.py b/scripts/ununicode.py
index 1fcbe32..628f2b2 100755
--- a/scripts/ununicode.py
+++ b/scripts/ununicode.py
@@ -2,6 +2,7 @@
 # -*- coding: utf-8 -*-
 import argparse
 import sys
+import unicodedata
 from pathlib import Path
 
 
@@ -10,6 +11,7 @@ def clean_text(txt: str) -> str:
     replacements = {
         "–": "-",  # en-dash → hyphen
         "‐": "-",  # unicode hyphen → hyphen
+        "‑": "-",  # unicode hyphen → hyphen
         "\u00a0": " ",  # non-breaking space → regular space
         "\u200b": "",  # zero-width space → removed
         "\u200c": "",  # zero-width non-joiner → removed
@@ -23,13 +25,36 @@ def clean_text(txt: str) -> str:
         "‘": "'",  # left single quote → straight
         "“": '"',  # left double quote → straight
         "”": '"',  # right double quote → straight
+        "【": "[",  # left square bracket → [
+        "】": "]",  # right square bracket → ]
+        "†": "+",  # dagger → *
+        "‡": "*",  # double dagger → *
+        "§": "*",  # section sign → *
+        "¶": "*",  # paragraph sign → *
+        "™": "*",  # trademark symbol → *
+        "©": "*",  # copyright symbol → *
+        "®": "*",  # registered trademark symbol → *
+        "`": "`",  # backtick → `,
+        "•": "*",
+        "◦": "*",
     }
     for uni, ascii_ in replacements.items():
         txt = txt.replace(uni, ascii_)
-    # bullet points → asterisks
-    txt = txt.replace("•", "-").replace("◦", "*")
-    # standardize “. ” → “* ”?
-    return txt
+    normalized = unicodedata.normalize("NFKD", txt)
+    result = []
+    for char in normalized:
+        # Try to get ASCII equivalent through decomposition
+        if ord(char) < 128:
+            result.append(char)
+        elif unicodedata.category(char).startswith(
+            "M"
+        ):  # Mark category (combining marks)
+            # Skip combining marks (they're removed in NFKD normalization)
+            continue
+        else:
+            # Default to "*" for non-ASCII
+            result.append("*")
+    return "".join(result)
 
 
 def main():
@@ -85,3 +110,7 @@ def main():
 
 if __name__ == "__main__":
     main()
+if __name__ == "__main__":
+    main()
+if __name__ == "__main__":
+    main()
diff --git a/setup.py b/setup.py
new file mode 100644
index 0000000..ca196ab
--- /dev/null
+++ b/setup.py
@@ -0,0 +1,255 @@
+"""Build script for the Hex-Rays sample16 Cython module.
+
+This ``setup.py`` mirrors the structure of the ``amplpy`` build
+script.  It detects the host platform and architecture in order to
+select appropriate compilation and linkage flags【561865107149785†L125-L155】.
+The build expects the IDA SDK and Hex-Rays SDK to be installed and
+accessible via the environment variables ``IDA_SDK`` and
+``HEXRAYS_SDK`` respectively.
+"""
+
+import functools
+import os
+import pathlib
+import platform
+import re
+
+from Cython.Build import cythonize
+from setuptools import Extension, find_packages, setup
+
+# ---------------------------------------------------------------------------
+# Platform detection
+# Use the host system and architecture to adjust compiler options.
+OSTYPE = platform.system()
+ARCH = platform.processor() or platform.machine()
+x64 = platform.architecture()[0] == "64bit"
+COMPILER_OPTIMIZATION_LEVEL = re.compile(r"-O[0-3]\b")
+DEBUG_MODE = os.environ.get("DEBUG") == "1"
+
+if ARCH == "ppc64le":
+    LIBRARY = "ppc64le"
+elif ARCH == "aarch64":
+    LIBRARY = "aarch64"
+elif ARCH == "arm" or ARCH == "arm64":
+    LIBRARY = "arm64"
+else:  # 'AMD64', 'x86_64', 'i686', 'i386'
+    LIBRARY = "amd64" if x64 else "intel32"
+
+if OSTYPE == "Darwin":
+    LIBRARY_EXT = ".dylib"
+elif OSTYPE == "Linux":
+    LIBRARY_EXT = ".so"
+else:
+    LIBRARY_EXT = ".dll"
+
+
+def compile_args():
+    """Return platform-specific compilation arguments."""
+    match OSTYPE:
+        case "Windows":
+            if DEBUG_MODE:
+                debug_flags = ["/Z7", "/Od"]
+            # For MSVC: `/TP` tells the compiler to treat sources as C++
+            # files and `/EHa` enables asynchronous exception handling.
+            return ["/TP", "/EHa"] + debug_flags
+        case "Linux":
+            # Suppress a few warnings that are often triggered by IDA
+            # headers.  These are the same warnings ignored in amplpy's
+            # build script【561865107149785†L125-L153】.
+            if DEBUG_MODE:
+                debug_flags = ["-g", "-O0", "-Wall", "-Wextra", "-Wpedantic"]
+            return [
+                "-Wno-stringop-truncation",
+                "-Wno-catch-value",
+                "-Wno-unused-variable",
+            ] + debug_flags
+        case "Darwin":
+            # On macOS specify the minimum supported version and optionally
+            # enable debug symbols when the DEBUG environment variable is set.
+            ignore_warnings = [
+                "-Wno-unused-variable",
+                "-Wno-nullability-completeness",
+                "-Wno-sign-compare",
+                "-Wno-logical-op-parentheses",
+                "-Wno-varargs",
+                "-Wno-unused-private-field",
+                "-Wno-c99-extensions",
+                "-Wno-nested-anon-types",
+                "-Wno-gnu-anonymous-struct",
+                "-Wno-nullability-extension",
+                "-Wno-extra-semi",
+            ]
+            debug_flags = []
+            if DEBUG_MODE:
+                debug_flags = [
+                    "-g",
+                    "-fno-omit-frame-pointer",
+                    "-O0",
+                    "-ggdb",
+                    "-UNDEBUG",
+                    "-Wall",
+                    "-Wextra",
+                    "-Wpedantic",
+                    # Cython is not deprecation-proof
+                    "-Wno-deprecated-declarations",
+                ]
+                # If DEBUG is set, ensure CFLAGS has -O0 (override any -O[0-3])
+                cflags = os.environ.get("CFLAGS", "")
+                # Remove any -O[0-3] flags
+                cflags = COMPILER_OPTIMIZATION_LEVEL.sub("", cflags)
+                # Add -O0 at the beginning (or just set if empty)
+                cflags = "-O0 " + cflags.strip()
+                os.environ["CFLAGS"] = cflags.strip()
+            return ["-mmacosx-version-min=10.9"] + debug_flags + ignore_warnings
+        case _:
+            # Default: no extra flags
+            return []
+
+
+def link_args():
+    """Return platform-specific linker arguments."""
+    match OSTYPE:
+        case "Darwin":
+            # Use @loader_path to encode a relative rpath.  The placeholder
+            # ``{rpath}`` will be substituted at runtime below.
+            rpath = os.path.join("lib")
+            if DEBUG_MODE:
+                debug_flags = ["-g"]
+            return [
+                "-Wl,-headerpad_max_install_names,-rpath,@loader_path/" + rpath
+            ] + debug_flags
+        case "Linux":
+            rpath = os.path.join("lib")
+            if DEBUG_MODE:
+                debug_flags = ["-g"]
+            return ["-Wl,-rpath,$ORIGIN/" + rpath] + debug_flags
+        case "Windows":
+            if DEBUG_MODE:
+                return ["/DEBUG"]
+            return []
+        case _:
+            return []
+
+
+IDA_SDK = pathlib.Path(os.environ.get("IDA_SDK", "/opt/ida/9/sdk"))
+if not IDA_SDK.exists():
+    raise FileNotFoundError(f"IDA SDK not found at {IDA_SDK}")
+
+include_dirs = [
+    IDA_SDK / "include",
+    pathlib.Path(__file__).parent / "include",
+]
+library_dirs = [
+    IDA_SDK / "lib",
+]
+
+libraries = []
+match OSTYPE:
+    case "Windows":
+        libraries.extend(
+            [
+                "Qt5Core",
+                "Qt5Gui",
+                "Qt5PrintSupport",
+                "Qt5Widgets",
+                "ida",
+                "idalib",
+                "network",
+            ]
+        )
+        library_dirs.append(IDA_SDK / "lib" / "x64_win_vc_64")
+        library_dirs.append(IDA_SDK / "lib" / "x64_win_qt")
+    case "Darwin":
+        if LIBRARY == "arm64" or LIBRARY == "aarch64":
+            library_dirs.append(IDA_SDK / "lib" / "arm64_mac_clang_64")
+        else:
+            library_dirs.append(IDA_SDK / "lib" / "x64_mac_clang_64")
+    case "Linux":
+        library_dirs.append(IDA_SDK / "lib" / "x64_linux_gcc_64")
+    case _:
+        pass
+
+
+def ext_modules():
+    include_paths = [str(path) for path in include_dirs]
+    library_paths = [str(path) for path in library_dirs]
+
+    modules = [
+        # Extension(
+        #     "d810.tree.tree_builder",
+        #     sources=[
+        #         "src/d810/tree/tree_builder.cpp",
+        #     ],
+        #     include_dirs=include_paths,
+        #     library_dirs=library_paths,
+        #     libraries=libraries,
+        #     export_symbols=[
+        #         "build_tree",
+        #         "destroy_tree",
+        #         "new_node",
+        #     ],
+        # ),
+    ]
+    macros: list[tuple[str, str | None]] = [("__EA64__", "1")] if x64 else []
+    if DEBUG_MODE:
+        # Profiling requires special macro directives
+        macros.append(("CYTHON_TRACE", "1"))
+        macros.append(("CYTHON_CLINE_IN_TRACEBACK", "1"))
+        macros.append(("CYTHON_CLINE_IN_TRACEBACK_RUNTIME", "1"))
+        macros.append(("CYTHON_USE_SYS_MONITORING", "1"))
+
+    partialed_cythonize = functools.partial(
+        cythonize,
+        compiler_directives={
+            "language_level": "3",
+            "binding": True,
+            "embedsignature": True,
+            "boundscheck": False,
+            "wraparound": False,
+            # these are enabled for debugging only
+            "profile": DEBUG_MODE,
+            "linetrace": DEBUG_MODE,
+        },
+        annotate=DEBUG_MODE,
+        gdb_debug=DEBUG_MODE,
+    )
+    modules += partialed_cythonize(
+        Extension(
+            "*",
+            ["src/d810/**/*.pyx"],
+            language="c++",
+            include_dirs=include_paths,
+            library_dirs=library_paths,
+            libraries=libraries,
+            extra_compile_args=compile_args(),
+            extra_link_args=link_args(),
+            define_macros=macros,
+        )
+    )
+    modules += partialed_cythonize(
+        Extension(
+            "d810.cache",
+            ["src/d810/cache.py"],
+            language="c++",
+            include_dirs=include_paths,
+            library_dirs=library_paths,
+            libraries=libraries,
+            extra_compile_args=compile_args(),
+            extra_link_args=link_args(),
+            define_macros=macros,
+        )
+    )
+
+    return modules
+
+
+setup(
+    name="d810",
+    version="0.1.0",
+    description="D-810 next generation plugin for IDA Pro",
+    ext_modules=ext_modules(),
+    packages=find_packages(include=("d810*",)),
+    package_dir={"": "src"},
+    python_requires=">=3.10",
+    zip_safe=False,
+)
diff --git a/src/D810.py b/src/D810.py
index 183f87b..99cb848 100644
--- a/src/D810.py
+++ b/src/D810.py
@@ -10,6 +10,7 @@ import pathlib
 import pkgutil
 import sys
 import time
+import traceback
 import typing
 
 import ida_hexrays
@@ -592,7 +593,7 @@ class _Scanner:
             except BaseException as e:  # //NOSONAR
                 sys.modules.pop(module.__name__)
                 print(
-                    f"Error while loading extension {spec.name}: {e}",
+                    f"Error while loading extension {spec.name} - {e}\n{traceback.format_exc()}",
                     file=sys.stderr,
                 )
                 return
@@ -796,8 +797,7 @@ class D810Plugin(ReloadablePlugin):
 
     @_compat.override
     def run(self, args):
-        with self.plugin_setup_reload():
-            self.reload()
+        self.reload()
 
     @_compat.override
     def term(self):
@@ -821,13 +821,13 @@ class D810Plugin(ReloadablePlugin):
         The helper prints a concise warning listing only the *core* cycles it
         found; modules merely *blocked* by a cycle are ordered automatically.
         """
-
-        _reload_package_with_graph(
-            pkg_path=d810.__path__,
-            base_package=self.base_package_name,
-            skip_prefixes=(f"{self.base_package_name}.registry",),
-            suppress_errors=self.suppress_reload_errors,
-        )
+        with self.plugin_setup_reload():
+            _reload_package_with_graph(
+                pkg_path=d810.__path__,
+                base_package=self.base_package_name,
+                skip_prefixes=(f"{self.base_package_name}.registry",),
+                suppress_errors=self.suppress_reload_errors,
+            )
 
 
 def PLUGIN_ENTRY():
diff --git a/src/d810/cache.py b/src/d810/cache.py
index f38f9e2..303ca0a 100644
--- a/src/d810/cache.py
+++ b/src/d810/cache.py
@@ -3,7 +3,6 @@ import collections
 import contextlib
 import dataclasses
 import functools
-import sys
 import threading
 import time
 import typing
@@ -18,32 +17,6 @@ V = typing.TypeVar("V")
 Eviction = typing.Callable[["Cache"], None]
 
 
-class SurvivesReload(abc.ABCMeta):
-    """
-    SurvivesReload intercepts calls to the constructor (`__call__`) and, when `survive_reload=True`, stores or returns a shared instance on the module object keyed by `reload_key` (or `_SHARED_<ClassName>`).
-
-    `survive_reload`/`reload_key` are handled only in the metaclass.
-    """
-
-    def __call__(
-        cls,
-        *args,
-        survive_reload: bool = False,
-        reload_key: str = "",
-        **kwargs,
-    ):
-        if survive_reload:
-            module = sys.modules[cls.__module__]
-            key = reload_key or f"_SHARED_{cls.__name__}"
-            existing = getattr(module, key, None)
-            if existing is not None:
-                return existing
-            inst = super().__call__(*args, **kwargs)
-            setattr(module, key, inst)
-            return inst
-        return super().__call__(*args, **kwargs)
-
-
 class OverweightError(Exception):
     pass
 
@@ -139,7 +112,11 @@ def LFU(cache: "Cache") -> None:
     cache._kill(cache._root.lfu_prev)  # type: ignore
 
 
-class CacheImpl(Cache[K, V], metaclass=SurvivesReload):
+def _const_one(_: object) -> float:
+    return 1.0
+
+
+class CacheImpl(Cache[K, V]):
     SKIP = object()
 
     @dataclasses.dataclass(slots=True)
@@ -167,26 +144,21 @@ class CacheImpl(Cache[K, V], metaclass=SurvivesReload):
         max_size: int = DEFAULT_MAX_SIZE,
         max_weight: float | None = None,
         identity_keys: bool = False,
-        expire_after_access: float | None = None,
-        expire_after_write: float | None = None,
+        expire_after_access: int | None = None,
+        expire_after_write: int | None = None,
         removal_listener: (
             typing.Callable[[K | weakref.ref, V | weakref.ref], None] | None
         ) = None,
         clock: typing.Callable[[], float] | None = None,
         weak_keys: bool = False,
         weak_values: bool = False,
-        weigher: typing.Callable[[V], float] = lambda _: 1.0,
+        weigher: typing.Callable[[V], float] = _const_one,
         lock: "threading.RLock | None" = None,
         raise_overweight: bool = False,
         eviction: Eviction = LRU,
         track_frequency: bool | None = None,
-        # for SurvivesReload only (no-op in __init__)
-        survive_reload: bool = False,
-        reload_key: str = "",
     ) -> None:
         super().__init__()
-        # prevent pylint unused-argument
-        _ = (survive_reload, reload_key)  # pylint: disable=unused-variable
         if clock is None:
             if expire_after_access is not None or expire_after_write is not None:
                 clock = time.time
@@ -411,7 +383,7 @@ class CacheImpl(Cache[K, V], metaclass=SurvivesReload):
             return True
         return False
 
-    def clear(self) -> None:
+    def clear(self, reset_stats: bool = False) -> None:
         with self._lock:
             self._cache.clear()
             while True:
@@ -421,6 +393,24 @@ class CacheImpl(Cache[K, V], metaclass=SurvivesReload):
                 if link.unlinked:
                     raise TypeError
                 self._unlink(link)
+            if reset_stats:
+                self.reset_stats()
+
+    def reset_stats(self) -> None:
+        """Reset all statistics counters for this cache.
+
+        This does not mutate the cache contents. Use in tandem with
+        `clear()` if you also want to drop entries.
+
+        After calling this, `stats()` will report zeros for `hits`, `misses`,
+        `max_size_ever`, `max_weight_ever`, and `seq`.
+        """
+        with self._lock:
+            self._hits = 0
+            self._misses = 0
+            self._max_size_ever = 0
+            self._max_weight_ever = 0.0
+            self._seq = 0
 
     def __setitem__(self, key: K, value: V) -> None:
         weight = self._weigher(value)
@@ -534,7 +524,6 @@ class CacheImpl(Cache[K, V], metaclass=SurvivesReload):
                     raise ValueError
                 link = nxt
 
-    @property
     def stats(self) -> Stats:
         with self._lock:
             return Stats(
@@ -559,13 +548,13 @@ def cache(
     max_size: int = CacheImpl.DEFAULT_MAX_SIZE,
     max_weight: float | None = None,
     identity_keys: bool = False,
-    expire_after_access: float | None = None,
-    expire_after_write: float | None = None,
+    expire_after_access: int | None = None,
+    expire_after_write: int | None = None,
     removal_listener: typing.Callable | None = None,
     clock: typing.Callable[[], float] | None = None,
     weak_keys: bool = False,
     weak_values: bool = False,
-    weigher: typing.Callable[[V], float] = lambda _: 1.0,
+    weigher: typing.Callable[[V], float] = _const_one,
     lock: "threading.RLock | None" = None,
     raise_overweight: bool = False,
     eviction: Eviction = LRU,
@@ -626,12 +615,14 @@ def cache(
 def lru_cache(
     user_function: typing.Callable | None = None,
     *,
-    max_size: int = CacheImpl.DEFAULT_MAX_SIZE,
+    max_size: int | None = None,
     **kwargs,
 ) -> typing.Callable:
     """
     Like functools.lru_cache, but backed by our CacheImpl.
     """
+    if max_size is None:
+        max_size = CacheImpl.DEFAULT_MAX_SIZE
     return cache(user_function, max_size=max_size, eviction=LRU, **kwargs)
 
 
diff --git a/src/d810/conf/bogus_loops.json b/src/d810/conf/bogus_loops.json
new file mode 100644
index 0000000..dca1115
--- /dev/null
+++ b/src/d810/conf/bogus_loops.json
@@ -0,0 +1,63 @@
+{
+    "description": "Configuration to unroll bogus loops",
+    "ins_rules": [],
+    "blk_rules": [
+        {
+            "name": "BogusLoopRemover",
+            "is_activated": true,
+            "config": {
+                "dump_intermediate_microcode": false
+            }
+        },
+        {
+            "name": "UnflattenerSwitchCase",
+            "is_activated": true,
+            "config": {}
+        },
+        {
+            "name": "UnflattenerTigressIndirect",
+            "is_activated": true,
+            "config": {}
+        },
+        {
+            "name": "BadWhileLoop",
+            "is_activated": true,
+            "config": {}
+        },
+        {
+            "name": "Unflattener",
+            "is_activated": true,
+            "config": {
+                "max_passes": 5,
+                "min_dispatcher_internal_block": 2,
+                "min_dispatcher_exit_block": 2,
+                "min_dispatcher_comparison_value": 2,
+                "min_entropy": 0.0,
+                "max_entropy": 1.0
+            }
+        },
+        {
+            "name": "JumpFixer",
+            "is_activated": true,
+            "config": {
+                "enabled_rules": [
+                    "CompareConstantRule1",
+                    "CompareConstantRule2",
+                    "CompareConstantRule3",
+                    "JaeRule1",
+                    "JbRule1",
+                    "JnzRule1",
+                    "JnzRule2",
+                    "JnzRule3",
+                    "JnzRule4",
+                    "JnzRule5",
+                    "JnzRule6",
+                    "JnzRule7",
+                    "JnzRule8",
+                    "JmpRuleZ3Const"
+                ],
+                "dump_intermediate_microcode": false
+            }
+        }
+    ]
+}
diff --git a/src/d810/conf/flatfold-noswitch.json b/src/d810/conf/flatfold-noswitch.json
new file mode 100644
index 0000000..e456f4c
--- /dev/null
+++ b/src/d810/conf/flatfold-noswitch.json
@@ -0,0 +1,1297 @@
+{
+  "description": "flatfold deobfuscation",
+  "ins_rules": [
+    {
+      "name": "RotateHelperInlineRule",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "FoldReadonlyDataRule",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "ConstantCallResultFoldRule",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "ReplaceReadonlyAddressOfWithImmediate",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "XorChain",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "AndChain",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "OrChain",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "ArithmeticChain",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "WeirdRule1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "WeirdRule2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "WeirdRule3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "WeirdRule4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "WeirdRule5",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "WeirdRule6",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_HackersDelightRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_HackersDelightRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_HackersDelightRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_HackersDelightRule_4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_HackersDelightRule_5",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_MbaRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_MbaRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_MbaRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_FactorRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_FactorRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_FactorRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_SpecialConstantRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_SpecialConstantRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor1_MbaRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_Rule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_Rule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_Rule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "XorAlmost_Rule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_NestedStuff",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Sub_HackersDelightRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Sub_HackersDelightRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Sub_HackersDelightRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Sub_HackersDelightRule_4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Sub1_FactorRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Sub1_FactorRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Sub1Add_HackersDelightRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Sub1And_HackersDelightRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Sub1Or_MbaRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Sub1And1_MbaRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredSetnzRule1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredSetnzRule2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredSetnzRule3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredSetnzRule4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredSetnzRule5",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredSetnzRule6",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredSetnzRule8",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredSetzRule1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredSetzRule2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredSetzRule3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredSetbRule1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredOdd1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredOdd2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Pred0Rule1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Pred0Rule2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Pred0Rule3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Pred0Rule4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Pred0Rule5",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredFFRule1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredFFRule2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredFFRule3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredFFRule4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredOr2_Rule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredOr1_Rule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Or_HackersDelightRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Or_HackersDelightRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Or_HackersDelightRule_2_variant_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Or_MbaRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Or_MbaRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Or_MbaRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Or_FactorRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Or_FactorRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Or_FactorRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Or_OllvmRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Or_Rule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Or_Rule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Or_Rule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Or_Rule_4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "OrBnot_FactorRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "OrBnot_FactorRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "OrBnot_FactorRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "OrBnot_FactorRule_4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Neg_HackersDelightRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Neg_HackersDelightRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "NegSub_HackersDelightRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "NegAdd_HackersDelightRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "NegAdd_HackersDelightRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "NegOr_HackersDelightRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "NegXor_HackersDelightRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "NegXor_HackersDelightRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Mul_MbaRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Mul_MbaRule_4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Mul_FactorRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Mul_FactorRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "GetIdentRule1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "GetIdentRule2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "GetIdentRule3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule5",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule6",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule7",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule8",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule9",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule10",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule11",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule13",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule14",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule15",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule16",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule17",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule18",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule19",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule20",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule21",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule22",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Bnot_HackersDelightRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Bnot_HackersDelightRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Bnot_MbaRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Bnot_FactorRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Bnot_FactorRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Bnot_FactorRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Bnot_FactorRule_4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "BnotXor_Rule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "BnotXor_Rule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "BnotXor_Rule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "BnotXor_FactorRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "BnotAnd_FactorRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "BnotAnd_FactorRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "BnotAnd_FactorRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "BnotAnd_FactorRule_4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "BnotOr_FactorRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "BnotAdd_MbaRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Bnot_Rule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Bnot_XorRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "And_HackersDelightRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "And_HackersDelightRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "And_HackersDelightRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "And_HackersDelightRule_4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "And_OllvmRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "And_OllvmRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "And_OllvmRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "And_FactorRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "And_FactorRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "AndBnot_HackersDelightRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "AndBnot_HackersDelightRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "AndBnot_FactorRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "AndBnot_FactorRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "AndBnot_FactorRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "AndBnot_FactorRule_4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "AndOr_FactorRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "AndXor_FactorRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "And1_MbaRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Add_HackersDelightRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Add_HackersDelightRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Add_HackersDelightRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Add_HackersDelightRule_4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Add_HackersDelightRule_5",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Add_SpecialConstantRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Add_SpecialConstantRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Add_SpecialConstantRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Add_OllvmRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Add_OllvmRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Add_OllvmRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Add_OllvmRule_4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "AddXor_Rule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "AddXor_Rule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Z3setzRuleGeneric",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Z3setnzRuleGeneric",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Z3lnotRuleGeneric",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Z3SmodRuleGeneric",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Z3ConstantOptimization",
+      "is_activated": true,
+      "config": {
+        "min_nb_opcode": 4,
+        "min_nb_constant": 3,
+        "dump_intermediate_microcode": false
+      }
+    }
+  ],
+  "blk_rules": [
+    {
+      "name": "UnflattenControlFlowRule",
+      "is_activated": true,
+      "config": {}
+    },
+    {
+      "name": "StackVariableConstantPropagationRule",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "UnflattenerFakeJump",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "BadWhileLoop",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "FixPredecessorOfConditionalJumpBlock",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "JumpFixer",
+      "is_activated": true,
+      "config": {
+        "enabled_rules": [
+          "CompareConstantRule1",
+          "CompareConstantRule2",
+          "CompareConstantRule3",
+          "JaeRule1",
+          "JbRule1",
+          "JnzRule1",
+          "JnzRule2",
+          "JnzRule3",
+          "JnzRule4",
+          "JnzRule5",
+          "JnzRule6",
+          "JnzRule7",
+          "JnzRule8",
+          "JmpRuleZ3Const"
+        ],
+        "dump_intermediate_microcode": false
+      }
+    }
+  ]
+}
\ No newline at end of file
diff --git a/src/d810/conf/flatfold.json b/src/d810/conf/flatfold.json
index 74ab259..9820c23 100644
--- a/src/d810/conf/flatfold.json
+++ b/src/d810/conf/flatfold.json
@@ -1,922 +1,1262 @@
 {
-    "description": "flatfold deobfuscation",
-    "ins_rules": [
-        {
-            "name": "AddXor_Rule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "AddXor_Rule_2",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Add_HackersDelightRule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Add_HackersDelightRule_2",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Add_HackersDelightRule_3",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Add_HackersDelightRule_4",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Add_HackersDelightRule_5",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Add_OllvmRule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Add_OllvmRule_2",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Add_OllvmRule_3",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Add_OllvmRule_4",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Add_SpecialConstantRule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Add_SpecialConstantRule_2",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Add_SpecialConstantRule_3",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "And1_MbaRule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "AndBnot_FactorRule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "AndBnot_FactorRule_2",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "AndBnot_FactorRule_3",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "AndBnot_FactorRule_4",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "AndBnot_HackersDelightRule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "AndBnot_HackersDelightRule_2",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "AndOr_FactorRule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "AndXor_FactorRule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "And_FactorRule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "And_FactorRule_2",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "And_HackersDelightRule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "And_HackersDelightRule_2",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "And_HackersDelightRule_3",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "And_HackersDelightRule_4",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "And_OllvmRule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "And_OllvmRule_2",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "And_OllvmRule_3",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "BnotAdd_MbaRule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "BnotAnd_FactorRule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "BnotAnd_FactorRule_2",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "BnotAnd_FactorRule_3",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "BnotAnd_FactorRule_4",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "BnotOr_FactorRule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "BnotXor_FactorRule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "BnotXor_Rule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "BnotXor_Rule_2",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "BnotXor_Rule_3",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Bnot_FactorRule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Bnot_FactorRule_2",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Bnot_FactorRule_3",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Bnot_FactorRule_4",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Bnot_HackersDelightRule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Bnot_HackersDelightRule_2",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Bnot_MbaRule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Bnot_Rule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Bnot_XorRule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "CstSimplificationRule1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "CstSimplificationRule10",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "CstSimplificationRule11",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "CstSimplificationRule13",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "CstSimplificationRule14",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "CstSimplificationRule15",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "CstSimplificationRule16",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "CstSimplificationRule17",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "CstSimplificationRule18",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "CstSimplificationRule19",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "CstSimplificationRule20",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "CstSimplificationRule21",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "CstSimplificationRule22",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "CstSimplificationRule3",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "CstSimplificationRule4",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "CstSimplificationRule5",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "CstSimplificationRule6",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "CstSimplificationRule7",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "CstSimplificationRule8",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "CstSimplificationRule9",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "GetIdentRule1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "GetIdentRule2",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "GetIdentRule3",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Mul_FactorRule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Mul_FactorRule_2",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Mul_MbaRule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Mul_MbaRule_4",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "NegAdd_HackersDelightRule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "NegSub_HackersDelightRule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "NegAdd_HackersDelightRule_2",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "NegOr_HackersDelightRule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "NegXor_HackersDelightRule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "NegXor_HackersDelightRule_2",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Neg_HackersDelightRule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Neg_HackersDelightRule_2",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "OrBnot_FactorRule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "OrBnot_FactorRule_2",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "OrBnot_FactorRule_3",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "OrBnot_FactorRule_4",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Or_FactorRule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Or_FactorRule_2",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Or_FactorRule_3",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Or_HackersDelightRule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Or_HackersDelightRule_2",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Or_HackersDelightRule_2_variant_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Or_MbaRule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Or_MbaRule_2",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Or_MbaRule_3",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Or_OllvmRule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Or_Rule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Or_Rule_2",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Or_Rule_3",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Or_Rule_4",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Pred0Rule1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Pred0Rule2",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Pred0Rule3",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Pred0Rule4",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Pred0Rule5",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "PredFFRule1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "PredFFRule2",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "PredFFRule3",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "PredFFRule4",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "PredOdd1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "PredOdd2",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "PredOr1_Rule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "PredOr2_Rule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "PredSetbRule1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "PredSetnzRule1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "PredSetnzRule2",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "PredSetnzRule3",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "PredSetnzRule4",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "PredSetnzRule5",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "PredSetnzRule6",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "PredSetnzRule8",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "PredSetzRule1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "PredSetzRule2",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "PredSetzRule3",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Sub1Add_HackersDelightRule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Sub1And1_MbaRule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Sub1And_HackersDelightRule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Sub1Or_MbaRule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Sub1_FactorRule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Sub1_FactorRule_2",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Sub_HackersDelightRule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Sub_HackersDelightRule_2",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Sub_HackersDelightRule_3",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Sub_HackersDelightRule_4",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "WeirdRule1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "WeirdRule2",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "WeirdRule3",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "WeirdRule4",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "WeirdRule5",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "WeirdRule6",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Xor1_MbaRule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "XorAlmost_Rule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Xor_FactorRule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Xor_FactorRule_2",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Xor_FactorRule_3",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Xor_HackersDelightRule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Xor_HackersDelightRule_2",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Xor_HackersDelightRule_3",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Xor_HackersDelightRule_4",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Xor_HackersDelightRule_5",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Xor_MbaRule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Xor_MbaRule_2",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Xor_MbaRule_3",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Xor_NestedStuff",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Xor_Rule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Xor_Rule_2",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Xor_Rule_3",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Xor_SpecialConstantRule_1",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Xor_SpecialConstantRule_2",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "AndChain",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "ArithmeticChain",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "OrChain",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "XorChain",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Z3ConstantOptimization",
-            "is_activated": true,
-            "config": {
-                "min_nb_opcode": 4,
-                "min_nb_constant": 3
-            }
-        },
-        {
-            "name": "Z3SmodRuleGeneric",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Z3lnotRuleGeneric",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Z3setnzRuleGeneric",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "Z3setzRuleGeneric",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "ConstantCallResultFoldRule",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "RotateHelperInlineRule",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "FoldReadonlyDataRule",
-            "is_activated": true,
-            "config": {}
-        }
-    ],
-    "blk_rules": [
-        {
-            "name": "StackVariableConstantPropagationRule",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "UnflattenControlFlowRule",
-            "is_activated": false,
-            "config": {}
-        },
-        {
-            "name": "UnflattenerSwitchCase",
-            "is_activated": true,
-            "config": {}
-        },
-        {
-            "name": "JumpFixer",
-            "is_activated": true,
-            "config": {
-                "enabled_rules": [
-                    "CompareConstantRule1",
-                    "CompareConstantRule2",
-                    "CompareConstantRule3",
-                    "JaeRule1",
-                    "JbRule1",
-                    "JnzRule1",
-                    "JnzRule2",
-                    "JnzRule3",
-                    "JnzRule4",
-                    "JnzRule5",
-                    "JnzRule6",
-                    "JnzRule7",
-                    "JnzRule8",
-                    "JmpRuleZ3Const"
-                ]
-            }
-        }
-    ]
-}
+  "description": "flatfold deobfuscation",
+  "ins_rules": [
+    {
+      "name": "RotateHelperInlineRule",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "FoldReadonlyDataRule",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "ConstantCallResultFoldRule",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "ReplaceReadonlyAddressOfWithImmediate",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "XorChain",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "AndChain",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "OrChain",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "ArithmeticChain",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "WeirdRule1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "WeirdRule2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "WeirdRule3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "WeirdRule4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "WeirdRule5",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "WeirdRule6",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_HackersDelightRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_HackersDelightRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_HackersDelightRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_HackersDelightRule_4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_HackersDelightRule_5",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_MbaRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_MbaRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_MbaRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_FactorRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_FactorRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_FactorRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_SpecialConstantRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_SpecialConstantRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor1_MbaRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_Rule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_Rule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_Rule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "XorAlmost_Rule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_NestedStuff",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Sub_HackersDelightRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Sub_HackersDelightRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Sub_HackersDelightRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Sub_HackersDelightRule_4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Sub1_FactorRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Sub1_FactorRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Sub1Add_HackersDelightRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Sub1And_HackersDelightRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Sub1Or_MbaRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Sub1And1_MbaRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredSetnzRule1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredSetnzRule2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredSetnzRule3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredSetnzRule4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredSetnzRule5",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredSetnzRule6",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredSetnzRule8",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredSetzRule1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredSetzRule2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredSetzRule3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredSetbRule1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredOdd1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredOdd2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Pred0Rule1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Pred0Rule2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Pred0Rule3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Pred0Rule4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Pred0Rule5",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredFFRule1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredFFRule2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredFFRule3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredFFRule4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredOr2_Rule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredOr1_Rule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Or_HackersDelightRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Or_HackersDelightRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Or_HackersDelightRule_2_variant_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Or_MbaRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Or_MbaRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Or_MbaRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Or_FactorRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Or_FactorRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Or_FactorRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Or_OllvmRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Or_Rule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Or_Rule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Or_Rule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Or_Rule_4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "OrBnot_FactorRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "OrBnot_FactorRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "OrBnot_FactorRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "OrBnot_FactorRule_4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Neg_HackersDelightRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Neg_HackersDelightRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "NegSub_HackersDelightRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "NegAdd_HackersDelightRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "NegAdd_HackersDelightRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "NegOr_HackersDelightRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "NegXor_HackersDelightRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "NegXor_HackersDelightRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Mul_MbaRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Mul_MbaRule_4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Mul_FactorRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Mul_FactorRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "GetIdentRule1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "GetIdentRule2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "GetIdentRule3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule5",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule6",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule7",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule8",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule9",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule10",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule11",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule13",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule14",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule15",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule16",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule17",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule18",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule19",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule20",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule21",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule22",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Bnot_HackersDelightRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Bnot_HackersDelightRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Bnot_MbaRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Bnot_FactorRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Bnot_FactorRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Bnot_FactorRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Bnot_FactorRule_4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "BnotXor_Rule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "BnotXor_Rule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "BnotXor_Rule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "BnotXor_FactorRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "BnotAnd_FactorRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "BnotAnd_FactorRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "BnotAnd_FactorRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "BnotAnd_FactorRule_4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "BnotOr_FactorRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "BnotAdd_MbaRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Bnot_Rule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Bnot_XorRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "And_HackersDelightRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "And_HackersDelightRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "And_HackersDelightRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "And_HackersDelightRule_4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "And_OllvmRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "And_OllvmRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "And_OllvmRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "And_FactorRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "And_FactorRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "AndBnot_HackersDelightRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "AndBnot_HackersDelightRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "AndBnot_FactorRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "AndBnot_FactorRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "AndBnot_FactorRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "AndBnot_FactorRule_4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "AndOr_FactorRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "AndXor_FactorRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "And1_MbaRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Add_HackersDelightRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Add_HackersDelightRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Add_HackersDelightRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Add_HackersDelightRule_4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Add_HackersDelightRule_5",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Add_SpecialConstantRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Add_SpecialConstantRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Add_SpecialConstantRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Add_OllvmRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Add_OllvmRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Add_OllvmRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Add_OllvmRule_4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "AddXor_Rule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "AddXor_Rule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Z3setzRuleGeneric",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Z3setnzRuleGeneric",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Z3lnotRuleGeneric",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Z3SmodRuleGeneric",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Z3ConstantOptimization",
+      "is_activated": true,
+      "config": {
+        "min_nb_opcode": 4,
+        "min_nb_constant": 3,
+        "dump_intermediate_microcode": false
+      }
+    }
+  ],
+  "blk_rules": [
+    {
+      "name": "StackVariableConstantPropagationRule",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "UnflattenerSwitchCase",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "FixPredecessorOfConditionalJumpBlock",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    }
+  ]
+}
\ No newline at end of file
diff --git a/src/d810/conf/flatfold_no_predicate_loop_fix.json b/src/d810/conf/flatfold_no_predicate_loop_fix.json
new file mode 100755
index 0000000..9bd1e12
--- /dev/null
+++ b/src/d810/conf/flatfold_no_predicate_loop_fix.json
@@ -0,0 +1,1305 @@
+{
+  "description": "flatfold deobfuscation",
+  "ins_rules": [
+    {
+      "name": "RotateHelperInlineRule",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "FoldReadonlyDataRule",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "ConstantCallResultFoldRule",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "ReplaceReadonlyAddressOfWithImmediate",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "XorChain",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "AndChain",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "OrChain",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "ArithmeticChain",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "WeirdRule1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "WeirdRule2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "WeirdRule3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "WeirdRule4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "WeirdRule5",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "WeirdRule6",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_HackersDelightRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_HackersDelightRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_HackersDelightRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_HackersDelightRule_4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_HackersDelightRule_5",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_MbaRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_MbaRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_MbaRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_FactorRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_FactorRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_FactorRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_SpecialConstantRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_SpecialConstantRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor1_MbaRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_Rule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_Rule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_Rule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "XorAlmost_Rule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Xor_NestedStuff",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Sub_HackersDelightRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Sub_HackersDelightRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Sub_HackersDelightRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Sub_HackersDelightRule_4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Sub1_FactorRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Sub1_FactorRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Sub1Add_HackersDelightRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Sub1And_HackersDelightRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Sub1Or_MbaRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Sub1And1_MbaRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredSetnzRule1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredSetnzRule2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredSetnzRule3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredSetnzRule4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredSetnzRule5",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredSetnzRule6",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredSetnzRule8",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredSetzRule1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredSetzRule2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredSetzRule3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredSetbRule1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredOdd1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredOdd2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Pred0Rule1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Pred0Rule2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Pred0Rule3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Pred0Rule4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Pred0Rule5",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredFFRule1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredFFRule2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredFFRule3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredFFRule4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredOr2_Rule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "PredOr1_Rule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Or_HackersDelightRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Or_HackersDelightRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Or_HackersDelightRule_2_variant_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Or_MbaRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Or_MbaRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Or_MbaRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Or_FactorRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Or_FactorRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Or_FactorRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Or_OllvmRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Or_Rule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Or_Rule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Or_Rule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Or_Rule_4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "OrBnot_FactorRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "OrBnot_FactorRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "OrBnot_FactorRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "OrBnot_FactorRule_4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Neg_HackersDelightRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Neg_HackersDelightRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "NegSub_HackersDelightRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "NegAdd_HackersDelightRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "NegAdd_HackersDelightRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "NegOr_HackersDelightRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "NegXor_HackersDelightRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "NegXor_HackersDelightRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Mul_MbaRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Mul_MbaRule_4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Mul_FactorRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Mul_FactorRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "GetIdentRule1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "GetIdentRule2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "GetIdentRule3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule5",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule6",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule7",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule8",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule9",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule10",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule11",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule13",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule14",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule15",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule16",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule17",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule18",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule19",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule20",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule21",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "CstSimplificationRule22",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Bnot_HackersDelightRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Bnot_HackersDelightRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Bnot_MbaRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Bnot_FactorRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Bnot_FactorRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Bnot_FactorRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Bnot_FactorRule_4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "BnotXor_Rule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "BnotXor_Rule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "BnotXor_Rule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "BnotXor_FactorRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "BnotAnd_FactorRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "BnotAnd_FactorRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "BnotAnd_FactorRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "BnotAnd_FactorRule_4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "BnotOr_FactorRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "BnotAdd_MbaRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Bnot_Rule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Bnot_XorRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "And_HackersDelightRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "And_HackersDelightRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "And_HackersDelightRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "And_HackersDelightRule_4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "And_OllvmRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "And_OllvmRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "And_OllvmRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "And_FactorRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "And_FactorRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "AndBnot_HackersDelightRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "AndBnot_HackersDelightRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "AndBnot_FactorRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "AndBnot_FactorRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "AndBnot_FactorRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "AndBnot_FactorRule_4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "AndOr_FactorRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "AndXor_FactorRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "And1_MbaRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Add_HackersDelightRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Add_HackersDelightRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Add_HackersDelightRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Add_HackersDelightRule_4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Add_HackersDelightRule_5",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Add_SpecialConstantRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Add_SpecialConstantRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Add_SpecialConstantRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Add_OllvmRule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Add_OllvmRule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Add_OllvmRule_3",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Add_OllvmRule_4",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "AddXor_Rule_1",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "AddXor_Rule_2",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Z3setzRuleGeneric",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Z3setnzRuleGeneric",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Z3lnotRuleGeneric",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Z3SmodRuleGeneric",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Z3ConstantOptimization",
+      "is_activated": true,
+      "config": {
+        "min_nb_opcode": 4,
+        "min_nb_constant": 3,
+        "dump_intermediate_microcode": false
+      }
+    }
+  ],
+  "blk_rules": [
+    {
+      "name": "StackVariableConstantPropagationRule",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "UnflattenerSwitchCase",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "UnflattenerTigressIndirect",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "UnflattenerFakeJump",
+      "is_activated": true,
+      "config": {}
+    },
+    {
+      "name": "BadWhileLoop",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "Unflattener",
+      "is_activated": true,
+      "config": {
+        "max_passes": 3,
+        "maturities": [
+          "MMAT_LOCOPT",
+          "MMAT_CALLS"
+        ],
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "SimplifiedLoopUnflattener",
+      "is_activated": true,
+      "config": {
+        "max_passes": 3,
+        "maturities": [
+          "MMAT_LOCOPT",
+          "MMAT_CALLS"
+        ],
+        "dump_intermediate_microcode": false
+      }
+    },
+    {
+      "name": "JumpFixer",
+      "is_activated": true,
+      "config": {
+        "dump_intermediate_microcode": false
+      }
+    }
+  ]
+}
\ No newline at end of file
diff --git a/src/d810/conf/state_machine_loops.json b/src/d810/conf/state_machine_loops.json
new file mode 100755
index 0000000..f924ffe
--- /dev/null
+++ b/src/d810/conf/state_machine_loops.json
@@ -0,0 +1,45 @@
+{
+    "description": "Configuration to unroll state machine loops with guaranteed single iteration",
+    "ins_rules": [],
+    "blk_rules": [
+        {
+            "name": "StateMachineLoopUnroller",
+            "is_activated": true,
+            "config": {
+                "dump_intermediate_microcode": false
+            }
+        },
+        {
+            "name": "BogusLoopRemover",
+            "is_activated": true,
+            "config": {
+                "dump_intermediate_microcode": false
+            }
+        },
+        {
+            "name": "JumpFixer",
+            "is_activated": true,
+            "config": {
+                "enabled_rules": [
+                    "CompareConstantRule1",
+                    "CompareConstantRule2",
+                    "CompareConstantRule3",
+                    "JaeRule1",
+                    "JbRule1",
+                    "JnzRule1",
+                    "JnzRule2",
+                    "JnzRule3",
+                    "JnzRule4",
+                    "JnzRule5",
+                    "JnzRule6",
+                    "JnzRule7",
+                    "JnzRule8",
+                    "JmpRuleZ3Const"
+                ],
+                "dump_intermediate_microcode": false
+            }
+        }
+    ]
+}
+
+
diff --git a/src/d810/cythxr/__init__.py b/src/d810/cythxr/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/src/d810/cythxr/_chexrays.pxd b/src/d810/cythxr/_chexrays.pxd
new file mode 100644
index 0000000..439691f
--- /dev/null
+++ b/src/d810/cythxr/_chexrays.pxd
@@ -0,0 +1,2506 @@
+# cython: language_level=3, embedsignature=True
+# distutils: language=c++
+# distutils: define_macros=__EA64__=1
+
+from libc.stdint cimport uintptr_t
+from libc.stddef cimport size_t
+from libcpp.pair cimport pair
+from libcpp.map cimport map
+from libcpp.unordered_map cimport unordered_map
+from libcpp.memory cimport shared_ptr
+from cpython cimport PyObject, PyObject_GetAttrString
+from cython.operator cimport dereference as deref
+
+# cython does not wrap stdarg
+cdef extern from "stdarg.h":
+    ctypedef struct va_list:
+        pass
+
+cdef extern from "swigpyobject.h":
+    ctypedef struct SwigPyObject:
+        void *ptr
+    
+    T swigtocpp[T](PyObject *obj)
+
+ctypedef SwigPyObject* SwigPyObjectPtr
+
+cdef inline void* _swig_ptr(object obj):
+    addr = PyObject_GetAttrString(obj, "this")
+    # Return the actual C++ pointer (s.ptr) stored in the SwigPyObject
+    return (<SwigPyObjectPtr>addr).ptr
+
+
+cdef extern from "pro.h":
+    ctypedef unsigned char uchar   # unsigned 8 bit value/
+    ctypedef unsigned short ushort  # unsigned 16 bit value
+    ctypedef unsigned int uint    # unsigned 32 bit value
+    ctypedef unsigned char uint8 # unsigned 8 bit value
+    ctypedef          char int8 # signed 8 bit value      
+    ctypedef unsigned short uint16 # unsigned 16 bit value
+    ctypedef          short int16 # signed 16 bit value    
+    ctypedef unsigned int uint32 # unsigned 32 bit value
+    ctypedef          int int32 # signed 32 bit value    
+    ctypedef unsigned long long uint64 # unsigned 64 bit value
+    ctypedef          long long int64 # signed 64 bit value
+         
+    ctypedef uint64 ea_t
+    ctypedef uint64 sel_t
+    ctypedef uint64 asize_t
+    ctypedef int64 adiff_t
+
+    ctypedef asize_t uval_t   # unsigned value used by the processor.
+                              #  - for 32-bit ::ea_t - ::uint32
+                              #  - for 64-bit ::ea_t - ::uint64
+    ctypedef adiff_t sval_t   # signed value used by the processor.
+                              #  - for 32-bit ::ea_t - ::int32
+                              #  - for 64-bit ::ea_t - ::int64
+
+    ctypedef uint32 ea32_t    # 32-bit address, regardless of IDA bitness.
+                              # this type can be used when we know in advance
+                              # that 32 bits are enough to hold an address.
+    ctypedef uint64 ea64_t    # 64-bit address, regardless of IDA bitness.
+                              # we need this type for interoperability with
+                              # debug servers, lumina, etc
+
+    # Error code (errno)
+    ctypedef int error_t
+
+    ctypedef uint8 op_dtype_t
+
+    # The inode_t type is the specialization specific inode number.
+    # For example, it can represent a local type ordinal or a structure id.
+    ctypedef uval_t inode_t
+
+    # A position in the difference source.
+    # This is an abstract value that depends on the difference source.
+    # It should be something that can be used to conveniently retrieve information
+    # from a difference source. For example, for the name list it can be the index
+    # in the name list. For structure view it can be the position in the list of structs.
+    # Please note that this is not necessarily an address. However, for the purpose
+    # of comparing the contents of the disassembly listing it can be an address.
+    #
+    # diffpos_t instances must have the following property: adding or removing
+    # items to diff_source_t should not invalidate the existing diffpos_t instances.
+    # They must stay valid after adding or removing items to diff_source_t.
+    # Naturally, deleting an item pointed by diffpos_t may render it incorrect,
+    # this is acceptable and expected.
+    ctypedef size_t diffpos_t
+
+    
+    cdef cppclass qvector[T]:
+        ctypedef T value_type  # the type of objects contained in this qvector
+
+        # Constructor
+        qvector() except +
+        # Constructor - creates a new qvector identical to 'x'
+        qvector_from_copy "qvector"(const qvector[T]& x) except +
+        
+        # Move constructor (not supported in Cython, emulate with copy constructor)
+        # Cython does not support rvalue references; use the copy constructor instead.
+        # Users should be aware this will copy, not move.
+        # qvector_from_move "qvector"(qvector[T]&& x) except +
+
+        # Append a new element to the end the qvector.
+        void push_back(const T& x) except +
+        # Append a new element to the end the qvector with a move semantics.
+        # Requires a fix for IDA Pro's `pro.h` header.
+        # /// Append a new element to the end the qvector with a move semantics.
+        # void push_back(T &&x)
+        # {
+        # #ifdef TESTABLE_BUILD
+        #     QASSERT(1977, !ref_within_range(x));
+        # #endif
+        #     reserve(n+1);
+        #     // FIX: IDA has a bug here.
+        #     // - new (array+n) T(x);  <--- bug
+        #     // + new (array+n) T(std::move(x));  <--- fix
+        #     new (array+n) T(std::move(x));
+        #     ++n;
+        # }
+        void push_back_move "push_back"(T&& x) except +
+        # Append a new empty element to the end of the qvector.
+        # \return a reference to this new element
+        T& push_back_and_get_ref "push_back"(const T& x) except +
+
+        # Remove the last element in the qvector
+        void pop_back() except +
+
+        # Get the number of elements in the qvector
+        size_t size() const
+        # Does the qvector have 0 elements?
+        bint empty() const
+
+        # Allows use of typical c-style array indexing for qvectors (const)
+        const T& const_op_getitem "operator[]"(size_t _idx) const
+        # Allows use of typical c-style array indexing for qvectors
+        T& operator[](size_t _idx)
+
+        # Get element at index '_idx' (const)
+        const T& const_at "at"(size_t _idx) const
+        # Get element at index '_idx'
+        T& at(size_t _idx)
+
+        # Get the first element in the qvector (const)
+        const T& const_front "front"() const
+        # Get the first element in the qvector
+        T& front()
+
+        # Get the last element in the qvector (const)
+        const T& const_back "back"() const
+        # Get the last element in the qvector
+        T& back()
+
+        # Destroy all elements but do not free memory
+        void qclear() except +
+        # Destroy all elements and free memory
+        void clear() except +
+
+        # Allow assignment of one qvector to another using '='
+        qvector[T]& assign_from_copy "operator="(const qvector[T]& x) except +
+        # Move assignment operator (not supported in Cython, emulate with copy constructor)
+        # Cython does not support rvalue references; use the copy constructor instead.
+        # Users should be aware this will copy, not move.
+        # qvector[T]& assign_from_move "operator="(qvector[T]&& x) except +
+
+        # Resize to the given size.
+        # If the given size (_newsize) is less than the current size (n) of the qvector, then the last n - _newsize elements are simply deleted.
+        # If the given size is greater than the current size, the qvector is grown to _newsize, and the last _newsize - n elements will be filled with copies of 'x'.
+        # If the given size is equal to the current size, this function does nothing.
+        void resize(size_t _newsize, const T& x) except +
+        # Same as resize(size_t, const T &), but extra space is filled with empty elements
+        void resize_with_default "resize"(size_t _newsize) except +
+        # Resize the array but do not initialize elements
+        void resize_noinit(size_t _newsize) except +
+
+        # Add an element to the end of the qvector, which will be a new T() if x is not given
+        void grow(const T& x) except +
+
+        # Get the number of elements that this qvector can contain - not the same
+        # as the number of elements currently in the qvector (size())
+        size_t capacity() const
+        # Increase the capacity of the qvector. If cnt is not greater than the current capacity
+        # this function does nothing.
+        void reserve(size_t cnt) except +
+
+        # Shrink the capacity down to the current number of elements
+        void truncate() except +
+
+        # Replace all attributes of this qvector with that of 'r', and vice versa.
+        # Effectively sets this = r and r = this without copying/allocating any memory.
+        void swap(qvector[T]& r) except +
+
+        # Empty the qvector and return a pointer to it's contents.
+        # The caller must free the result of this function
+        T* extract() except +
+        # Populate the qvector with dynamic memory.
+        # The qvector must be empty before calling this method!
+        void inject(T* s, size_t len) except +
+
+        # Allow ability to test the equality of two qvectors using '=='
+        bint operator==(const qvector[T]& r) const
+        # Allow ability to test equality of two qvectors using '!='
+        bint operator!=(const qvector[T]& r) const
+
+        ctypedef T* iterator
+        ctypedef const T* const_iterator
+
+        # Get an iterator that points to the first element in the qvector
+        iterator begin()
+        # Get a const iterator that points to the first element in the qvector
+        const_iterator const_begin "begin"() const
+        # Get an iterator that points to the end of the qvector (NOT the last element)
+        iterator end()
+        # Get a const iterator that points to the end of the qvector (NOT the last element)
+        const_iterator const_end "end"() const
+
+        # Insert an element into the qvector at a specified position.
+        # \param it  an iterator that points to the desired position of the new element
+        # \param x  the element to insert
+        # \return an iterator that points to the newly inserted element
+        iterator insert_copy "insert"(iterator it, const T& x) except +
+        # Insert an element into the qvector with a move semantics.
+        iterator insert_move "insert"(iterator it, T&& x) except +
+        # # Insert a several elements to the qvector at a specified position.
+        # # \param it     position at which new elements will be inserted
+        # # \param first  pointer to first element to be inserted
+        # # \param last   pointer to end of elements to be inserted (the element pointed to by 'last' will not be included)
+        # # \return an iterator that points to the first newly inserted element.
+        # iterator insert_range "insert"(iterator it, it2 first, it2 last) except +
+
+        # Remove an element from the qvector.
+        # \param it  pointer to element to be removed
+        # \return pointer to the element that took its place
+        iterator erase_at "erase"(iterator it) except +
+        # Remove a subset of the qvector.
+        # \param first  pointer to head of subset to be removed
+        # \param last   pointer to end of subset to be removed (element pointed to by last will not be removed)
+        # \return a pointer to the element that took the place of 'first'
+        iterator erase_range "erase"(iterator first, iterator last) except +
+
+        # Find an element in the qvector.
+        # \param x  element to find
+        # \return an iterator that points to the first occurrence of 'x'
+        iterator find(const T& x)
+        # \copydoc find (const version)
+        const_iterator const_find "find"(const T& x) const
+
+        # Find index of the specified value or return -1
+        ssize_t index(const T& x) const
+
+        # Add an element to the end of the qvector
+        void add(const T& x) except +
+        # Add an element to the end of the qvector (move version)
+        void add_move "add"(T&& x) except +
+
+        # Does the qvector contain x?
+        bint has(const T& x) const
+        # Add an element to the end of the qvector - only if it isn't already present.
+        # \param x  the element to add
+        # \return false if 'x' is already in the qvector, true otherwise
+        bint add_unique(const T& x) except +
+
+        # Find an element and remove it.
+        # \param x  the element to remove
+        # \return false if 'x' was not found, true otherwise
+        bint delete "del"(const T& x) except +
+
+        # debug print
+        const char* dstr() const
+    
+    cdef cppclass _qstring[T]:
+
+        # Constructor
+        _qstring() except +
+        # Constructor - creates a new qstring from an existing char *
+        _qstring(const T *ptr) except +
+        # Constructor - creates a new qstring using first 'len' chars from 'ptr'
+        _qstring(const T *ptr, size_t len) except +
+        # Constructor - constructs the string with 'count' copies of character 'ch'
+        _qstring(size_t count, T ch) except +
+
+        # Swap contents of two qstrings. see qvector::swap()
+        void swap(_qstring[T] &r) except +
+        # Get number of chars in this qstring (not including terminating zero)
+        size_t length() const
+        # Get number of chars in this qstring (including terminating zero)
+        size_t size() const
+        # Get number of chars this qstring can contain (including terminating zero)
+        size_t capacity() const
+
+        # Resize to the given size.
+        # The resulting qstring will have length() = s, and size() = s+1
+        # if 's' is greater than the current size then the extra space is filled with 'c'.
+        # if 's' is less than the current size then the trailing chars are removed
+        void resize(size_t s, T c) except +
+        # Similar to resize(size_t, qchar) - but any extra space is filled with zeroes
+        void resize_no_fill "resize"(size_t s) except +
+        void remove_last(int cnt=1) except +
+        # Increase capacity the qstring. see qvector::reserve()
+        void reserve(size_t cnt) except +
+        # Clear qstring and free memory
+        void clear() except +
+        # Clear qstring but do not free memory yet
+        void qclear() except +
+        # Does the qstring have 0 non-null elements?
+        bint empty() const
+        # Convert the qstring to a char *
+        const T *c_str() const
+
+        ctypedef T *iterator
+        ctypedef const T *const_iterator
+        # Get a pointer to the beginning of the qstring
+        iterator begin()
+        # Get a const pointer to the beginning of the qstring
+        const_iterator const_begin "begin"() const
+        # Get a pointer to the end of the qstring (this is not the terminating zero)
+        iterator end()
+        # Get a const pointer to the end of the qstring (this is not the terminating zero)
+        const_iterator const_end "end"() const
+
+        # Allow assignment of qstrings using '='
+        _qstring[T]& operator=(const T *str) except +
+        _qstring[T]& assign_from_qstring "operator="(const _qstring[T] &qstr) except +
+
+        # Append a char using '+='
+        _qstring[T]& operator_iadd_char "operator+="(T c) except +
+        # Append another qstring using '+='
+        _qstring[T]& operator_iadd_qstring "operator+="(const _qstring[T] &r) except +
+        # Get result of appending two qstrings using '+'
+        _qstring[T] operator+(const _qstring[T] &r) const
+
+        bint operator==(const _qstring[T]& r) const
+        bint operator!=(const _qstring[T]& r) const
+        bint operator<(const _qstring[T]& r) const
+        bint operator>(const _qstring[T]& r) const
+        bint operator<=(const _qstring[T]& r) const
+        bint operator>=(const _qstring[T]& r) const
+
+        # Test equality of a qstring and a const char* using '=='
+        bint operator_eq_ptr "operator=="(const T *r) const
+        # Test equality of a qstring and a const char* with '!='
+        bint operator_ne_ptr "operator!=" (const T *r) const
+        # Compare two qstrings using '<'. see qstrcmp()
+        bint operator_lt_ptr "operator<"(const T *r) const
+
+        # Does the string start with the specified prefix?
+        bint starts_with_qstring "starts_with"(const _qstring[T] &str) const
+        bint starts_with_ptr "starts_with"(const T *ptr, ssize_t len) const
+        # Does the string end with the specified suffix?
+        bint ends_with_qstring "ends_with"(const _qstring[T] &str) const
+        bint ends_with_ptr "ends_with"(const T *ptr, ssize_t len) const
+
+        # Retrieve char at index 'idx' using '[]'
+        const T& const_op_getitem "operator[]"(size_t idx) const
+        # Retrieve char at index 'idx' using '[]'
+        T& operator[](size_t idx)
+        
+        # Retrieve const char at index 'idx'
+        const T& const_at "at"(size_t idx) const
+        # Retrieve char at index 'idx'
+        T& at(size_t idx) except +
+        
+        # Extract C string from _qstring. Must qfree() it.
+        T *extract() except +
+        # Assign this qstring to an existing char *.
+        # See qvector::inject(T *, size_t)
+        void inject(T *s, size_t len) except +
+        # Same as to inject(qchar *, size_t), with len = strlen(s)
+        void inject_ptr "inject"(T *s) except +
+        # Get the last qchar in the string (for concatenation checks)
+        T last() const
+
+        # Find a substring.
+        # \param str  the substring to look for
+        # \param pos  starting position
+        # \return the position of the beginning of the first occurrence of str, _qstring::npos of none exists
+        size_t find_ptr "find"(const T *str, size_t pos=0) const
+        # Same as find(const qchar *, size_t), but takes a qstring parameter
+        size_t find_qstring "find"(const _qstring[T] &str, size_t pos=0) const
+        # Find a character in the qstring.
+        # \param c    the character to look for
+        # \param pos  starting position
+        # \return index of first occurrence of 'c' if c is found, _qstring::npos otherwise
+        size_t find_char "find"(T c, size_t pos=0) const
+        
+        # Replace all occurrences of 'what' with 'with_this'.
+        # \return false if 'what' is not found in the qstring, true otherwise
+        bint replace(const T *what, const T *with_this) except +
+        
+        # Search backwards for a character in the qstring.
+        # \param c    the char to look for
+        # \param pos  starting position
+        # \return index of first occurrence of 'c' if c is found, _qstring::npos otherwise
+        size_t rfind(T c, size_t pos=0) const
+
+        # Get a substring.
+        # \param pos   starting position
+        # \param n     ending position (non-inclusive)
+        # \return the resulting substring
+        _qstring[T] substr(size_t pos, size_t n) const
+
+        # Remove characters from the qstring.
+        # \param idx  starting position
+        # \param cnt  number of characters to remove
+        _qstring[T]& remove(size_t idx, size_t cnt) except +
+
+        # Insert a character into the qstring.
+        # \param idx  position of insertion (if idx >= length(), the effect is the same as append)
+        # \param c    char to insert
+        _qstring[T]& insert_char_at "insert"(size_t idx, T c) except +
+        # Insert a string into the qstring.
+        # \param idx     position of insertion (if idx >= length(), the effect is the same as append)
+        # \param str     the string to insert
+        # \param addlen  number of chars from 'str' to insert
+        _qstring[T]& insert_substring_at "insert"(size_t idx, const T *str, size_t addlen) except +
+        # Same as insert(size_t, const qchar *, size_t), but all chars in str are inserted
+        _qstring[T]& insert_string_at "insert"(size_t idx, const T *str) except +
+        # Same as insert(size_t, const qchar *), but takes a qstring parameter
+        _qstring[T]& insert_qstring_at "insert"(size_t idx, const _qstring[T] &qstr) except +
+        # Prepend the qstring with 'c'
+        _qstring[T]& insert_char "insert"(T c) except +
+        # Prepend the qstring with 'str'
+        _qstring[T]& insert_string "insert"(const T *str) except +
+        # Prepend the qstring with 'qstr'
+        _qstring[T]& insert_qstring "insert"(const _qstring[T] &qstr) except +
+
+        # Append c to the end of the qstring
+        _qstring[T]& append(T c) except +
+        # Append a string to the qstring.
+        # \param str     the string to append
+        # \param addlen  number of characters from 'str' to append
+        _qstring[T]& append_substring "append"(const T *str, size_t addlen) except +
+        # Same as append(const qchar *, size_t), but all chars in 'str' are appended
+        _qstring[T]& append_string "append"(const T *str) except +
+        # Same as append(const qchar *), but takes a qstring argument
+        _qstring[T]& append_qstring "append"(const _qstring[T] &qstr) except +
+
+        # Append result of qvsnprintf() to qstring
+        _qstring[T]& cat_vsprnt(const char *format, ...) except +
+        # Replace qstring with the result of qvsnprintf()
+        _qstring[T]& vsprnt(const char *format, ...) except +
+        # Append result of qsnprintf() to qstring
+        _qstring[T]& cat_sprnt(const char *format, ...) except +
+        # Replace qstring with the result of qsnprintf()
+        _qstring[T]& sprnt(const char *format, ...) except +
+        # Replace qstring with the result of qsnprintf()
+        # \sa inline int nowarn_qsnprintf(char *buf, size_t size, const char *format, ...)
+        _qstring[T]& nowarn_sprnt(const char *format, ...) except +
+
+        # Fill qstring with a character.
+        # The qstring is resized if necessary until 'len' chars have been filled
+        # \param pos  starting position
+        # \param c    the character to fill
+        # \param len  number of positions to fill with 'c'
+        _qstring[T]& fill(size_t pos, T c, size_t len) except +
+        # Clear contents of qstring and fill with 'c'
+        _qstring[T]& fill_all "fill"(T c, size_t len) except +
+
+        # Remove all instances of the specified char from the beginning of the qstring
+        _qstring[T]& ltrim(T blank) except +
+        # Remove all instances of the specified char from the end of the qstring
+        _qstring[T]& rtrim(T blank, size_t minlen = 0) except +
+        # Remove all whitespace from the end of the qstring
+        _qstring[T]& rtrim_whitespace "rtrim"() except +
+        # Remove all instances of the specified char from both ends of the qstring
+        _qstring[T]& trim2(T blank) except +
+    
+
+    # Vector of bytes (use for dynamic memory)
+    cdef cppclass bytevec_t(qvector[uchar]):
+        pass
+
+    # instantiate for int
+    ctypedef _qstring[char] qstring
+    ctypedef qvector[qstring] qstringvec_t        
+    ctypedef qvector[uval_t] uvalvec_t    #< vector of unsigned values
+    ctypedef qvector[sval_t] svalvec_t    #< vector of signed values
+    ctypedef qvector[ea_t] eavec_t        #< vector of addresses
+    ctypedef qvector[int] intvec_t        #< vector of integers
+    ctypedef qvector[bint] boolvec_t      #< vector of bools
+    ctypedef qvector[size_t] sizevec_t    #< vector of sizes    
+    int qsnprintf(char *str, size_t n, const char *fmt, ...)
+    
+    # cdef cppclass _qstring[T]:
+    #     # Split a string on SEP, appending the parts to OUT
+    #     # \param out storage
+    #     # \param sep the separator to split on
+    #     # \param flags a combination of \ref qstring_split_flags
+    #     void split(qstringvec_t *out, const T *sep, uint32 flags) const       
+    #     # Join the provided parts into a single string with each element
+    #     # separated by SEP
+    #     # \param parts the parts to join
+    #     # \param sep the separator to join on (it can be an empty string)
+    #     # \return the combined string
+    #     @staticmethod
+    #     _qstring[T] join(const qstringvec_t &parts, const T *sep)    
+
+
+cdef extern from "ieee.h":
+    cdef int FPVAL_NWORDS = 8  # number of words in fpvalue_t
+    cdef cppclass fpvalue_t:
+        ctypedef uint16[8] w
+
+cdef extern from "ida.hpp":
+    ctypedef uchar cm_t
+
+cdef extern from "nalt.hpp":
+    ctypedef uchar type_t;
+    ctypedef uchar reftype_t;  # see \ref reftype_    
+    #-------------------------------------------------------------------------
+    # \name Array representation
+    #@{
+    # Describes how to display an array
+    cdef cppclass array_parameters_t:
+        int32 flags
+        # AP_ALLOWDUPS    0x00000001      # use 'dup' construct
+        # AP_SIGNED       0x00000002      # treats numbers as signed
+        # AP_INDEX        0x00000004      # display array element indexes as comments
+        # AP_ARRAY        0x00000008      # create as array (this flag is not stored in database)
+        # AP_IDXBASEMASK  0x000000F0      # mask for number base of the indexes
+        #   AP_IDXDEC     0x00000000      # display indexes in decimal
+        #   AP_IDXHEX     0x00000010      # display indexes in hex
+        #   AP_IDXOCT     0x00000020      # display indexes in octal
+        #   AP_IDXBIN     0x00000030      # display indexes in binary
+
+        int32 lineitems        # number of items on a line
+        int32 alignment        # -1 - don't align.
+                               # 0  - align automatically.
+                               # else item width
+
+        # Constructor
+        array_parameters_t(int32 _f, int32 _l, int32 _a)
+        # Returns true if the parameters are default
+        bint is_default() const
+
+
+    # Information about a reference
+    cdef cppclass refinfo_t:
+        ea_t target      # reference target (#BADADDR-none)
+        ea_t base        # base of reference (may be BADADDR)
+        adiff_t tdelta   # offset from the target
+        uint32 flags     # \ref REFINFO_
+
+        # \defgroup REFINFO_ Reference info flags
+        # Used by refinfo_t::flags
+        #@{
+        #define REFINFO_TYPE      0x000F  # reference type (reftype_t), or custom
+                                          # reference ID if REFINFO_CUSTOM set
+        #define REFINFO_RVAOFF    0x0010  # based reference (rva);
+                                          # refinfo_t::base will be forced to get_imagebase();
+                                          # such a reference is displayed with the \ash{a_rva} keyword
+        #define REFINFO_PASTEND   0x0020  # reference past an item;
+                                          # it may point to an nonexistent address;
+                                          # do not destroy alignment dirs
+        #define REFINFO_CUSTOM    0x0040  # a custom reference.
+                                          # see custom_refinfo_handler_t.
+                                          # the id of the custom refinfo is
+                                          # stored under the REFINFO_TYPE mask.
+        #define REFINFO_NOBASE    0x0080  # don't create the base xref;
+                                          # implies that the base can be any value.
+                                          # nb: base xrefs are created only if the offset base
+                                          # points to the middle of a segment
+        #define REFINFO_SUBTRACT  0x0100  # the reference value is subtracted from the base value instead of (as usual) being added to it
+        #define REFINFO_SIGNEDOP  0x0200  # the operand value is sign-extended (only supported for REF_OFF8/16/32/64)
+        #define REFINFO_NO_ZEROS  0x0400  # an opval of 0 will be considered invalid
+        #define REFINFO_NO_ONES   0x0800  # an opval of ~0 will be considered invalid
+        #define REFINFO_SELFREF   0x1000  # the self-based reference;
+                                          # refinfo_t::base will be forced to the reference address
+        #@}
+
+        reftype_t type() const
+        # \ref is_reftype_target_optional()
+        bint is_target_optional() const
+        bint no_base_xref() const
+        bint is_pastend() const
+        bint is_rvaoff() const
+        bint is_custom() const
+        bint is_subtract() const
+        bint is_signed() const
+        bint is_no_zeros() const
+        bint is_no_ones() const
+        bint is_selfref() const
+
+        # RT can include REFINFO_CUSTOM bit
+        void set_type(reftype_t rt)
+
+        # init the structure with some default values
+        # reft_and_flags should be REF_xxx optionally ORed with some REFINFO_xxx flags
+        void init(uint32 reft_and_flags, ea_t _base, ea_t _target, adiff_t _tdelta)
+
+        # internal use
+        bint _require_base() const
+
+cdef extern from "typeinf.hpp":
+    ctypedef uint64 typid_t
+
+    # Extended type attributes.
+    # One-symbol keys are reserved to be used by the kernel.
+    # The ones starting with an underscore are reserved too.
+    cdef cppclass type_attr_t:
+        qstring key      # one symbol keys are reserved to be used by the kernel
+                         # the ones starting with an underscore are reserved too
+        bytevec_t value  # attribute bytes
+
+    # This vector must be sorted by keys
+    ctypedef qvector[type_attr_t] type_attrs_t
+    
+    ctypedef int argloc_type_t
+    cdef cppclass tinfo_t:
+        typid_t typid
+        
+
+    # A high level variant of custom_data_type_ids_t
+    cdef struct custom_data_type_info_t:
+        int16 dtid    # data type id
+        int16 fid     # data format ids
+    
+    # Visual representation of a member of a complex type (struct/union/enum)
+    cdef cppclass value_repr_t:
+        uint64 bits
+
+        # Mask for the value type (* means requires additional info):
+        # FRB_MASK   0xF     
+        # FRB_UNK    0x0     #   Unknown
+        # FRB_NUMB   0x1     #   Binary number
+        # FRB_NUMO   0x2     #   Octal number
+        # FRB_NUMH   0x3     #   Hexadecimal number
+        # FRB_NUMD   0x4     #   Decimal number
+        # FRB_FLOAT  0x5     #   Floating point number (for interpreting an integer type as a floating value)
+        # FRB_CHAR   0x6     #   Char
+        # FRB_SEG    0x7     #   Segment
+        # FRB_ENUM   0x8     #   *Enumeration
+        # FRB_OFFSET 0x9     #   *Offset
+        # FRB_STRLIT 0xA     #   *String literal (used for arrays)
+        # FRB_STROFF 0xB     #   *Struct offset
+        # FRB_CUSTOM 0xC     #   *Custom data type
+        # FRB_INVSIGN  0x0100 # Invert sign (0x01 is represented as -0xFF)
+        # FRB_INVBITS  0x0200 # Invert bits (0x01 is represented as ~0xFE)
+        # FRB_SIGNED   0x0400 # Force signed representation
+        # FRB_LZERO    0x0800 # Toggle leading zeroes (used for integers)
+        # FRB_TABFORM  0x1000 # has additional tabular parameters
+
+        # Additional info
+        refinfo_t ri      # FRB_OFFSET
+        int32 strtype     # FRB_STRLIT
+        adiff_t delta     # FRB_STROFF
+        uint32 type_ordinal # FRB_STROFF, FRB_ENUM
+        custom_data_type_info_t cd # FRB_CUSTOM
+
+        array_parameters_t ap # FRB_TABFORM, AP_SIGNED is ignored, use FRB_SIGNED instead
+
+        # Swap with another value_repr_t
+        void swap(value_repr_t &r)
+
+        # Clear the value_repr_t (bits = 0)
+        void clear()
+
+        # Returns True if empty (bits == 0)
+        bint empty() const
+
+        # Returns True if enum ((bits & FRB_MASK) == FRB_ENUM)
+        bint is_enum() const
+
+        # Returns True if offset ((bits & FRB_MASK) == FRB_OFFSET)
+        bint is_offset() const
+
+        # Returns True if strlit ((bits & FRB_MASK) == FRB_STRLIT)
+        bint is_strlit() const
+
+        # Returns True if custom ((bits & FRB_MASK) == FRB_CUSTOM)
+        bint is_custom() const
+
+        # Returns True if stroff ((bits & FRB_MASK) == FRB_STROFF)
+        bint is_stroff() const
+
+        # Returns True if enum or stroff
+        bint is_typref() const
+
+        # Returns True if signed ((bits & FRB_SIGNED) != 0)
+        bint is_signed() const
+
+        # Returns True if has tabular form ((bits & FRB_TABFORM) != 0)
+        bint has_tabform() const
+
+        # Returns True if has leading zeroes ((bits & FRB_LZERO) != 0)
+        bint has_lzeroes() const
+
+        # Get value type (bits & FRB_MASK)
+        uint64 get_vtype() const
+
+        # Set value type
+        void set_vtype(uint64 vt)
+
+        # Set signed flag
+        void set_signed(bint on)
+
+        # Set tabular form flag
+        void set_tabform(bint on)
+
+        # Set leading zeroes flag
+        void set_lzeroes(bint on)
+
+        # Set array parameters
+        void set_ap(const array_parameters_t &_ap)
+
+        # Initialize array parameters
+        void init_ap(array_parameters_t *_ap) const
+
+        # Populate from opinfo
+        # bint from_opinfo(flags64_t flags, aflags_t afl, const opinfo_t *opinfo, const array_parameters_t *_ap)
+
+        # Print to qstring, optionally colored
+        size_t print(qstring *result, bint colored) const
+
+        # Parse value representation from string
+        bint parse_value_repr(const qstring &attr, type_t target_type)
+        
+    #-------------------------------------------------------------------------
+    # An object to represent struct or union members
+    cdef cppclass udm_t:
+        uint64 offset      # member offset in bits
+        uint64 size        # size in bits
+        qstring name       # member name
+        qstring cmt        # member comment
+        tinfo_t type       # member type
+        value_repr_t repr  # radix, refinfo, strpath, custom_id, strtype
+        int effalign       # effective field alignment (in bytes)
+        uint32 tafld_bits  # TAH bits
+        uchar fda          # field alignment (shift amount)
+
+        udm_t() except +
+        # # Create a structure/union member, with the specified name and arbitrary type.
+        # #
+        # # The 'size' will be set automatically.
+        # #
+        # # \param      _name    Member name. Must not be empty.
+        # # \param      _type    Member type. Must not be empty. Can be any valid
+        # #                      udt member type, like a pointer, array, etc.
+        # # \param[in]  _offset  Member offset in bits. It is the caller's
+        # #                      responsibility to specify correct offsets.
+        # udm_t(const char *_name, const tinfo_t &_type, uint64 _offset) except +
+
+        # # Create a structure/union member, with the specified name and simple type.
+        # #
+        # # The 'type' will be created from type_t, and the 'size' will
+        # # be set automatically.
+        # #
+        # # \param      _name    Member name. Must not be empty.
+        # # \param      _type    Member type. Must not be empty.
+        # #                      Can be only a simple type (integral/floating/bool).
+        # # \param[in]  _offset  Member offset in bits. It is the caller's
+        # #                      responsibility to specify correct offsets.
+        # udm_t(const char *_name, const type_t _type, uint64 _offset) except +
+
+        # # Create a structure/union member, with the specified name and type.
+        # #
+        # # The 'type' object will be created by parsing the '_type' type
+        # # declaration, and the 'size' will be set automatically.
+        # #
+        # # \param      _name    Member name. Must not be empty.
+        # # \param      _type    Member type. Must not a valid C type declaration.
+        # # \param[in]  _offset  Member offset in bits. It is the caller's
+        # #                      responsibility to specify correct offsets.
+        # udm_t(const char *_name, const char *_type, uint64 _offset) except +
+
+        # a udt member must at least have a type
+        bint empty() const
+
+        bint is_bitfield() const
+        bint is_zero_bitfield() const
+        bint is_unaligned() const
+        bint is_baseclass() const
+        bint is_virtbase() const
+        bint is_vftable() const
+        bint is_method() const
+        bint is_gap() const
+        bint is_regcmt() const
+        bint is_retaddr() const
+        bint is_savregs() const
+        bint is_special_member() const
+        bint is_by_til() const
+
+        void set_unaligned(bint on=True)
+        void set_baseclass(bint on=True)
+        void set_virtbase(bint on=True)
+        void set_vftable(bint on=True)
+        void set_method(bint on=True)
+        void set_regcmt(bint on=True)
+        void set_retaddr(bint on=True)
+        void set_savregs(bint on=True)
+        void set_by_til(bint on=True)
+        void clr_unaligned()
+        void clr_baseclass()
+        void clr_virtbase()
+        void clr_vftable()
+        void clr_method()
+        uint64 begin() const
+        uint64 end() const
+        bint operator<(const udm_t &r) const
+        bint operator==(const udm_t &r) const
+        bint operator!=(const udm_t &r) const
+
+        # memory allocation and swap
+        # DEFINE_MEMORY_ALLOCATION_FUNCS()
+        void swap(udm_t &r)
+
+        # the user cannot enter anonymous fields in ida (they can come only from tils),
+        # so we use the following trick: if the field type starts with $ and the name
+        # with __, then we consider the field as anonymous
+        bint is_anonymous_udm() const
+
+        void set_value_repr(const value_repr_t &r)
+        bint can_be_dtor() const
+        bint can_rename() const
+           
+    cdef cppclass rrel_t:
+        sval_t off
+        int reg
+
+    cdef cppclass argloc_t:
+        # Note: Only public data members and methods can be declared in cdef cppclass.
+        # The actual C++ class has private members and unions, but for Cython interop,
+        # we only declare what we need to access. The union is flattened as per Cython convention.
+
+        # The type of argument location
+        argloc_type_t type
+
+        # The union members (flattened)
+        sval_t sval           # ::ALOC_STACK, ::ALOC_STATIC
+        uint32 reginfo        # ::ALOC_REG1, ::ALOC_REG2
+        rrel_t* rrel          # ::ALOC_RREL
+        scattered_aloc_t* dist # ::ALOC_DIST
+        void* custom          # ::ALOC_CUSTOM
+        size_t biggest        # to facilitate manipulation of this union
+    
+    cdef cppclass argpart_t(argloc_t):
+        ushort off
+        ushort size
+
+    ctypedef qvector[argpart_t] argpartvec_t
+    cdef cppclass scattered_aloc_t(argpartvec_t):
+        pass
+
+cdef extern from "xref.hpp":
+    ctypedef qvector[svalvec_t] casevec_t
+    
+cdef extern from "idp.hpp":
+    # Get register number and size from register name
+    cdef cppclass reg_info_t:
+        int reg    # register number
+        int size   # register size
+  
+    ctypedef qvector[reg_info_t] reginfovec_t #< vector of register info objects
+    
+cdef extern from "range.hpp":
+    # --------------------------------------------------------------------------
+    # Base class for a range. This class is used as a base class for
+    # a class with real information - see segment.hpp for example.
+    # The end address is excluded, it points past the range end.
+    cdef cppclass range_t:
+        ea_t start_ea   # start_ea included
+        ea_t end_ea     # end_ea excluded
+        
+    # Vector of range_t instances
+    ctypedef qvector[range_t] rangevec_base_t
+    cdef cppclass rangevec_t(rangevec_base_t):
+        pass
+        
+cdef extern from "funcs.hpp":
+
+    # ------------------------------------------------------------------------
+    # A function is a set of continuous ranges of addresses with characteristics
+    cdef cppclass func_t(range_t):
+        # \ref FUNC_
+        uint64 flags
+
+        # Is a far function?
+        bint is_far() const
+        # Does function return?
+        bint does_return() const
+        # Has SP-analysis been performed?
+        bint analyzed_sp() const
+        # Needs prolog analysis?
+        bint need_prolog_analysis() const
+
+        # --- Function entry chunk attributes ---
+        #
+        # Stack frame of the function. It is represented as a structure:
+        #
+        #    +------------------------------------------------+
+        #    | function arguments                             |
+        #    +------------------------------------------------+
+        #    | return address (isn't stored in func_t)        |
+        #    +------------------------------------------------+
+        #    | saved registers (SI, DI, etc - func_t::frregs) |
+        #    +------------------------------------------------+ <- typical BP
+        #    |                                                |  |
+        #    |                                                |  | func_t::fpd
+        #    |                                                |  |
+        #    |                                                | <- real BP
+        #    | local variables (func_t::frsize)               |
+        #    |                                                |
+        #    |                                                |
+        #    +------------------------------------------------+ <- SP
+        #
+        uval_t frame           # netnode id of frame structure - see frame.hpp
+        asize_t frsize         # size of local variables part of frame in bytes.
+                              # If FUNC_FRAME is set and fpd==0, the frame pointer
+                              # (EBP) is assumed to point to the top of the local
+                              # variables range.
+        ushort frregs          # size of saved registers in frame. This range is
+                              # immediately above the local variables range.
+        asize_t argsize        # number of bytes purged from the stack upon returning
+        asize_t fpd            # frame pointer delta. (usually 0, i.e. realBP==typicalBP)
+                              # use update_fpd() to modify it.
+
+        # bgcolor_t color        # user defined function color
+
+        # # The following fields should not be accessed directly:
+
+        # uint32 pntqty          # number of SP change points
+        # stkpnt_t* points       # array of SP change points.
+        #                       # use ...stkpnt...() functions to access this array.
+
+        # int regvarqty          # number of register variables (-1-not read in yet)
+        #                       # use find_regvar() to read register variables
+        # regvar_t* regvars      # array of register variables.
+        #                       # this array is sorted by: start_ea.
+        #                       # use ...regvar...() functions to access this array.
+
+        # int llabelqty          # number of local labels
+        # llabel_t* llabels      # local labels array.
+        #                       # this array shouldn't be modified directly; name.hpp's
+        #                       # SN_LOCAL should be used instead.
+
+        # int regargqty          # number of register arguments.
+        #                       # During analysis IDA tries to guess the register
+        #                       # arguments. It stores the guessing outcome
+        #                       # in this field. As soon as it determines the final
+        #                       # function prototype, regargqty is set to zero.
+        # regarg_t* regargs      # unsorted array of register arguments.
+        #                       # use ...regarg...() functions to access this array.
+        #                       # regargs are destroyed when the full function
+        #                       # type is determined.
+
+        # int tailqty            # number of function tails
+        # range_t* tails         # array of tails, sorted by ea.
+        #                       # use func_tail_iterator_t to access function tails.
+
+        # # --- Function tail chunk attributes ---
+        # ea_t owner             # the address of the main function possessing this tail
+        # int refqty             # number of referers
+        # ea_t* referers         # array of referers (function start addresses).
+        #                       # use func_parent_iterator_t to access the referers.
+    
+cpdef enum class OPTI(unsigned int):
+    ADDREXPRS = 0x0001  # optimize all address expressions (&x+N; &x-&y)
+    MINSTKREF = 0x0002  # may update minstkref
+    COMBINSNS = 0x0004  # may combine insns (only for optimize_insn)
+    NO_LDXOPT = 0x0008  # the function is called after 
+                        # the propagation attempt, we do not optimize
+                        # low/high(ldx) in this case
+    NO_VALRNG = 0x0010  # forbid using valranges
+
+    
+# Declare the C++ structures from the Hex-Rays SDK.
+cdef extern from "hexrays.hpp":
+    ctypedef uint8 mopt_t
+
+    ctypedef int mreg_t
+
+    # Exact dummy sizes so struct packing matches the real SDK.
+    cdef cppclass ivl_with_name_t:
+        char _pad[32]              # sizeof(ivl_with_name_t) = 32
+
+    cdef cppclass rlist_t:
+        char _pad[16]              # sizeof(rlist_t) = 16
+
+    cdef cppclass ivlset_t:
+        char _pad[24]              # sizeof(ivlset_t) = 24
+
+    cdef cppclass mlist_t:
+        rlist_t reg
+        ivlset_t mem
+        const char *dstr() const;
+
+    # Local variable locator.
+    # Local variables are located using definition ea and location.
+    # Each variable must have a unique locator, this is how we tell them apart.
+    cdef cppclass lvar_locator_t:
+        vdloc_t location      # Variable location.
+        ea_t defea           # Definition address. Usually, this is the address
+                             # of the instruction that initializes the variable.
+                             # In some cases it can be a fictional address.
+
+
+    # Definition of a local variable (register or stack) #var #lvar
+    cdef cppclass lvar_t(lvar_locator_t):
+        # friend class mba_t;  # Not expressible in Cython, skip
+
+        int flags              # \ref CVAR_
+
+        qstring name          # variable name.
+                              # use mba_t::set_nice_lvar_name() and
+                              # mba_t::set_user_lvar_name() to modify it
+        qstring cmt           # variable comment string
+        tinfo_t tif           # variable type
+        int width             # variable size in bytes
+        int defblk            # first block defining the variable.
+                              # 0 for args, -1 if unknown
+        uint64 divisor        # max known divisor of the variable
+
+
+    # Vector of local variables
+    ctypedef qvector[lvar_t] lvars_t
+
+    cdef cppclass operand_locator_t:
+        # Operand locator.
+        # It is used to denote a particular operand in the ctree, for example,
+        # when the user right clicks on a constant and requests to represent it,
+        # say, as a hexadecimal number.
+        
+        ea_t ea     # Address of the original processor instruction
+        int opnum   # Operand number in the instruction
+
+        
+    cpdef enum class mblock_type_t(unsigned int):
+        BLT_NONE = 0 # unknown block type
+        BLT_STOP = 1 # stops execution regularly (must be the last block)
+        BLT_0WAY = 2 # does not have successors (tail is a noret function)
+        BLT_1WAY = 3 # passes execution to one block (regular or goto block)
+        BLT_2WAY = 4 # passes execution to two blocks (conditional jump)
+        BLT_NWAY = 5 # passes execution to many blocks (switch idiom)
+        BLT_XTRN = 6 # external block (out of function address)
+
+
+    ## Function roles.
+    ## They are used to calculate use/def lists and to recognize functions
+    ## without using string comparisons.
+    cpdef enum class funcrole_t(unsigned int):
+        ROLE_UNK = 0                   # unknown function role
+        ROLE_EMPTY = 1                 # empty, does not do anything (maybe spoils regs)
+        ROLE_MEMSET = 2                # memset(void *dst, uchar value, size_t count);
+        ROLE_MEMSET32 = 3              # memset32(void *dst, uint32 value, size_t count);
+        ROLE_MEMSET64 = 4              # memset64(void *dst, uint64 value, size_t count);
+        ROLE_MEMCPY = 5                # memcpy(void *dst, const void *src, size_t count);
+        ROLE_STRCPY = 6                # strcpy(char *dst, const char *src);
+        ROLE_STRLEN = 7                # strlen(const char *src);
+        ROLE_STRCAT = 8                # strcat(char *dst, const char *src);
+        ROLE_TAIL = 9                  # char *tail(const char *str);
+        ROLE_BUG = 10                  # BUG() helper macro: never returns, causes exception
+        ROLE_ALLOCA = 11               # alloca() function
+        ROLE_BSWAP = 12                # bswap() function (any size)
+        ROLE_PRESENT = 13              # present() function (used in patterns)
+        ROLE_CONTAINING_RECORD = 14    # CONTAINING_RECORD() macro
+        ROLE_FASTFAIL = 15             # __fastfail()
+        ROLE_READFLAGS = 16            # __readeflags, __readcallersflags
+        ROLE_IS_MUL_OK = 17            # is_mul_ok
+        ROLE_SATURATED_MUL = 18        # saturated_mul
+        ROLE_BITTEST = 19              # [lock] bt
+        ROLE_BITTESTANDSET = 20        # [lock] bts
+        ROLE_BITTESTANDRESET = 21      # [lock] btr
+        ROLE_BITTESTANDCOMPLEMENT = 22 # [lock] btc
+        ROLE_VA_ARG = 23               # va_arg() macro
+        ROLE_VA_COPY = 24              # va_copy() function
+        ROLE_VA_START = 25             # va_start() function
+        ROLE_VA_END = 26               # va_end() function
+        ROLE_ROL = 27                  # rotate left
+        ROLE_ROR = 28                  # rotate right
+        ROLE_CFSUB3 = 29               # carry flag after subtract with carry
+        ROLE_OFSUB3 = 30               # overflow flag after subtract with carry
+        ROLE_ABS = 31                  # integer absolute value
+        ROLE_3WAYCMP0 = 32             # 3-way compare helper, returns -1/0/1
+        ROLE_3WAYCMP1 = 33             # 3-way compare helper, returns 0/1/2
+        ROLE_WMEMCPY = 34              # wchar_t *wmemcpy(wchar_t *dst, const wchar_t *src, size_t n)
+        ROLE_WMEMSET = 35              # wchar_t *wmemset(wchar_t *dst, wchar_t wc, size_t n)
+        ROLE_WCSCPY = 36               # wchar_t *wcscpy(wchar_t *dst, const wchar_t *src);
+        ROLE_WCSLEN = 37               # size_t wcslen(const wchar_t *s)
+        ROLE_WCSCAT = 38               # wchar_t *wcscat(wchar_t *dst, const wchar_t *src)
+        ROLE_SSE_CMP4 = 39             # e.g. _mm_cmpgt_ss
+        ROLE_SSE_CMP8 = 40             # e.g. _mm_cmpgt_sd
+
+    #-------------------------------------------------------------------------
+    # We use our own class to store argument and variable locations.
+    # It is called vdloc_t that stands for 'vd location'.
+    # 'vd' is the internal name of the decompiler, it stands for 'visual decompiler'.
+    # The main differences between vdloc and argloc_t:
+    #   ALOC_REG1: the offset is always 0, so it is not used. the register number
+    #              uses the whole ~VLOC_MASK field.
+    #   ALOC_STACK: stack offsets are always positive because they are based on
+    #              the lowest value of sp in the function.
+    cdef cppclass vdloc_t(argloc_t):
+        pass
+
+    cdef cppclass lvar_ref_t:
+        # Pointer to the parent mba_t object.
+        # Since we need to access the 'mba->vars' array in order to retrieve
+        # the referenced variable, we keep a pointer to mba_t here.
+        # Note: this means this class and consequently mop_t, minsn_t, mblock_t
+        #       are specific to a mba_t object and cannot migrate between
+        #       them. fortunately this is not something we need to do.
+        #       second, lvar_ref_t's appear only after MMAT_LVARS.
+        mba_t* mba  # mba_t *const mba
+
+        sval_t off  # offset from the beginning of the variable
+        int idx     # index into mba->vars
+
+    # Scattered operand info. Used for mop_sc
+    cdef cppclass scif_t(vdloc_t):
+        # Pointer to the parent mba_t object.
+        # Some operations may convert a scattered operand into something simpler,
+        # (a stack operand, for example). We will need to create stkvar_ref_t at
+        # that moment, this is why we need this pointer.
+        # See notes for lvar_ref_t::mba.
+        mba_t* mba
+        # Usually scattered operands are created from a function prototype,
+        # which has the name information. We preserve it and use it to name
+        # the corresponding local variable.
+        qstring name
+
+        # Scattered operands always have type info assigned to them
+        # because without it we won't be able to manipulate them.
+        tinfo_t type
+
+    # --- mop_t and its internal union members ---
+    cdef struct stkvar_ref_t:
+        # Pointer to the parent mba_t object.
+        # We need it in order to retrieve the referenced stack variable.
+        # See notes for lvar_ref_t::mba.
+        mba_t* mba  # mba_t* const mba
+
+        # Offset to the stack variable from the bottom of the stack frame.
+        # It is called 'decompiler stkoff' and it is different from IDA stkoff.
+        # See a note and a picture about 'decompiler stkoff' below.
+        sval_t off
+        # Swap the contents of this stkvar_ref_t with another.
+        void swap(stkvar_ref_t &r)
+
+        # Retrieve the referenced stack variable.
+        # \param[out] udm  stkvar, may be nullptr
+        # \param p_idaoff if specified, will hold IDA stkoff after the call.
+        # \return index of stkvar in the frame or -1
+        ssize_t get_stkvar(udm_t *udm, uval_t *p_idaoff)
+
+    cdef cppclass mnumber_t(operand_locator_t):
+        uint64 value  # value
+        uint64 org_value  # original value before changing the operand size
+
+    #-------------------------------------------------------------------------
+    # Floating point constant. Used for mop_fn
+    # For more details, please see the ieee.h file from IDA SDK.
+    cdef cppclass fnumber_t:
+        fpvalue_t fnum    # Internal representation of the number
+        int nbytes        # Original size of the constant in bytes
+        # Get displayable text without tags in a static buffer
+        const char *dstr() const
+    # Pair of operands
+    cdef cppclass mop_pair_t:
+        mop_t lop      # low operand
+        mop_t hop      # high operand
+        # HEXRAYS_MEMORY_ALLOCATION_FUNCS() is a macro for custom new/delete, not needed in .pxd
+
+    cdef cppclass mop_t:
+        mop_t() except +
+        mopt_t t         # operand type
+        uint8 oprops
+        uint16 valnum
+        int size
+        # The following union holds additional details about the operand.
+        # Depending on the operand type different kinds of info are stored.
+        # You should access these fields only after verifying the operand type.
+        # All pointers are owned by the operand and are freed by its destructor.
+        
+        mreg_t r           # mop_r   register number
+        mnumber_t *nnn     # mop_n   immediate value
+        minsn_t *d         # mop_d   result (destination) of another instruction
+        stkvar_ref_t *s    # mop_S   stack variable
+        ea_t g             # mop_v   global variable (its linear address)
+        int b              # mop_b   block number (used in jmp,call instructions)
+        mcallinfo_t *f     # mop_f   function call information
+        lvar_ref_t *l      # mop_l   local variable
+        mop_addr_t *a      # mop_a   variable whose address is taken
+        char *helper       # mop_h   helper function name
+        char *cstr         # mop_str utf8 string constant, user representation
+        mcases_t *c        # mop_c   cases
+        fnumber_t *fpc     # mop_fn  floating point constant
+        mop_pair_t *pair   # mop_p   operand pair
+        scif_t *scif       # mop_sc  scattered operand info
+        # Get displayable text without tags in a static buffer
+        const char *dstr() const
+        void make_number(uint64 val, int size)
+        void assign(const mop_t& other)
+
+    
+    # Address of an operand (mop_l, mop_v, mop_S, mop_r)
+    cdef cppclass mop_addr_t(mop_t):
+        int insize     # how many bytes of the pointed operand can be read
+        int outsize    # how many bytes of the pointed operand can be written
+        
+    # A call argument
+    cdef cppclass mcallarg_t(mop_t):  # #callarg
+        ea_t ea      # address where the argument was initialized. BADADDR means unknown.
+        tinfo_t type # formal argument type
+        qstring name # formal argument name
+        argloc_t argloc # ida argloc
+        uint32 flags # FAI_...
+        # Get displayable text without tags in a static buffer
+        const char *dstr() const
+
+    ctypedef qvector[mop_t] mopvec_t;
+    ctypedef qvector[mcallarg_t] mcallargs_t;
+    # Information about a call
+    cdef cppclass mcallinfo_t:
+        ea_t callee
+        int solid_args
+        int call_spd
+        int stkargs_top
+        cm_t cc
+        mcallargs_t args
+        mopvec_t retregs
+        tinfo_t return_type
+        argloc_t return_argloc
+        mlist_t return_regs
+        mlist_t spoiled
+        mlist_t pass_regs
+        ivlset_t visible_memory
+        mlist_t dead_regs
+        int flags
+        funcrole_t role
+        type_attrs_t fti_attrs
+        # Get displayable text without tags in a static buffer
+        const char *dstr() const
+
+
+    # List of switch cases and targets
+    cdef cppclass mcases_t:
+        casevec_t values
+        intvec_t targets
+
+    # -------------------------------------------------------------------------
+    # \defgroup MERR_ Microcode error codes
+    #@{
+    ctypedef enum merror_t:
+        MERR_OK        = 0    # ok
+        MERR_BLOCK     = 1    # no error, switch to new block
+        MERR_INTERR    = -1   # internal error
+        MERR_INSN      = -2   # cannot convert to microcode
+        MERR_MEM       = -3   # not enough memory
+        MERR_BADBLK    = -4   # bad block found
+        MERR_BADSP     = -5   # positive sp value has been found
+        MERR_PROLOG    = -6   # prolog analysis failed
+        MERR_SWITCH    = -7   # wrong switch idiom
+        MERR_EXCEPTION = -8   # exception analysis failed
+        MERR_HUGESTACK = -9   # stack frame is too big
+        MERR_LVARS     = -10  # local variable allocation failed
+        MERR_BITNESS   = -11  # 16-bit functions cannot be decompiled
+        MERR_BADCALL   = -12  # could not determine call arguments
+        MERR_BADFRAME  = -13  # function frame is wrong
+        MERR_UNKTYPE   = -14  # undefined type %s (currently unused error code)
+        MERR_BADIDB    = -15  # inconsistent database information
+        MERR_SIZEOF    = -16  # wrong basic type sizes in compiler settings
+        MERR_REDO      = -17  # redecompilation has been requested
+        MERR_CANCELED  = -18  # decompilation has been cancelled
+        MERR_RECDEPTH  = -19  # max recursion depth reached during lvar allocation
+        MERR_OVERLAP   = -20  # variables would overlap: %s
+        MERR_PARTINIT  = -21  # partially initialized variable %s
+        MERR_COMPLEX   = -22  # too complex function
+        MERR_LICENSE   = -23  # no license available
+        MERR_ONLY32    = -24  # only 32-bit functions can be decompiled for the current database
+        MERR_ONLY64    = -25  # only 64-bit functions can be decompiled for the current database
+        MERR_BUSY      = -26  # already decompiling a function
+        MERR_FARPTR    = -27  # far memory model is supported only for pc
+        MERR_EXTERN    = -28  # special segments cannot be decompiled
+        MERR_FUNCSIZE  = -29  # too big function
+        MERR_BADRANGES = -30  # bad input ranges
+        MERR_BADARCH   = -31  # current architecture is not supported
+        MERR_DSLOT     = -32  # bad instruction in the delay slot
+        MERR_STOP      = -33  # no error, stop the analysis
+        MERR_CLOUD     = -34  # cloud: %s
+        MERR_MAX_ERR   = 34
+        MERR_LOOP      = -35  # internal code: redo last loop (never reported)
+    #@}
+
+    # Get textual description of an error code
+    # \param out  the output buffer for the error description
+    # \param code \ref MERR_
+    # \param mba  the microcode array
+    # \return the error address
+    ea_t get_merror_desc(qstring *out, merror_t code, mba_t *mba)
+  
+    # -------------------------------------------------------------------------
+    # List of microinstruction opcodes.
+    # The order of setX and jX insns is important, it is used in the code.
+
+    # Instructions marked with *F may have the FPINSN bit set and operate on fp values
+    # Instructions marked with +F must have the FPINSN bit set. They always operate on fp values
+    # Other instructions do not operate on fp values.
+    ctypedef enum mcode_t:
+        m_nop    # nop                       // no operation
+        m_stx    # stx  l,    {r=sel, d=off} // store register to memory     *F
+        m_ldx    # ldx  {l=sel,r=off}, d     // load register from memory    *F
+        m_ldc    # ldc  l=const,     d       // load constant
+        m_mov    # mov  l,           d       // move                         *F
+        m_neg    # neg  l,           d       // negate
+        m_lnot   # lnot l,           d       // logical not
+        m_bnot   # bnot l,           d       // bitwise not
+        m_xds    # xds  l,           d       // extend (signed)
+        m_xdu    # xdu  l,           d       // extend (unsigned)
+        m_low    # low  l,           d       // take low part
+        m_high   # high l,           d       // take high part
+        m_add    # add  l,   r,      d       // l + r -> dst
+        m_sub    # sub  l,   r,      d       // l - r -> dst
+        m_mul    # mul  l,   r,      d       // l * r -> dst
+        m_udiv   # udiv l,   r,      d       // l / r -> dst
+        m_sdiv   # sdiv l,   r,      d       // l / r -> dst
+        m_umod   # umod l,   r,      d       // l % r -> dst
+        m_smod   # smod l,   r,      d       // l % r -> dst
+        m_or     # or   l,   r,      d       // bitwise or
+        m_and    # and  l,   r,      d       // bitwise and
+        m_xor    # xor  l,   r,      d       // bitwise xor
+        m_shl    # shl  l,   r,      d       // shift logical left
+        m_shr    # shr  l,   r,      d       // shift logical right
+        m_sar    # sar  l,   r,      d       // shift arithmetic right
+        m_cfadd  # cfadd l,  r,    d=carry   // calculate carry    bit of (l+r)
+        m_ofadd  # ofadd l,  r,    d=overf   // calculate overflow bit of (l+r)
+        m_cfshl  # cfshl l,  r,    d=carry   // calculate carry    bit of (l<<r)
+        m_cfshr  # cfshr l,  r,    d=carry   // calculate carry    bit of (l>>r)
+        m_sets   # sets  l,          d=byte  SF=1          Sign
+        m_seto   # seto  l,  r,      d=byte  OF=1          Overflow of (l-r)
+        m_setp   # setp  l,  r,      d=byte  PF=1          Unordered/Parity        *F
+        m_setnz  # setnz l,  r,      d=byte  ZF=0          Not Equal               *F
+        m_setz   # setz  l,  r,      d=byte  ZF=1          Equal                   *F
+        m_setae  # setae l,  r,      d=byte  CF=0          Unsigned Above or Equal *F
+        m_setb   # setb  l,  r,      d=byte  CF=1          Unsigned Below          *F
+        m_seta   # seta  l,  r,      d=byte  CF=0 & ZF=0   Unsigned Above          *F
+        m_setbe  # setbe l,  r,      d=byte  CF=1 | ZF=1   Unsigned Below or Equal *F
+        m_setg   # setg  l,  r,      d=byte  SF=OF & ZF=0  Signed Greater
+        m_setge  # setge l,  r,      d=byte  SF=OF         Signed Greater or Equal
+        m_setl   # setl  l,  r,      d=byte  SF!=OF        Signed Less
+        m_setle  # setle l,  r,      d=byte  SF!=OF | ZF=1 Signed Less or Equal
+        m_jcnd   # jcnd   l,         d       // d is mop_v or mop_b
+        m_jnz    # jnz    l, r,      d       // ZF=0          Not Equal               *F
+        m_jz     # jz     l, r,      d       // ZF=1          Equal                   *F
+        m_jae    # jae    l, r,      d       // CF=0          Unsigned Above or Equal *F
+        m_jb     # jb     l, r,      d       // CF=1          Unsigned Below          *F
+        m_ja     # ja     l, r,      d       // CF=0 & ZF=0   Unsigned Above          *F
+        m_jbe    # jbe    l, r,      d       // CF=1 | ZF=1   Unsigned Below or Equal *F
+        m_jg     # jg     l, r,      d       // SF=OF & ZF=0  Signed Greater
+        m_jge    # jge    l, r,      d       // SF=OF         Signed Greater or Equal
+        m_jl     # jl     l, r,      d       // SF!=OF        Signed Less
+        m_jle    # jle    l, r,      d       // SF!=OF | ZF=1 Signed Less or Equal
+        m_jtbl   # jtbl   l, r=mcases        // Table jump
+        m_ijmp   # ijmp       {r=sel, d=off} // indirect unconditional jump
+        m_goto   # goto   l                  // l is mop_v or mop_b
+        m_call   # call   l          d       // l is mop_v or mop_b or mop_h
+        m_icall  # icall  {l=sel, r=off} d   // indirect call
+        m_ret    # ret
+        m_push   # push   l
+        m_pop    # pop               d
+        m_und    # und               d       // undefine
+        m_ext    # ext  in1, in2,  out1      // external insn, not microcode *F
+        m_f2i    # f2i    l,    d       int(l) => d; convert fp -> integer   +F
+        m_f2u    # f2u    l,    d       uint(l)=> d; convert fp -> uinteger  +F
+        m_i2f    # i2f    l,    d       fp(l)  => d; convert integer -> fp   +F
+        m_u2f    # i2f    l,    d       fp(l)  => d; convert uinteger -> fp  +F
+        m_f2f    # f2f    l,    d       l      => d; change fp precision     +F
+        m_fneg   # fneg   l,    d       -l     => d; change sign             +F
+        m_fadd   # fadd   l, r, d       l + r  => d; add                     +F
+        m_fsub   # fsub   l, r, d       l - r  => d; subtract                +F
+        m_fmul   # fmul   l, r, d       l * r  => d; multiply                +F
+        m_fdiv   # fdiv   l, r, d       l / r  => d; divide                  +F
+
+
+    # --- Other Hex-Rays types ---
+    cdef cppclass minsn_t:
+        mcode_t opcode
+        int iprops           # combination of IPROP_ bits
+        minsn_t *next        # next insn in doubly linked list. check also nexti()
+        minsn_t *prev        # prev insn in doubly linked list. check also previ()
+        ea_t ea              # instruction address
+        mop_t l              # left operand
+        mop_t r              # right operand
+        mop_t d              # destination operand
+
+        # Get displayable text without tags in a static buffer
+        const char *dstr() const
+
+        # Change the instruction address.
+        # This function modifies subinstructions as well.
+        void setaddr(ea_t new_ea)
+
+        # Optimize one instruction without context.
+        # This function does not have access to the instruction context (the
+        # previous and next instructions in the list, the block number, etc).
+        # It performs only basic optimizations that are available without this info.
+        # \param optflags combination of \ref OPTI_ bits
+        # \return number of changes, 0-unchanged
+        # See also mblock_t::optimize_insn()
+        int optimize_solo(int optflags)
+        
+        # Does the instruction have a side effect?
+        # \param include_ldx_and_divs consider ldx/div/mod as having side effects?
+        #                    stx is always considered as having side effects.
+        # Apart from ldx/std only call may have side effects.
+        bint has_side_effects(bint include_ldx_and_divs) const
+        
+        # Is a helper call with the specified name?
+        # Helper calls usually have well-known function names (see \ref FUNC_NAME_)
+        # but they may have any other name. The decompiler does not assume any
+        # special meaning for non-well-known names.
+        bint is_helper(const char *name) const
+        
+        # Is an unknown call?
+        # Unknown calls are calls without the argument list (mcallinfo_t).
+        # Usually the argument lists are determined by mba_t::analyze_calls().
+        # Unknown calls exist until the MMAT_CALLS maturity level.
+        # See also \ref mblock_t::is_call_block
+        bint is_unknown_call() const
+
+    cdef cppclass mblock_t:
+        mblock_t *nextb              #< next block in the doubly linked list
+        mblock_t *prevb              #< previous block in the doubly linked list
+        uint32 flags                 #< combination of \ref MBL_ bits     
+        ea_t start                   #< start address
+        ea_t end                     #< end address
+        minsn_t* head                #< pointer to the first instruction of the block
+        minsn_t* tail                #< pointer to the last instruction of the block
+        mba_t* mba                   #< the parent micro block array
+        int serial                   #< block number
+        mblock_type_t type           #< block type (BLT_NONE - not computed yet)
+        mlist_t dead_at_start        #< data that is dead at the block entry
+        mlist_t mustbuse             #< data that must be used by the block
+        mlist_t maybuse              #< data that may  be used by the block
+        mlist_t mustbdef             #< data that must be defined by the block
+        mlist_t maybdef              #< data that may  be defined by the block
+        mlist_t dnu                  #< data that is defined but not used in the block
+        sval_t maxbsp                #< maximal sp value in the block (0...stacksize)
+        sval_t minbstkref            #< lowest stack location accessible with indirect
+                                      #< addressing (offset from the stack bottom)
+                                      #< initially it is 0 (not computed)
+        sval_t minbargref            #< the same for arguments
+        intvec_t predset             #< control flow graph: list of our predecessors
+                                      #< use npred() and pred() to access it
+        intvec_t succset             #< control flow graph: list of our successors
+
+
+        # Mark the block's use-def lists as dirty and request propagation
+        void mark_lists_dirty()
+        # Request propagation for the block
+        void request_propagation()
+        # Check if propagation is needed
+        bint needs_propagation() const
+        # Request demotion to 64-bit
+        void request_demote64()
+        # Check if lists are dirty
+        bint lists_dirty() const
+        # Check if lists are ready
+        bint lists_ready() const
+        # Make lists ready, returns number of changes
+        int make_lists_ready()
+
+        # Get number of block predecessors (number of xrefs to the block)
+        int npred() const
+        # Get number of block successors (number of xrefs from the block)
+        int nsucc() const
+        # Get predecessor number N
+        int pred(int n) const
+        # Get successor number N
+        int succ(int n) const
+
+        # mblock_t() = delete; -- not needed in Cython
+        # Virtual destructor
+        # (Cython: use 'cppclass' destructor syntax)
+        # ~mblock_t();
+        # HEXRAYS_MEMORY_ALLOCATION_FUNCS() -- not needed in Cython
+        # Check if block is empty
+        bint empty() const
+
+        # Print block contents.
+        # \param vp print helpers class. it can be used to direct the printed info to any destination
+        # void print(vd_printer_t &vp) const
+
+        # Dump block info.
+        # This function is useful for debugging, see mba_t::dump for info
+        void dump() const
+        # Print block with va_list
+        void vdump_block(const char *title, ...) const
+        # Print block with variadic arguments
+        void dump_block(const char *title, ...) const
+
+        #-----------------------------------------------------------------------
+        # Functions to insert/remove insns during the microcode optimization phase.
+        # See codegen_t, microcode_filter_t, udcall_t classes for the initial
+        # microcode generation.
+        #-----------------------------------------------------------------------
+        # Insert instruction into the doubly linked list
+        # \param nm new instruction
+        # \param om existing instruction, part of the doubly linked list
+        #           if nullptr, then the instruction will be inserted at the beginning
+        # NM will be inserted immediately after OM
+        # \return pointer to NM
+        minsn_t *insert_into_block(minsn_t *nm, minsn_t *om)
+
+        # Remove instruction from the doubly linked list
+        # \param m instruction to remove
+        # The removed instruction is not deleted, the caller gets its ownership
+        # \return pointer to the next instruction
+        minsn_t *remove_from_block(minsn_t *m)
+
+        #-----------------------------------------------------------------------
+        # Iterator over instructions and operands
+        #-----------------------------------------------------------------------
+        # Visit all instructions.
+        # This function visits subinstructions too.
+        # \param mv instruction visitor
+        # \return zero or the value returned by mv.visit_insn()
+        # See also mba_t::for_all_topinsns()
+        #int for_all_insns(minsn_visitor_t &mv)
+
+        # Visit all operands.
+        # This function visit subinstruction operands too.
+        # \param mv operand visitor
+        # \return zero or the value returned by mv.visit_mop()
+        #int for_all_ops(mop_visitor_t &mv)
+
+        # Visit all operands that use LIST.
+        # \param list ptr to the list of locations. it may be modified:
+        #             parts that get redefined by the instructions in [i1,i2)
+        #             will be deleted.
+        # \param i1   starting instruction. must be a top level insn.
+        # \param i2   ending instruction (excluded). must be a top level insn.
+        # \param mmv  operand visitor
+        # \return zero or the value returned by mmv.visit_mop()
+        # int for_all_uses(
+        #         mlist_t *list,
+        #         minsn_t *i1,
+        #         minsn_t *i2,
+        #         mlist_mop_visitor_t &mmv)
+
+        #-----------------------------------------------------------------------
+        # Optimization functions
+        #-----------------------------------------------------------------------
+        # Optimize one instruction in the context of the block.
+        # \param m pointer to a top level instruction
+        # \param optflags combination of \ref OPTI_ bits
+        # \return number of changes made to the block
+        # This function may change other instructions in the block too.
+        # However, it will not destroy top level instructions (it may convert them
+        # to nop's). This function performs only intrablock modifications.
+        # See also minsn_t::optimize_solo()
+        int optimize_insn(minsn_t *m, int optflags=OPTI.MINSTKREF|OPTI.COMBINSNS)
+
+        # Optimize a basic block.
+        # Usually there is no need to call this function explicitly because the
+        # decompiler will call it itself if optinsn_t::func or optblock_t::func
+        # return non-zero.
+        # \return number of changes made to the block
+        int optimize_block()
+
+        # Build def-use lists and eliminate deads.
+        # \param kill_deads do delete dead instructions?
+        # \return the number of eliminated instructions
+        # Better mblock_t::call make_lists_ready() rather than this function.
+        int build_lists(bint kill_deads)
+
+        # Remove a jump at the end of the block if it is useless.
+        # This function preserves any side effects when removing a useless jump.
+        # Both conditional and unconditional jumps are handled (and jtbl too).
+        # This function deletes useless jumps, not only replaces them with a nop.
+        # (please note that \optimize_insn does not handle useless jumps).
+        # \return number of changes made to the block
+        int optimize_useless_jump()
+
+        #-----------------------------------------------------------------------
+        # Functions that build with use/def lists. These lists are used to
+        # reprsent list of registers and stack locations that are either modified
+        # or accessed by microinstructions.
+        #-----------------------------------------------------------------------
+        # Append use-list of an operand.
+        # This function calculates list of locations that may or must be used
+        # by the operand and appends it to LIST.
+        # \param list    ptr to the output buffer. we will append to it.
+        # \param op      operand to calculate the use list of
+        # \param maymust should we calculate 'may-use' or 'must-use' list?
+        #                see \ref maymust_t for more details.
+        # \param mask    if only part of the operand should be considered,
+        #                a bitmask can be used to specify which part.
+        #                example: op=AX,mask=0xFF means that we will consider only AL.
+        # void append_use_list(
+        #         mlist_t *list,
+        #         mop_t& op,
+        #         maymust_t maymust,
+        #         bitrange_t mask=MAXRANGE) const
+
+        # Append def-list of an operand.
+        # This function calculates list of locations that may or must be modified
+        # by the operand and appends it to LIST.
+        # \param list    ptr to the output buffer. we will append to it.
+        # \param op      operand to calculate the def list of
+        # \param maymust should we calculate 'may-def' or 'must-def' list?
+        #                see \ref maymust_t for more details.
+        # void append_def_list(
+        #         mlist_t *list,
+        #         mop_t& op,
+        #         maymust_t maymust) const
+
+        # Build use-list of an instruction.
+        # This function calculates list of locations that may or must be used
+        # by the instruction. Examples:
+        #   "ldx ds.2, eax.4, ebx.4", may-list: all aliasable memory
+        #   "ldx ds.2, eax.4, ebx.4", must-list: empty
+        # Since LDX uses EAX for indirect access, it may access any aliasable
+        # memory. On the other hand, we cannot tell for sure which memory cells
+        # will be accessed, this is why the must-list is empty.
+        # \param ins     instruction to calculate the use list of
+        # \param maymust should we calculate 'may-use' or 'must-use' list?
+        #                see \ref maymust_t for more details.
+        # \return the calculated use-list
+        # mlist_t build_use_list(const minsn_t& ins, maymust_t maymust) const
+
+        # Build def-list of an instruction.
+        # This function calculates list of locations that may or must be modified
+        # by the instruction. Examples:
+        #   "stx ebx.4, ds.2, eax.4", may-list: all aliasable memory
+        #   "stx ebx.4, ds.2, eax.4", must-list: empty
+        # Since STX uses EAX for indirect access, it may modify any aliasable
+        # memory. On the other hand, we cannot tell for sure which memory cells
+        # will be modified, this is why the must-list is empty.
+        # \param ins     instruction to calculate the def list of
+        # \param maymust should we calculate 'may-def' or 'must-def' list?
+        #                see \ref maymust_t for more details.
+        # \return the calculated def-list
+        # mlist_t build_def_list(const minsn_t& ins, maymust_t maymust) const
+
+        #-----------------------------------------------------------------------
+        # The use/def lists can be used to search for interesting instructions
+        #-----------------------------------------------------------------------
+        # Is the list used by the specified instruction range?
+        # \param list list of locations. LIST may be modified by the function:
+        #             redefined locations will be removed from it.
+        # \param i1   starting instruction of the range (must be a top level insn)
+        # \param i2   end instruction of the range (must be a top level insn)
+        #             i2 is excluded from the range. it can be specified as nullptr.
+        #             i1 and i2 must belong to the same block.
+        # \param maymust should we search in 'may-access' or 'must-access' mode?
+        # bint is_used(mlist_t *list, const minsn_t *i1, const minsn_t *i2, maymust_t maymust=MAY_ACCESS) const
+
+        # Find the first insn that uses the specified list in the insn range.
+        # \param list list of locations. LIST may be modified by the function:
+        #             redefined locations will be removed from it.
+        # \param i1   starting instruction of the range (must be a top level insn)
+        # \param i2   end instruction of the range (must be a top level insn)
+        #             i2 is excluded from the range. it can be specified as nullptr.
+        #             i1 and i2 must belong to the same block.
+        # \param maymust should we search in 'may-access' or 'must-access' mode?
+        # \return pointer to such instruction or nullptr.
+        #         Upon return LIST will contain only locations not redefined
+        #         by insns [i1..result]
+        # const minsn_t *find_first_use(mlist_t *list, const minsn_t *i1, const minsn_t *i2, maymust_t maymust=MAY_ACCESS) const
+        # minsn_t *find_first_use(mlist_t *list, minsn_t *i1, const minsn_t *i2, maymust_t maymust=MAY_ACCESS) const
+
+        # Is the list redefined by the specified instructions?
+        # \param list list of locations to check.
+        # \param i1   starting instruction of the range (must be a top level insn)
+        # \param i2   end instruction of the range (must be a top level insn)
+        #             i2 is excluded from the range. it can be specified as nullptr.
+        #             i1 and i2 must belong to the same block.
+        # \param maymust should we search in 'may-access' or 'must-access' mode?
+        # bint is_redefined(
+        #         const mlist_t &list,
+        #         const minsn_t *i1,
+        #         const minsn_t *i2,
+        #         maymust_t maymust=MAY_ACCESS) const
+
+        # Find the first insn that redefines any part of the list in the insn range.
+        # \param list list of locations to check.
+        # \param i1   starting instruction of the range (must be a top level insn)
+        # \param i2   end instruction of the range (must be a top level insn)
+        #             i2 is excluded from the range. it can be specified as nullptr.
+        #             i1 and i2 must belong to the same block.
+        # \param maymust should we search in 'may-access' or 'must-access' mode?
+        # \return pointer to such instruction or nullptr.
+        # const minsn_t *find_redefinition(
+        #         const mlist_t &list,
+        #         const minsn_t *i1,
+        #         const minsn_t *i2,
+        #         maymust_t maymust=MAY_ACCESS) const
+        # minsn_t *find_redefinition(
+        #         const mlist_t &list,
+        #         minsn_t *i1,
+        #         const minsn_t *i2,
+        #         maymust_t maymust=MAY_ACCESS) const
+
+        # Is the right hand side of the instruction redefined the insn range?
+        # "right hand side" corresponds to the source operands of the instruction.
+        # \param ins instruction to consider
+        # \param i1   starting instruction of the range (must be a top level insn)
+        # \param i2   end instruction of the range (must be a top level insn)
+        #             i2 is excluded from the range. it can be specified as nullptr.
+        #             i1 and i2 must belong to the same block.
+        bint is_rhs_redefined(const minsn_t *ins, const minsn_t *i1, const minsn_t *i2) const
+
+        # Find the instruction that accesses the specified operand.
+        # This function search inside one block.
+        # \param op     operand to search for
+        # \param[in,out] parent ptr to ptr to a top level instruction.
+        #               in: denotes the beginning of the search range.
+        #               out: denotes the parent of the found instruction.
+        # \param mend   end instruction of the range (must be a top level insn)
+        #               mend is excluded from the range. it can be specified as nullptr.
+        #               parent and mend must belong to the same block.
+        # \param fdflags combination of \ref FD_ bits
+        # \return       the instruction that accesses the operand. this instruction
+        #               may be a sub-instruction. to find out the top level
+        #               instruction, check out *parent.
+        #               nullptr means 'not found'.
+        minsn_t *find_access(
+                mop_t& op,
+                minsn_t **parent,
+                const minsn_t *mend,
+                int fdflags) const
+
+        # FD_ bits for mblock_t::find_access
+        #define FD_BACKWARD 0x0000  # search direction
+        #define FD_FORWARD  0x0001  # search direction
+        #define FD_USE      0x0000  # look for use
+        #define FD_DEF      0x0002  # look for definition
+        #define FD_DIRTY    0x0004  # ignore possible implicit definitions by function calls and indirect memory access
+
+        # Convenience functions:
+        minsn_t *find_def(
+                mop_t& op,
+                minsn_t **p_i1,
+                const minsn_t *i2,
+                int fdflags)
+        minsn_t *find_use(
+                mop_t& op,
+                minsn_t **p_i1,
+                const minsn_t *i2,
+                int fdflags)
+
+        # Find possible values for a block.
+        # \param res     set of value ranges
+        # \param vivl    what to search for
+        # \param vrflags combination of \ref VR_ bits
+        # bint get_valranges(
+        #         valrng_t *res,
+        #         vivl_t& vivl,
+        #         int vrflags) const
+
+        # Find possible values for an instruction.
+        # \param res     set of value ranges
+        # \param vivl    what to search for
+        # \param m       insn to search value ranges at. \sa VR_ bits
+        # \param vrflags combination of \ref VR_ bits
+        # bint get_valranges(
+        #         valrng_t *res,
+        #         vivl_t& vivl,
+        #         const minsn_t *m,
+        #         int vrflags) const
+
+        # VR_ bits for get_valranges
+        #define VR_AT_START 0x0000    # get value ranges before the instruction or at the block start (if M is nullptr)
+        #define VR_AT_END   0x0001    # get value ranges after the instruction or at the block end, just after the last instruction (if M is nullptr)
+        #define VR_EXACT    0x0002    # find exact match. if not set, the returned valrng size will be >= vivl.size
+
+        # Erase the instruction (convert it to nop) and mark the lists dirty.
+        # This is the recommended function to use because it also marks the block use-def lists dirty.
+        void make_nop(minsn_t *m)
+
+        # Calculate number of regular instructions in the block.
+        # Assertions are skipped by this function.
+        # \return Number of non-assertion instructions in the block.
+        size_t get_reginsn_qty() const
+
+        # Check if block is a call block
+        bint is_call_block() const
+        # Check if block is an unknown call
+        bint is_unknown_call() const
+        # Check if block is nway
+        bint is_nway() const
+        # Check if block is a branch
+        bint is_branch() const
+        # Check if block is a simple goto block
+        bint is_simple_goto_block() const
+        # Check if block is a simple jcnd block
+        bint is_simple_jcnd_block() const
+
+    #-------------------------------------------------------------------------
+    # Warning ids
+    cdef enum warnid_t:
+        WARN_VARARG_REGS         #  0 cannot handle register arguments in vararg function, discarded them
+        WARN_ILL_PURGED          #  1 odd caller purged bytes %d, correcting
+        WARN_ILL_FUNCTYPE        #  2 invalid function type '%s' has been ignored
+        WARN_VARARG_TCAL         #  3 cannot handle tail call to vararg
+        WARN_VARARG_NOSTK        #  4 call vararg without local stack
+        WARN_VARARG_MANY         #  5 too many varargs, some ignored
+        WARN_ADDR_OUTARGS        #  6 cannot handle address arithmetics in outgoing argument area of stack frame -- unused
+        WARN_DEP_UNK_CALLS       #  7 found interdependent unknown calls
+        WARN_ILL_ELLIPSIS        #  8 erroneously detected ellipsis type has been ignored
+        WARN_GUESSED_TYPE        #  9 using guessed type %s;
+        WARN_EXP_LINVAR          # 10 failed to expand a linear variable
+        WARN_WIDEN_CHAINS        # 11 failed to widen chains
+        WARN_BAD_PURGED          # 12 inconsistent function type and number of purged bytes
+        WARN_CBUILD_LOOPS        # 13 too many cbuild loops
+        WARN_NO_SAVE_REST        # 14 could not find valid save-restore pair for %s
+        WARN_ODD_INPUT_REG       # 15 odd input register %s
+        WARN_ODD_ADDR_USE        # 16 odd use of a variable address
+        WARN_MUST_RET_FP         # 17 function return type is incorrect (must be floating point)
+        WARN_ILL_FPU_STACK       # 18 inconsistent fpu stack
+        WARN_SELFREF_PROP        # 19 self-referencing variable has been detected
+        WARN_WOULD_OVERLAP       # 20 variables would overlap: %s
+        WARN_ARRAY_INARG         # 21 array has been used for an input argument
+        WARN_MAX_ARGS            # 22 too many input arguments, some ignored
+        WARN_BAD_FIELD_TYPE      # 23 incorrect structure member type for %s::%s, ignored
+        WARN_WRITE_CONST         # 24 write access to const memory at %a has been detected
+        WARN_BAD_RETVAR          # 25 wrong return variable
+        WARN_FRAG_LVAR           # 26 fragmented variable at %s may be wrong
+        WARN_HUGE_STKOFF         # 27 exceedingly huge offset into the stack frame
+        WARN_UNINITED_REG        # 28 reference to an uninitialized register has been removed: %s
+        WARN_FIXED_INSN          # 29 fixed broken insn
+        WARN_WRONG_VA_OFF        # 30 wrong offset of va_list variable
+        WARN_CR_NOFIELD          # 31 CONTAINING_RECORD: no field '%s' in struct '%s' at %d
+        WARN_CR_BADOFF           # 32 CONTAINING_RECORD: too small offset %d for struct '%s'
+        WARN_BAD_STROFF          # 33 user specified stroff has not been processed: %s
+        WARN_BAD_VARSIZE         # 34 inconsistent variable size for '%s'
+        WARN_UNSUPP_REG          # 35 unsupported processor register '%s'
+        WARN_UNALIGNED_ARG       # 36 unaligned function argument '%s'
+        WARN_BAD_STD_TYPE        # 37 corrupted or unexisting local type '%s'
+        WARN_BAD_CALL_SP         # 38 bad sp value at call
+        WARN_MISSED_SWITCH       # 39 wrong markup of switch jump, skipped it
+        WARN_BAD_SP              # 40 positive sp value %a has been found
+        WARN_BAD_STKPNT          # 41 wrong sp change point
+        WARN_UNDEF_LVAR          # 42 variable '%s' is possibly undefined
+        WARN_JUMPOUT             # 43 control flows out of bounds
+        WARN_BAD_VALRNG          # 44 values range analysis failed
+        WARN_BAD_SHADOW          # 45 ignored the value written to the shadow area of the succeeding call
+        WARN_OPT_VALRNG          # 46 conditional instruction was optimized away because %s
+        WARN_RET_LOCREF          # 47 returning address of temporary local variable '%s'
+        WARN_BAD_MAPDST          # 48 too short map destination '%s' for variable '%s'
+        WARN_BAD_INSN            # 49 bad instruction
+        WARN_ODD_ABI             # 50 encountered odd instruction for the current ABI
+        WARN_UNBALANCED_STACK    # 51 unbalanced stack, ignored a potential tail call
+        WARN_OPT_VALRNG2         # 52 mask 0x%X is shortened because %s <= 0x%X"
+        WARN_OPT_VALRNG3         # 53 masking with 0X%X was optimized away because %s <= 0x%X
+        WARN_OPT_USELESS_JCND    # 54 simplified comparisons for '%s': %s became %s
+        WARN_SUBFRAME_OVERFLOW   # 55 call arguments overflow the function chunk frame
+        WARN_MAX                 # may be used in notes as a placeholder when the
+                                 # warning id is not available
+
+    # Warning instances
+    cdef cppclass hexwarn_t:
+        ea_t ea        #< Address where the warning occurred
+        warnid_t id    #< Warning id
+        qstring text   #< Fully formatted text of the warning
+
+    ctypedef qvector[hexwarn_t] hexwarns_t
+
+    #-------------------------------------------------------------------------
+    # Microcode maturity levels
+    cpdef enum class mba_maturity_t:
+        MMAT_ZERO = 0         # microcode does not exist
+        MMAT_GENERATED = 1    # generated microcode
+        MMAT_PREOPTIMIZED = 2 # preoptimized pass is complete
+        MMAT_LOCOPT = 3       # local optimization of each basic block is complete.
+                              # control flow graph is ready too.
+        MMAT_CALLS = 4        # detected call arguments. see also hxe_calls_done
+        MMAT_GLBOPT1 = 5      # performed the first pass of global optimization
+        MMAT_GLBOPT2 = 6      # most global optimization passes are done
+        MMAT_GLBOPT3 = 7      # completed all global optimization. microcode is fixed now.
+        MMAT_LVARS = 8        # allocated local variables
+
+
+    # -------------------------------------------------------------------------
+    # Ranges to decompile. Either a function or an explicit vector of ranges.
+    cdef cppclass mba_ranges_t:
+        func_t *pfn           # function to decompile. if not null, then function mode.
+        rangevec_t ranges     # snippet mode: ranges to decompile.
+                              # functions mode
+                              
+
+    cdef cppclass mba_t:
+        """
+        Array of micro blocks representing microcode for a decompiled function.
+        The first micro block is the entry point, the last one is the exit point.
+        The entry and exit blocks are always empty. The exit block is generated
+        at MMAT_LOCOPT maturity level.
+        """
+        
+        # flags for the mba_t
+        uint32 flags   # this is private in the original code
+        uint32 flags2  # this is private in the original code
+        
+        # Various flag-checking methods
+        bint precise_defeas() const
+        bint optimized() const
+        bint short_display() const
+        bint show_reduction() const
+        bint graph_insns() const
+        bint loaded_gdl() const
+        bint should_beautify() const
+        bint rtype_refined() const
+        bint may_refine_rettype() const
+        bint use_wingraph32() const
+        bint display_numaddrs() const
+        bint display_valnums() const
+        bint is_pattern() const
+        bint is_thunk() const
+        bint saverest_done() const
+        bint callinfo_built() const
+        bint really_alloc() const
+        bint lvars_allocated() const
+        bint chain_varnums_ok() const
+        bint returns_fpval() const
+        bint has_passregs() const
+        bint generated_asserts() const
+        bint propagated_asserts() const
+        bint deleted_pairs() const
+        bint common_stkvars_stkargs() const
+        bint lvar_names_ok() const
+        bint lvars_renamed() const
+        bint has_over_chains() const
+        bint valranges_done() const
+        bint argidx_ok() const
+        bint argidx_sorted() const
+        bint code16_bit_removed() const
+        bint has_stack_retval() const
+        bint has_outlines() const
+        bint is_ctr() const
+        bint is_dtr() const
+        bint is_cdtr() const
+        bint prop_complex() const
+        int get_mba_flags() const
+        int get_mba_flags2() const
+        void set_mba_flags(int f)
+        void clr_mba_flags(int f)
+        void set_mba_flags2(int f)
+        void clr_mba_flags2(int f)
+        void clr_cdtr()
+        int calc_shins_flags() const
+        
+        #
+        #                      +-----------+ <- inargtop
+        #                      |   prmN    |
+        #                      |   ...     | <- minargref
+        #                      |   prm0    |
+        #                      +-----------+ <- inargoff
+        #                      |shadow_args|
+        #                      +-----------+
+        #                      |  retaddr  |
+        #      frsize+frregs   +-----------+ <- initial esp  |
+        #                      |  frregs   |                 |
+        #            +frsize   +-----------+ <- typical ebp  |
+        #                      |           |  |              |
+        #                      |           |  | fpd          |
+        #                      |           |  |              |
+        #                      |  frsize   | <- current ebp  |
+        #                      |           |                 |
+        #                      |           |                 |
+        #                      |           |                 | stacksize
+        #                      |           |                 |
+        #                      |           |                 |
+        #                      |           | <- minstkref    |
+        #  stkvar base off 0   +---..      |                 |    | current
+        #                      |           |                 |    | stack
+        #                      |           |                 |    | pointer
+        #                      |           |                 |    | range
+        #                      |tmpstk_size|                 |    | (what getspd() returns)
+        #                      |           |                 |    |
+        #                      |           |                 |    |
+        #                      +-----------+ <- minimal sp   |    | offset 0 for the decompiler (vd)
+        #
+        # There is a detail that may add confusion when working with stack variables.
+        # The decompiler does not use the same stack offsets as IDA.
+        # The picture above should explain the difference:
+        # - IDA stkoffs are displayed on the left, decompiler stkoffs - on the right
+        # - Decompiler stkoffs are always >= 0
+        # - IDA stkoff==0 corresponds to stkoff==tmpstk_size in the decompiler
+        # - See stkoff_vd2ida and stkoff_ida2vd below to convert IDA stkoffs to vd stkoff
+
+        # Convert a stack offset used in vd to a stack offset used in ida stack frame
+        sval_t stkoff_vd2ida(sval_t off) const
+        
+        # Convert a ida stack frame offset to a stack offset used in vd
+        sval_t stkoff_ida2vd(sval_t off) const
+        sval_t argbase() const                
+        
+        # Stack argument check
+        bint is_stkarg(const lvar_t &v) const   
+        
+        ssize_t get_stkvar(
+            udm_t *udm,
+            sval_t vd_stkoff,
+            uval_t *p_idaoff = NULL,
+            tinfo_t *p_frame = NULL) const
+
+        argloc_t get_ida_argloc(const lvar_t &v) const      
+          
+        # Data members    
+        mba_ranges_t mbr
+        ea_t entry_ea
+        ea_t last_prolog_ea
+        ea_t first_epilog_ea
+        int qty                  #< number of basic blocks
+        int npurged              #< -1 - unknown
+        cm_t cc                  #< calling convention
+        sval_t tmpstk_size       #< size of the temporary stack part
+                                 #< (which dynamically changes with push/pops)
+        sval_t frsize            #< size of local stkvars range in the stack frame
+        sval_t frregs            #< size of saved registers range in the stack frame
+        sval_t fpd               #< frame pointer delta
+        int pfn_flags            #< copy of func_t::flags
+        int retsize              #< size of return address in the stack frame
+        int shadow_args          #< size of shadow argument area
+        sval_t fullsize          #< Full stack size including incoming args
+        sval_t stacksize         #: The maximal size of the function stack including
+                                 #: bytes allocated for outgoing call arguments
+                                 #: (up to retaddr)
+        sval_t inargoff          #: offset of the first stack argument;
+                                 #: after fix_scattered_movs() INARGOFF may
+                                 #: be less than STACKSIZE
+        sval_t minstkref         #: The lowest stack location whose address was taken
+        ea_t minstkref_ea        #: address with lowest minstkref (for debugging)
+        sval_t minargref         #: The lowest stack argument location whose address was taken
+                                 #: This location and locations above it can be aliased
+                                 #: It controls locations >= inargoff-shadow_args
+        sval_t spd_adjust        #: If sp>0, the max positive sp value
+        ivlset_t gotoff_stkvars  #: stkvars that hold .got offsets. considered to be unaliasable
+        ivlset_t restricted_memory
+        ivlset_t aliased_memory  #: aliased_memory+restricted_memory=ALLMEM
+        mlist_t nodel_memory     #: global dead elimination may not delete references to this area
+        rlist_t consumed_argregs #: registers converted into stack arguments, should not be used as arguments
+
+        mba_maturity_t maturity  #: current maturity level
+        mba_maturity_t reqmat    #: required maturity level
+
+        bint final_type          #: is the function type final? (specified by the user)
+        tinfo_t idb_type         #: function type as retrieved from the database
+        reginfovec_t idb_spoiled #: MBA_SPLINFO && final_type: info in ida format
+        mlist_t spoiled_list     #: MBA_SPLINFO && !final_type: info in vd format
+        int fti_flags            #: FTI_... constants for the current function
+
+        qstring label             #: name of the function or pattern (colored)
+        lvars_t vars              #: local variables
+        intvec_t argidx           #: input arguments (indexes into 'vars')
+        int retvaridx             #: index of variable holding the return value
+                                  #: -1 means none
+
+        ea_t error_ea             #: during microcode generation holds ins.ea
+        qstring error_strarg
+
+        mblock_t* blocks          #: double linked list of blocks
+        mblock_t** natural        #: natural order of blocks
+
+        ivl_with_name_t[6] std_ivls #: we treat memory as consisting of 6 parts
+                                    #: see \ref memreg_index_t
+
+        # 'mutable' is a C++ keyword, not needed in Cython
+        hexwarns_t notes
+        uchar[32] occurred_warns  # occurred warning messages    
+        
+         # Warning checks
+        # Was a write to a constant detected?
+        bint write_to_const_detected() const
+
+        # Was a bad call stack pointer detected?
+        bint bad_call_sp_detected() const
+
+        # Are register arguments not aligned?
+        bint regargs_is_not_aligned() const
+
+        # Does this have a bad stack pointer?
+        bint has_bad_sp() const
+        
+        char reserved[]
+        
+        void term()
+        # Get the current function.
+        func_t *get_curfunc() const
+
+        # Does this use a frame?
+        bint use_frame() const
+
+        # Does the range contain the given address?
+        bint range_contains(ea_t ea) const
+
+        # Is this a snippet?
+        bint is_snippet() const
+
+        # Set maturity level.
+        # \param mat new maturity level
+        # \return true if it is time to stop analysis
+        # Plugins may use this function to skip some parts of the analysis.
+        # The maturity level cannot be decreased.
+        bint set_maturity(mba_maturity_t mat)
+        
+        # Optimize each basic block locally
+        # \param locopt_bits combination of \ref LOCOPT_ bits
+        # \return number of changes. 0 means nothing changed
+        # This function is called by the decompiler, usually there is no need to
+        # call it explicitly.
+        int optimize_local(int locopt_bits)
+    
+        # Build control flow graph.
+        # This function may be called only once. It calculates the type of each
+        # basic block and the adjacency list. optimize_local() calls this function
+        # if necessary. You need to call this function only before MMAT_LOCOPT.
+        # \return error code
+        merror_t build_graph()
+
+        # Get control graph.
+        # Call build_graph() if you need the graph before MMAT_LOCOPT.
+        # mbl_graph_t *get_graph()
+
+        # Analyze calls and determine calling conventions.
+        # \param acflags permitted actions that are necessary for successful detection
+        #                of calling conventions. See \ref ACFL_
+        # \return number of calls. -1 means error.
+        int analyze_calls(int acflags)
+        
+        # Optimize microcode globally.
+        # This function applies various optimization methods until we reach the
+        # fixed point. After that it preallocates lvars unless reqmat forbids it.
+        # \return error code
+        # merror_t optimize_global()
+
+        # Allocate local variables.
+        # Must be called only immediately after optimize_global(), with no
+        # modifications to the microcode. Converts registers,
+        # stack variables, and similar operands into mop_l. This call will not fail
+        # because all necessary checks were performed in optimize_global().
+        # After this call the microcode reaches its final state.
+        void alloc_lvars()
+
+        # Dump microcode to a file.
+        # The file will be created in the directory pointed by IDA_DUMPDIR envvar.
+        # Dump will be created only if IDA is run under debugger.
+        void dump() const
+        void vdump_mba(bint _verify, const char *title, va_list va) const
+        void dump_mba(bint _verify, const char *title, ...) const
+
+        # Print microcode to any destination.
+        # \param vp print sink
+        # void print(vd_printer_t &vp) const
+
+        # Verify microcode consistency.
+        # \param always if false, the check will be performed only if ida runs
+        #               under debugger
+        # If any inconsistency is discovered, an internal error will be generated.
+        # We strongly recommend you to call this function before returing control
+        # to the decompiler from your callbacks, in the case if you modified
+        # the microcode. If the microcode is inconsistent, this function will
+        # generate an internal error. We provide the source code of this function
+        # in the plugins/hexrays_sdk/verifier directory for your reference.
+        void verify(bint always) const
+        
+        # Mark the microcode use-def chains dirty.
+        # Call this function if any inter-block data dependencies got changed
+        # because of your modifications to the microcode. Failing to do so may
+        # cause an internal error.
+        void mark_chains_dirty()
+        
+        # Get basic block by its serial number.
+        mblock_t *get_mblock(uint n)
+        
+        # Insert a block in the middle of the mbl array.
+        # The very first block of microcode must be empty, it is the entry block.
+        # The very last block of microcode must be BLT_STOP, it is the exit block.
+        # Therefore inserting a new block before the entry point or after the exit
+        # block is not a good idea.
+        # \param bblk the new block will be inserted before BBLK
+        # \return ptr to the new block
+        mblock_t *insert_block(int bblk)
+
+        # Split a block: insert a new one after the block, move some instructions
+        # to new block
+        # \param blk        block to be split
+        # \param start_insn all instructions to be moved to new block: starting with this one up to the end
+        # \return ptr to the new block
+        mblock_t *split_block(mblock_t *blk, minsn_t *start_insn)
+
+        # Delete a block.
+        # \param blk block to delete
+        # \return true if at least one of the other blocks became empty or unreachable
+        bint remove_block(mblock_t *blk)
+        bint remove_blocks(int start_blk, int end_blk)
+
+        # Make a copy of a block.
+        # This function makes a simple copy of the block. It does not fix the
+        # predecessor and successor lists, they must be fixed if necessary.
+        # \param blk         block to copy
+        # \param new_serial  position of the copied block
+        # \param cpblk_flags combination of \ref CPBLK_... bits
+        # \return pointer to the new copy
+        mblock_t *copy_block(mblock_t *blk, int new_serial, int cpblk_flags)
+
+        # Delete all empty and unreachable blocks.
+        # Blocks marked with MBL_KEEP won't be deleted.
+        bint remove_empty_and_unreachable_blocks()
+
+        # Merge blocks.
+        # This function merges blocks constituting linear flow.
+        # It calls remove_empty_and_unreachable_blocks() as well.
+        # \return true if changed any blocks
+        bint merge_blocks()
+
+        # Visit all operands of all instructions.
+        # \param mv operand visitor
+        # \return non-zero value returned by mv.visit_mop() or zero
+        #int for_all_ops(mop_visitor_t &mv)
+
+        # Visit all instructions.
+        # This function visits all instruction and subinstructions.
+        # \param mv instruction visitor
+        # \return non-zero value returned by mv.visit_mop() or zero
+        # int for_all_insns(minsn_visitor_t &mv)
+
+        # Visit all top level instructions.
+        # \param mv instruction visitor
+        # \return non-zero value returned by mv.visit_mop() or zero
+        # int for_all_topinsns(minsn_visitor_t &mv)
+
+        # Find an operand in the microcode.
+        # This function tries to find the operand that matches LIST.
+        # Any operand that overlaps with LIST is considered as a match.
+        # \param[out] ctx context information for the result
+        # \param ea       desired address of the operand. BADADDR means to accept any address.
+        # \param is_dest  search for destination operand? this argument may be
+        #                 ignored if the exact match could not be found
+        # \param list     list of locations the correspond to the operand
+        # \return pointer to the operand or nullptr.
+        # mop_t *find_mop(op_parent_info_t *ctx, ea_t ea, bint is_dest, const mlist_t &list)
+
+        # Create a call of a helper function.
+        # \param ea       The desired address of the instruction
+        # \param helper   The helper name
+        # \param rettype  The return type (nullptr or empty type means 'void')
+        # \param callargs The helper arguments (nullptr-no arguments)
+        # \param out      The operand where the call result should be stored.
+        #                 If this argument is not nullptr, "mov helper_call(), out"
+        #                 will be generated. Otherwise "call helper()" will be
+        #                 generated. Note: the size of this operand must be equal
+        #                 to the RETTYPE size
+        # \return pointer to the created instruction or nullptr if error
+        minsn_t *create_helper_call(
+            ea_t ea,
+            const char *helper,
+            const tinfo_t *rettype = NULL,
+            const mcallargs_t *callargs = NULL,
+            const mop_t *out = NULL)
+
+        # Prepare the lists of registers & memory that are defined/killed by a
+        # function
+        # \param[out] return_regs  defined regs to return (eax,edx)
+        # \param[out] spoiled      spoiled regs (flags,ecx,mem)
+        # \param      type         the function type
+        # \param      call_ea      the call insn address (if known)
+        # \param      tail_call    is it the tail call?
+        void get_func_output_lists(
+            mlist_t *return_regs,
+            mlist_t *spoiled,
+            const tinfo_t &type,
+            ea_t call_ea = BADADDR,
+            bint tail_call = False)
+
+        # Get input argument of the decompiled function.
+        # \param n argument number (0..nargs-1)
+        lvar_t &arg(int n)
+        const lvar_t &arg(int n) const
+
+        # Allocate a fictional address.
+        # This function can be used to allocate a new unique address for a new
+        # instruction, if re-using any existing address leads to conflicts.
+        # For example, if the last instruction of the function modifies R0
+        # and falls through to the next function, it will be a tail call:
+        #    LDM R0!, {R4,R7}
+        #    end of the function
+        #    start of another function
+        # In this case R0 generates two different lvars at the same address:
+        #   - one modified by LDM
+        #   - another that represents the return value from the tail call
+        #
+        # Another example: a third-party plugin makes a copy of an instruction.
+        # This may lead to the generation of two variables at the same address.
+        # Example 3: fictional addresses can be used for new instructions created
+        # while modifying the microcode.
+        # This function can be used to allocate a new unique address for a new
+        # instruction or a variable.
+        # The fictional address is selected from an unallocated address range.
+        # \param real_ea real instruction address (BADADDR is ok too)
+        # \return a unique fictional address
+        ea_t alloc_fict_ea(ea_t real_ea)
+
+        # Resolve a fictional address.
+        # This function provides a reverse of the mapping made by alloc_fict_ea().
+        # \param fict_ea fictional definition address
+        # \return the real instruction address
+        ea_t map_fict_ea(ea_t fict_ea) const
+
+        # Get information about various memory regions.
+        # We map the stack frame to the global memory, to some unused range.
+        # const ivl_t &get_std_region(memreg_index_t idx) const
+        # const ivl_t &get_lvars_region() const
+        # const ivl_t &get_shadow_region() const
+        # const ivl_t &get_args_region() const
+        # ivl_t get_stack_region() const
+
+        # Serialize mbl array into a sequence of bytes.
+        void serialize(bytevec_t &vout) const
+
+        # Deserialize a byte sequence into mbl array.
+        # \param bytes pointer to the beginning of the byte sequence.
+        # \param nbytes number of bytes in the byte sequence.
+        # \return new mbl array
+        @staticmethod
+        mba_t *deserialize(const uchar *bytes, size_t nbytes)
+
+        # Create and save microcode snapshot
+        void save_snapshot(const char *description)
+
+        # Allocate a kernel register.
+        # \param size size of the register in bytes
+        # \param check_size if true, only the sizes that correspond to a size of
+        #                   a basic type will be accepted.
+        # \return allocated register. mr_none means failure.
+        mreg_t alloc_kreg(size_t size, bint check_size = True)
+
+        # Free a kernel register.
+        # If wrong arguments are passed, this function will generate an internal error.
+        # \param reg a previously allocated kernel register
+        # \param size size of the register in bytes
+        void free_kreg(mreg_t reg, size_t size)
+
+        # \defgroup INLINE_ inline_func() flags
+        #define INLINE_EXTFRAME 0x0001 # Inlined function has its own (external) frame
+        #define INLINE_DONTCOPY 0x0002 # Do not reuse old inlined copy even if it exists
+
+        # Inline a range.
+        # Currently only functions are supported, not arbitrary ranges.
+        # This function may be called only during the initial microcode generation phase.
+        # \param cdg the codegenerator object
+        # \param blknum the block contaning the call/jump instruction to inline
+        # \param ranges the set of ranges to inline
+        # \param decomp_flags combination of \ref DECOMP_ bits
+        # \param inline_flags combination of \ref INLINE_ bits
+        # \return error code
+        # merror_t inline_func(
+        #     codegen_t &cdg,
+        #     int blknum,
+        #     mba_ranges_t &ranges,
+        #     int decomp_flags = 0,
+        #     int inline_flags = 0)
+
+        # Find a sp change point.
+        # returns stkpnt p, where p->ea <= ea
+        # const stkpnt_t *locate_stkpnt(ea_t ea) const
+
+        bint set_lvar_name(lvar_t &v, const char *name, int flagbits)
+        bint set_nice_lvar_name(lvar_t &v, const char *name)
+        bint set_user_lvar_name(lvar_t &v, const char *name)        
+        
+    # --- Standalone SDK functions ---
+    cdef int get_mreg_name(qstring *out, mreg_t reg, int width, void *ud)
+
+cdef enum LOCOPT_FLAGS:
+    LOCOPT_ALL     = 0x0001  # redo optimization for all blocks. if this bit is not set, only dirty blocks will be optimized
+    LOCOPT_REFINE  = 0x0002  # refine return type, ok to fail
+    LOCOPT_REFINE2 = 0x0004  # refine return type, try harder
+
+cdef enum OPERAND_PROPERTIES:
+    OPROP_IMPDONE = 0x01   # imported operand (a pointer) has been dereferenced
+    OPROP_UDT     = 0x02   # a struct or union
+    OPROP_FLOAT   = 0x04   # possibly floating value
+    OPROP_CCFLAGS = 0x08   # mop_n: a pc-relative value
+                           # mop_a: an address obtained from a relocation
+                           # else: value of a condition code register (like mr_cc)
+    OPROP_UDEFVAL = 0x10   # uses undefined value
+    OPROP_LOWADDR = 0x20   # a low address offset    
+
+# Apparently, cannot initialize a Cython enum with extern constants.
+cdef enum MOPT:
+    ZERO          =  0   # none
+    REGISTER      =  1   # register (they exist until MMAT_LVARS)
+    NUMBER        =  2   # immediate number constant
+    STRING        =  3   # immediate string constant (user representation)
+    DEST_RESULT   =  4   # result of another instruction
+    STACK         =  5   # local stack variable (they exist until MMAT_LVARS)
+    GLOBAL        =  6   # global variable
+    MBLOCK        =  7   # micro basic block (mblock_t)
+    ARGUMENT_LIST =  8   # list of arguments
+    LOCAL         =  9   # local variable
+    ADDRESS       = 10   # mop_addr_t: address of operand (mop_l, mop_v, mop_S, mop_r)
+    HELPER        = 11   # helper function
+    CASES         = 12   # mcases
+    FLOAT         = 13   # floating point constant
+    PAIR          = 14   # operand pair
+    SCATTERED     = 15   # scattered    
+
+ctypedef mop_t* mop_t_ptr
+ctypedef shared_ptr[mop_t] shared_mop_t_ptr
+ctypedef pair[mop_t_ptr, uint64] mop_off_pair_t
+# Store propagated constants as 64-bit unsigned to avoid overflow
+ctypedef pair[uint64, int] const_val_t
+ctypedef map[qstring, const_val_t] CppConstMap
+
+    
+    
+# Declare the public function from our .pyx file
+cpdef run_dataflow_cython(object mba_py)
+
+
+# cpdef enum class OPROP_FLAGS(unsigned int):
+#     OPROP_IMPDONE  = 0x01 # imported operand (a pointer) has been dereferenced
+#     OPROP_UDT      = 0x02 # a struct or union
+#     OPROP_FLOAT    = 0x04 # possibly floating value
+#     OPROP_CCFLAGS  = 0x08 # mop_n: a pc-relative value
+#                             # mop_a: an address obtained from a relocation
+#                             # else: value of a condition code register (like mr_cc)
+#     OPROP_UDEFVAL  = 0x10 # uses undefined value
+#     OPROP_LOWADDR  = 0x20 # a low address offset
+        
+# cpdef enum class MBL_FLAGS(unsigned int):
+#     MBL_PRIV        = 0x0001   # private block - no instructions except
+#                                 # the specified are accepted (used in patterns)
+#     MBL_NONFAKE     = 0x0000   # regular block
+#     MBL_FAKE        = 0x0002   # fake block
+#     MBL_GOTO        = 0x0004   # this block is a goto target
+#     MBL_TCAL        = 0x0008   # artificial call block for tail calls
+#     MBL_PUSH        = 0x0010   # needs "convert push/pop instructions"
+#     MBL_DMT64       = 0x0020   # needs "demote 64bits"
+#     MBL_COMB        = 0x0040   # needs "combine" pass
+#     MBL_PROP        = 0x0080   # needs 'propagation' pass
+#     MBL_DEAD        = 0x0100   # needs "eliminate deads" pass
+#     MBL_LIST        = 0x0200   # use/def lists are ready (not dirty)
+#     MBL_INCONST     = 0x0400   # inconsistent lists: we are building them
+#     MBL_CALL        = 0x0800   # call information has been built
+#     MBL_BACKPROP    = 0x1000   # performed backprop_cc
+#     MBL_NORET       = 0x2000   # dead end block: doesn't return execution control
+#     MBL_DSLOT       = 0x4000   # block for delay slot
+#     MBL_VALRANGES   = 0x8000   # should optimize using value ranges
+#     MBL_KEEP        = 0x10000  # do not remove even if unreachable
+#     MBL_INLINED     = 0x20000  # block was inlined, not originally part of mbr
+#     MBL_EXTFRAME    = 0x40000  # an inlined block with an external frame 
+
+
+# /// \defgroup CVAR_ Local variable property bits
+# /// Used in lvar_t::flags
+# ///@{
+# #define CVAR_USED    0x00000001 ///< is used in the code?
+# #define CVAR_TYPE    0x00000002 ///< the type is defined?
+# #define CVAR_NAME    0x00000004 ///< has nice name?
+# #define CVAR_MREG    0x00000008 ///< corresponding mregs were replaced?
+# #define CVAR_NOWD    0x00000010 ///< width is unknown
+# #define CVAR_UNAME   0x00000020 ///< user-defined name
+# #define CVAR_UTYPE   0x00000040 ///< user-defined type
+# #define CVAR_RESULT  0x00000080 ///< function result variable
+# #define CVAR_ARG     0x00000100 ///< function argument
+# #define CVAR_FAKE    0x00000200 ///< fake variable (return var or va_list)
+# #define CVAR_OVER    0x00000400 ///< overlapping variable
+# #define CVAR_FLOAT   0x00000800 ///< used in a fpu insn
+# #define CVAR_SPOILED 0x00001000 ///< internal flag, do not use: spoiled var
+# #define CVAR_MAPDST  0x00002000 ///< other variables are mapped to this var
+# #define CVAR_PARTIAL 0x00004000 ///< variable type is partialy defined
+# #define CVAR_THISARG 0x00008000 ///< 'this' argument of c++ member functions
+# #define CVAR_SPLIT   0x00010000 ///< variable was created by an explicit request
+#                                 ///< otherwise we could reuse an existing var
+# #define CVAR_REGNAME 0x00020000 ///< has a register name (like _RAX): if lvar
+#                                 ///< is used by an m_ext instruction
+# #define CVAR_NOPTR   0x00040000 ///< variable cannot be a pointer (user choice)
+# #define CVAR_DUMMY   0x00080000 ///< dummy argument (added to fill a hole in
+#                                 ///< the argument list)
+# #define CVAR_NOTARG  0x00100000 ///< variable cannot be an input argument
+# #define CVAR_AUTOMAP 0x00200000 ///< variable was automatically mapped
+# #define CVAR_BYREF   0x00400000 ///< the address of the variable was taken
+# #define CVAR_INASM   0x00800000 ///< variable is used in instructions translated
+#                                 ///< into __asm {...}
+# #define CVAR_UNUSED  0x01000000 ///< user-defined __unused attribute
+#                                 ///< meaningful only if: is_arg_var() && !mba->final_type
+# #define CVAR_SHARED  0x02000000 ///< variable is mapped to several chains
+# #define CVAR_SCARG   0x04000000 ///< variable is a stack argument that was
+#                                 ///< transformed from a scattered one
+# ///@}
+
+
+# Function flags (used by func_t::flags)
+# FUNC_NORET: Function doesn't return
+# FUNC_FAR: Far function
+# FUNC_LIB: Library function
+# FUNC_STATICDEF: Static function
+# FUNC_FRAME: Function uses frame pointer (BP)
+# FUNC_USERFAR: User has specified far-ness of the function
+# FUNC_HIDDEN: A hidden function chunk
+# FUNC_THUNK: Thunk (jump) function
+# FUNC_BOTTOMBP: BP points to the bottom of the stack frame
+# FUNC_NORET_PENDING: Function 'non-return' analysis must be performed.
+# FUNC_SP_READY: SP-analysis has been performed.
+# FUNC_FUZZY_SP: Function changes SP in untraceable way, e.g. and esp, 0FFFFFFF0h
+# FUNC_PROLOG_OK: Prolog analysis has been performed by last SP-analysis
+# FUNC_PURGED_OK: 'argsize' field has been validated.
+# FUNC_TAIL: This is a function tail. Other bits must be clear (except FUNC_HIDDEN).
+# FUNC_LUMINA: Function info is provided by Lumina.
+# FUNC_OUTLINE: Outlined code, not a real function.
+# FUNC_REANALYZE: Function frame changed, request to reanalyze after last insn.
+# FUNC_UNWIND: function is an exception unwind handler
+# FUNC_CATCH: function is an exception catch handler
+# FUNC_RESERVED: Reserved (for internal usage)
+# (These are #define constants in C++; define them in .pxd as needed in Python.)
diff --git a/src/d810/cythxr/_chexrays_api.pxd b/src/d810/cythxr/_chexrays_api.pxd
new file mode 100644
index 0000000..4e76f1e
--- /dev/null
+++ b/src/d810/cythxr/_chexrays_api.pxd
@@ -0,0 +1,23 @@
+# cython: language_level=3, embedsignature=True
+# distutils: language=c++
+from libcpp.unordered_map cimport unordered_map
+from libc.stdint cimport uintptr_t
+
+from ._chexrays cimport (
+    mop_t,
+    ea_t,
+    uint64,
+    qstring,
+)
+
+
+
+# ---------- tiny hash combiner (FNV-1a-ish) ----------
+cdef uint64 _mix64(uint64 h, uint64 x) noexcept nogil
+cdef uint64 _mask_nbits(uint64 v, int size) noexcept nogil
+# ---------- core recursive hasher ----------
+cdef uint64 _hash_mop_ptr(const mop_t* op,
+                            ea_t func_ea,
+                            unordered_map[uintptr_t, uint64]* insn_memo,
+                            int depth) noexcept nogil
+cdef qstring stack_var_name(mop_t* op)
diff --git a/src/d810/cythxr/_chexrays_api.pyx b/src/d810/cythxr/_chexrays_api.pyx
new file mode 100644
index 0000000..c0539f3
--- /dev/null
+++ b/src/d810/cythxr/_chexrays_api.pyx
@@ -0,0 +1,220 @@
+# cython: language_level=3, embedsignature=True
+# distutils: language=c++
+from libcpp.unordered_map cimport unordered_map
+from libcpp.pair cimport pair
+from cython.operator cimport dereference as deref
+from libc.stdint cimport uintptr_t
+
+from ._chexrays cimport (
+    mop_t,
+    ea_t,
+    uint32,
+    uint64,
+    sval_t,
+    MOPT,
+    minsn_t,
+    mop_addr_t,
+    mop_pair_t,
+    qstring,
+    stkvar_ref_t,
+    get_mreg_name,
+    OPERAND_PROPERTIES,
+    _swig_ptr,
+)
+
+
+
+# ---------- tiny hash combiner (FNV-1a-ish) ----------
+cdef uint64 _mix64(uint64 h, uint64 x) noexcept nogil:
+    h ^= x + 0x9e3779b97f4a7c15ULL + (h<<6) + (h>>2)
+    return h
+
+
+cdef uint64 _mask_nbits(uint64 v, int size) noexcept nogil:
+    if size <= 0 or size > 16:
+        return v
+    if size == 8:    # common case
+        return v & 0xFFFFFFFFFFFFFFFFULL
+    return v & ((1ULL << (size * 8)) - 1ULL)
+
+
+# ---------- core recursive hasher ----------
+cdef uint64 _hash_mop_ptr(const mop_t* op,
+                            ea_t func_ea,
+                            unordered_map[uintptr_t, uint64]* insn_memo,
+                            int depth) noexcept nogil:
+    cdef:
+        int t
+        int sz
+        uint32 oprops
+        uint64 h
+        const minsn_t* m
+        uintptr_t key
+        unordered_map[uintptr_t, uint64].iterator it
+        uint64 mh
+        const mop_addr_t* ap
+        mop_pair_t* pr
+
+    if op == NULL or depth > 128:
+        return 0xDEADBEAFULL  # nil
+
+    # common fields
+    t = <int>op.t
+    sz = <int>op.size
+    oprops = (<uint32>op.oprops) & (OPERAND_PROPERTIES.OPROP_FLOAT|OPERAND_PROPERTIES.OPROP_UDT|OPERAND_PROPERTIES.OPROP_CCFLAGS|OPERAND_PROPERTIES.OPROP_LOWADDR)
+    h = 0xCBF29CE484222325ULL
+
+    h = _mix64(h, <uint64>t)
+    h = _mix64(h, <uint64>sz)
+    h = _mix64(h, <uint64>oprops)
+
+    if t == MOPT.REGISTER:   # microregister
+        return _mix64(h, <uint64>op.r)
+    if t == MOPT.NUMBER:   # immediate
+        return _mix64(h, _mask_nbits(<uint64>op.nnn.value, sz))
+    if t == MOPT.GLOBAL:   # global EA
+        return _mix64(h, <uint64>op.g)
+    if t == MOPT.STACK:   # stack var
+        h = _mix64(h, func_ea)
+        return _mix64(h, <uint64>op.s.off)
+    if t == MOPT.LOCAL:   # lvar ref
+        h = _mix64(h, func_ea)
+        h = _mix64(h, <uint64>op.l.idx)
+        return _mix64(h, <uint64>op.l.off)
+    if t == MOPT.ADDRESS:   # address-of
+        ap = op.a
+        h = _mix64(h, <uint64>ap.insize)
+        h = _mix64(h, <uint64>ap.outsize)
+        return _mix64(h, _hash_mop_ptr(<const mop_t*>ap, func_ea, insn_memo, depth+1))
+    if t == MOPT.PAIR:   # pair (low/high)
+        pr = op.pair
+        h = _mix64(h, _hash_mop_ptr(<const mop_t*>&pr.lop, func_ea, insn_memo, depth+1))
+        return _mix64(h, _hash_mop_ptr(<const mop_t*>&pr.hop, func_ea, insn_memo, depth+1))
+    if t == MOPT.DEST_RESULT:   # result of subinstruction
+        m = op.d
+        key = <uintptr_t>m
+        it = insn_memo.find(key)
+        if it != insn_memo.end():
+            return _mix64(h, (<uint64>deref(it).second))
+        # compute once
+        mh = 0x1234567812345678ULL
+        mh = _mix64(mh, <uint64>m.opcode)
+        mh = _mix64(mh, _hash_mop_ptr(&m.l, func_ea, insn_memo, depth+1))
+        mh = _mix64(mh, _hash_mop_ptr(&m.r, func_ea, insn_memo, depth+1))
+        mh = _mix64(mh, _hash_mop_ptr(&m.d, func_ea, insn_memo, depth+1))
+        insn_memo.insert(pair[uintptr_t, uint64](key, mh))
+        return _mix64(h, mh)
+
+    # Rare kinds: produce a cheap, bounded structural salt (no strings).
+    # if t == MOPT.SCATTERED:
+    #     # scattered has a reg count + regs[] in most builds
+    #     try:
+    #         h = _mix64(h, <uint64>op.scif.nregs)
+    #         for i in range(op.scattered.nregs if op.scattered.nregs < 8 else 8):
+    #             h = _mix64(h, <uint64>op.scattered.regs[i])
+    #         return h
+    #     except Exception:
+    #         pass
+
+    # Unknown/rare kinds => tag with kind to avoid collapses
+    return _mix64(h, 0xFACEB00CULL + <uint64>t)
+
+# (Python API implemented in _chexrays_api.pyx)
+cpdef uint64 hash_mop(object py_mop, uint64 func_entry_ea=0):
+    """Return a stable structural hash for a Hex-Rays mop_t (no dstr()).
+
+    The hash is salted with the function entry EA to distinguish stack/local
+    references across functions. If unknown, pass 0.
+    """
+    cdef const mop_t* op = <const mop_t*> _swig_ptr(py_mop)
+    cdef unordered_map[uintptr_t, uint64] memo
+    return _hash_mop_ptr(op, <ea_t>func_entry_ea, &memo, 0)
+
+
+cdef qstring stack_var_name(mop_t* op):
+    cdef:
+        qstring name
+        qstring empty
+        stkvar_ref_t* s_ptr
+        sval_t ida_off
+        sval_t disp
+        
+    if op.t == MOPT.STACK:
+        s_ptr = <stkvar_ref_t*> op.s
+        if s_ptr != NULL:
+            if s_ptr.mba != NULL and s_ptr.mba.use_frame():
+                # Compute IDA frame offset once, then classify via frame layout.
+                ida_off = s_ptr.mba.stkoff_vd2ida(s_ptr.off)
+                if ida_off < s_ptr.mba.frsize:
+                    # Local: displacement from end of locals region
+                    disp = s_ptr.mba.frsize - ida_off
+                    name.sprnt("%%var_%X.%d", <unsigned>(disp), op.size)
+                elif ida_off >= s_ptr.mba.inargoff:
+                    # Argument: displacement from first argument
+                    disp = ida_off - s_ptr.mba.inargoff
+                    name.sprnt("arg_%X.%d", <unsigned>(disp), op.size)
+                else:
+                    # Middle area (saved regs/retaddr): fall back to raw labeling
+                    name.sprnt("stk_%llX.%d", <uint64> s_ptr.off, op.size)
+            else:
+                # No frame information; fall back to raw vd off
+                name.sprnt("stk_%llX.%d", <uint64> s_ptr.off, op.size)
+            name.cat_sprnt("{%d}", op.valnum)
+            return name
+    elif op.t == MOPT.REGISTER:
+        get_mreg_name(&name, op.r, op.size, <void*>NULL)
+        name.cat_sprnt("{%d}", op.valnum)
+        return name
+
+    return empty
+    
+cpdef str get_stack_or_reg_name(object py_mop):
+    """Return canonical name for stack/register mop, or "" if not applicable.
+
+    Matches Hex-Rays dstr() formatting for stack/register without calling dstr().
+    For stack operands with a frame: locals => "%var_%X.<size>", args => "arg_%X.<size>".
+    Appends SSA valnum as "{n}".
+    """
+    cdef mop_t* op = <mop_t*> _swig_ptr(py_mop)
+    return stack_var_name(op).c_str().decode('utf-8')
+    # cdef qstring name
+    # cdef bytes b
+    # cdef stkvar_ref_t* s_ptr
+    # cdef sval_t disp
+    # cdef sval_t ida_off
+    # cdef bint is_local
+    # if op == NULL:
+    #     return ""
+    # if op.t == MOPT.STACK:
+    #     s_ptr = <stkvar_ref_t*> op.s
+    #     if s_ptr != NULL:
+    #         if s_ptr.mba != NULL and s_ptr.mba.use_frame():
+    #             # Compute IDA frame offset once, then classify via frame layout.
+    #             ida_off = s_ptr.mba.stkoff_vd2ida(s_ptr.off)
+    #             if ida_off < s_ptr.mba.frsize:
+    #                 # Local: displacement from end of locals region
+    #                 is_local = 1
+    #                 disp = s_ptr.mba.frsize - ida_off
+    #                 name.sprnt("%%var_%X.%d", <unsigned>(disp), op.size)
+    #             elif ida_off >= s_ptr.mba.inargoff:
+    #                 # Argument: displacement from first argument
+    #                 is_local = 0
+    #                 disp = ida_off - s_ptr.mba.inargoff
+    #                 name.sprnt("arg_%X.%d", <unsigned>(disp), op.size)
+    #             else:
+    #                 # Middle area (saved regs/retaddr): fall back to raw labeling
+    #                 name.sprnt("stk_%llX.%d", <unsigned long long> s_ptr.off, op.size)
+    #         else:
+    #             # No frame information; fall back to raw vd off
+    #             name.sprnt("stk_%llX.%d", <unsigned long long> s_ptr.off, op.size)
+    #         name.cat_sprnt("{%d}", op.valnum)
+    #         b = name.c_str()
+    #         return b.decode('utf-8')
+    #     return ""
+    # elif op.t == MOPT.REGISTER:
+    #     get_mreg_name(&name, op.r, op.size, <void*>NULL)
+    #     name.cat_sprnt("{%d}", op.valnum)
+    #     b = name.c_str()
+    #     return b.decode('utf-8')
+    # return ""
+
diff --git a/src/d810/cythxr/cymode.py b/src/d810/cythxr/cymode.py
new file mode 100644
index 0000000..2691c56
--- /dev/null
+++ b/src/d810/cythxr/cymode.py
@@ -0,0 +1,41 @@
+import dataclasses
+
+from d810.registry import survives_reload
+from d810.singleton import SingletonMeta
+
+
+@survives_reload()
+@dataclasses.dataclass(slots=True)
+class CythonMode(metaclass=SingletonMeta):
+    """
+    Provides a controller to enable or disable the Cython-accelerated
+    implementations of AST functions at runtime.
+    """
+
+    _enabled: bool = dataclasses.field(default=True)
+
+    def _set_flag(self, value: bool) -> None:
+        self._enabled = bool(value)
+
+    def enable(self) -> None:
+        """Point the public API to the fast Cython implementations."""
+        if not self._enabled:
+            self._set_flag(True)
+            print("Cython AST implementation ENABLED.")
+
+    def disable(self) -> None:
+        """Point the public API to the pure Python implementations for debugging."""
+        if self._enabled:
+            self._set_flag(False)
+            print("Cython AST implementation DISABLED (using pure Python).")
+
+    def is_enabled(self) -> bool:
+        """Check if the Cython implementation is currently active."""
+        return self._enabled
+
+    def toggle(self) -> None:
+        """Toggle the Cython implementation on/off."""
+        if self._enabled:
+            self.disable()
+        else:
+            self.enable()
diff --git a/src/d810/expr/_ast.pyx b/src/d810/expr/_ast.pyx
new file mode 100644
index 0000000..3d3a121
--- /dev/null
+++ b/src/d810/expr/_ast.pyx
@@ -0,0 +1,2036 @@
+# distutils: language = c++
+# cython: language_level=3, embedsignature=True
+# cython: cdivision=True
+# distutils: define_macros=__EA64__=1
+from __future__ import annotations
+import abc
+import dataclasses
+import typing
+
+import cython
+
+import ida_hexrays
+import idaapi
+
+import d810._compat as _compat
+from d810.conf.loggers import getLogger
+from d810.errors import AstEvaluationException
+from d810.expr.utils import (
+    MOP_CONSTANT_CACHE,
+    MOP_TO_AST_CACHE
+)
+from d810.hexrays.hexrays_formatters import (
+    format_minsn_t,
+    format_mop_t,
+    mop_tree,
+    mop_type_to_string,
+    opcode_to_string,
+    sanitize_ea,
+)
+from d810.hexrays.hexrays_helpers import (
+    AND_TABLE,
+    MBA_RELATED_OPCODES,
+    MINSN_TO_AST_FORBIDDEN_OPCODES,
+    OPCODES_INFO,
+    Z3_SPECIAL_OPERANDS,
+    equal_mops_ignore_size,
+    is_rotate_helper_call,
+    structural_mop_hash,
+)
+from d810.expr._ast_evaluate import AstEvaluator
+from d810.registry import NOT_GIVEN
+
+logger = getLogger(__name__)
+
+
+def get_constant_mop(value: int, size: int) -> ida_hexrays.mop_t:
+    """
+    Returns a cached or new mop_t for a constant value.
+    This avoids repeated calls to mop_t.__init__ and make_number.
+    """
+    key = (value, size)
+    if key in MOP_CONSTANT_CACHE:
+        return MOP_CONSTANT_CACHE[key]
+
+    # Not in cache, create it once and store it.
+    cst_mop = ida_hexrays.mop_t()
+    cst_mop.make_number(value, size)
+    MOP_CONSTANT_CACHE[key] = cst_mop
+    return cst_mop
+
+@dataclasses.dataclass(slots=True)
+class AstInfo:
+    ast: AstBase
+    number_of_use: int
+
+    def __str__(self):
+        return f"{self.ast} used {self.number_of_use} times: {format_mop_t(self.ast.mop) if self.ast.mop else 0}"
+
+cdef class AstBase:
+    cdef public dict sub_ast_info_by_index
+    cdef public object mop
+    cdef public object dst_mop
+    cdef public object dest_size
+    cdef public object ea
+    cdef public object ast_index
+    # cdef public ida_hexrays.mop_t dst_mop
+    # cdef public int dest_size
+    # cdef public int ea
+    # cdef public int ast_index
+    
+    @abc.abstractmethod
+    def is_frozen(self) -> bool:
+        raise NotImplementedError("AstBase.is_frozen must be overridden")
+    
+    @abc.abstractmethod
+    def clone(self):
+        raise NotImplementedError("AstBase.clone must be overridden")
+        
+    @abc.abstractmethod
+    def freeze(self) -> None:
+        raise NotImplementedError("AstBase.freeze must be overridden")
+    
+    @abc.abstractmethod
+    def is_node(self) -> bool:
+        raise NotImplementedError("AstBase.is_node must be overridden")
+    
+    @abc.abstractmethod
+    def is_leaf(self) -> bool:
+        raise NotImplementedError("AstBase.is_leaf must be overridden")
+    
+    @abc.abstractmethod
+    def is_constant(self) -> bool:
+        raise NotImplementedError("AstBase.is_constant must be overridden")
+    
+    @abc.abstractmethod
+    def compute_sub_ast(self) -> None:
+        raise NotImplementedError("AstBase.compute_sub_ast must be overridden")
+    
+    @abc.abstractmethod
+    def get_leaf_list(self) -> list[AstLeaf]:
+        raise NotImplementedError("AstBase.get_leaf_list must be overridden")
+    
+    @abc.abstractmethod
+    def reset_mops(self) -> None:
+        raise NotImplementedError("AstBase.reset_mops must be overridden")
+    
+    @abc.abstractmethod
+    def _copy_mops_from_ast(self, other: AstBase, read_only: bool = False) -> bool:
+        raise NotImplementedError("AstBase._copy_mops_from_ast must be overridden")
+    
+    @abc.abstractmethod
+    def create_mop(self, ea: int) -> ida_hexrays.mop_t:
+        raise NotImplementedError("AstBase.create_mop must be overridden")
+    
+    @abc.abstractmethod
+    def get_pattern(self) -> str:
+        raise NotImplementedError("AstBase.get_pattern must be overridden")
+    
+    @abc.abstractmethod
+    def evaluate(self, dict_index_to_value: dict[int, int]) -> int:
+        raise NotImplementedError("AstBase.evaluate must be overridden")
+    
+    @abc.abstractmethod
+    def get_depth_signature(self, depth: int) -> list[str]:
+        raise NotImplementedError("AstBase.get_depth_signature must be overridden")
+    
+    def __bool__(self) -> bool:
+        return True    
+
+
+cdef class AstNode(AstBase):
+    cdef public object opcode
+    cdef public AstBase left
+    cdef public AstBase right
+    cdef public AstBase dst
+    cdef public object dst_size
+    cdef public object dst_mop
+    cdef public list opcodes
+    cdef public bint is_candidate_ok
+    cdef public list leafs
+    cdef public dict leafs_by_name
+    cdef public object func_name
+    cdef public bint _is_frozen
+    
+        
+    def __init__(
+        self,
+        opcode: int | None = None,
+        left: AstBase | None = None,
+        right: AstBase | None = None,
+        dst: AstBase | None = None,
+    ):
+        super().__init__()
+        self.opcode = opcode
+        # if left is not None:
+        #     self.left = left
+        # if right is not None:
+        #     self.right = right
+        # if dst is not None: 
+        #     self.dst = dst
+        self.left = left
+        self.right = right
+        self.dst = dst
+        self.mop = None
+        self.dst_mop = None
+
+        self.opcodes = []
+        self.is_candidate_ok = <bint>False
+
+        self.leafs = []
+        self.leafs_by_name = {}
+
+        self.ast_index = 0
+        self.sub_ast_info_by_index = {}
+
+        self.func_name = ""
+        self._is_frozen = <bint>False  # All newly created nodes are mutable by default
+
+    @_compat.override
+    def is_frozen(self) -> bool:
+        return bool(self._is_frozen)
+
+    @_compat.override
+    def freeze(self):
+        """Recursively freezes this node and all its children."""
+        if self._is_frozen:
+            return
+        self._is_frozen = <bint>True
+        if hasattr(self, "left") and self.left:
+            self.left.freeze()
+        if hasattr(self, "right") and self.right:
+            self.right.freeze()
+        if hasattr(self, "dst") and self.dst:
+            self.dst.freeze()
+
+    @property
+    def size(self):
+        return self.mop.d.d.size if self.mop else 0
+
+    def compute_sub_ast(self):
+        self.sub_ast_info_by_index = {}
+        assert self.ast_index is not None
+        self.sub_ast_info_by_index[self.ast_index] = AstInfo(self, 1)
+
+        if self.left is not None:
+            self.left.compute_sub_ast()
+            for ast_index, ast_info in self.left.sub_ast_info_by_index.items():
+                if ast_index not in self.sub_ast_info_by_index.keys():
+                    self.sub_ast_info_by_index[ast_index] = AstInfo(ast_info.ast, 0)
+                self.sub_ast_info_by_index[
+                    ast_index
+                ].number_of_use += ast_info.number_of_use
+
+        if self.right is not None:
+            self.right.compute_sub_ast()
+            for ast_index, ast_info in self.right.sub_ast_info_by_index.items():
+                if ast_index not in self.sub_ast_info_by_index.keys():
+                    self.sub_ast_info_by_index[ast_index] = AstInfo(ast_info.ast, 0)
+                self.sub_ast_info_by_index[
+                    ast_index
+                ].number_of_use += ast_info.number_of_use
+
+    def get_information(self):
+        leaf_info_list = []
+        cst_list = []
+        opcode_list = []
+        self.compute_sub_ast()
+
+        for _, ast_info in self.sub_ast_info_by_index.items():
+            if (ast_info.ast.mop is not None) and (
+                ast_info.ast.mop.t != ida_hexrays.mop_z
+            ):
+                if ast_info.ast.is_leaf():
+                    if ast_info.ast.is_constant():
+                        cst_list.append(ast_info.ast.mop.nnn.value)
+                    else:
+                        leaf_info_list.append(ast_info)
+                else:
+                    ast_node = typing.cast(AstNode, ast_info.ast)
+                    opcode_list += [ast_node.opcode] * ast_info.number_of_use
+
+        return leaf_info_list, cst_list, opcode_list
+
+    def __getitem__(self, k: str) -> AstLeaf:
+        return self.leafs_by_name[k]
+
+    def get_leaf_list(self) -> list[AstLeaf]:
+        leafs = []
+        if self.left is not None:
+            leafs += self.left.get_leaf_list()
+        if self.right is not None:
+            leafs += self.right.get_leaf_list()
+        return leafs
+
+    def add_leaf(self, leaf_name: str, leaf_mop: ida_hexrays.mop_t):
+        leaf = AstLeaf(leaf_name)
+        leaf.mop = leaf_mop
+        self.leafs.append(leaf)
+        self.leafs_by_name[leaf_name] = leaf
+
+    def add_constant_leaf(self, leaf_name: str, cst_value: int, cst_size: int):
+        masked_value = cst_value & AND_TABLE[cst_size]
+        cst_mop = get_constant_mop(masked_value, cst_size)
+        self.add_leaf(leaf_name, cst_mop)
+
+    def check_pattern_and_copy_mops(
+        self, ast: AstNode | AstLeaf, read_only: bool = False
+    ) -> bool:
+        if not read_only:
+            self.reset_mops()
+        if logger.debug_on:
+            logger.debug(
+                "AstNode.check_pattern_and_copy_mops from %r",
+                ast,
+            )
+        is_matching_shape = self._copy_mops_from_ast(ast, read_only)
+        if not is_matching_shape:
+            return False
+        return self._check_implicit_equalities()
+
+    def reset_mops(self):
+        self.mop = None
+        if self.left is not None:
+            self.left.reset_mops()
+        if self.right is not None:
+            self.right.reset_mops()
+
+    def _copy_mops_from_ast(
+        self, other: AstNode | AstLeaf, read_only: bool = False
+    ) -> bool:
+        if not other.is_node():
+            return False
+        other = typing.cast(AstNode, other)
+        if self.opcode != other.opcode:
+            return False
+
+        if not read_only:
+            self.mop = other.mop
+            self.dst_mop = other.dst_mop
+            self.dest_size = other.dest_size
+            self.ea = other.ea
+
+        if logger.debug_on:
+            logger.debug(
+                "AstNode._copy_mops_from_ast: self.left: %r, other.left: %r",
+                self.left,
+                other.left,
+            )
+        if self.left is not None and other.left is not None:
+            if not self.left._copy_mops_from_ast(other.left, read_only):
+                return False
+        if logger.debug_on:
+            logger.debug(
+                "AstNode._copy_mops_from_ast: self.right: %r, other.right: %r",
+                self.right,
+                other.right,
+            )
+        if self.right is not None and other.right is not None:
+            if not self.right._copy_mops_from_ast(other.right, read_only):
+                return False
+        return True
+
+    def _check_implicit_equalities(self) -> bool:
+        self.leafs = self.get_leaf_list()
+        self.leafs_by_name = {}
+        self.is_candidate_ok = <bint>True
+
+        for leaf in self.leafs:
+            ref_leaf = self.leafs_by_name.get(leaf.name)
+            if ref_leaf is not None and leaf.mop is not None:
+                if not equal_mops_ignore_size(ref_leaf.mop, leaf.mop):
+                    self.is_candidate_ok = <bint>False
+            self.leafs_by_name[leaf.name] = leaf
+        return bool(self.is_candidate_ok)
+
+    def update_leafs_mop(
+        self,
+        other: AstNode | AstProxy,
+        other2: AstNode | AstProxy | None = None,
+    ) -> bool:
+        self.leafs = self.get_leaf_list()
+        all_leafs_found = True
+        for leaf in self.leafs:
+            if other is not None and leaf.name in other.leafs_by_name:
+                leaf.mop = other.leafs_by_name[leaf.name].mop
+            elif other2 is not None and leaf.name in other2.leafs_by_name:
+                leaf.mop = other2.leafs_by_name[leaf.name].mop
+            else:
+                all_leafs_found = False
+        return all_leafs_found
+
+    def create_mop(self, ea: int) -> ida_hexrays.mop_t:
+        new_ins = self.create_minsn(ea)
+        new_ins_mop = ida_hexrays.mop_t()
+        new_ins_mop.create_from_insn(new_ins)
+        return new_ins_mop
+
+    def create_minsn(self, ea: int, dest=None) -> ida_hexrays.minsn_t:
+        new_ins = ida_hexrays.minsn_t(ea)
+        new_ins.opcode = self.opcode
+
+        if self.left is not None:
+            new_ins.l = self.left.create_mop(ea)
+            if self.right is not None:
+                new_ins.r = self.right.create_mop(ea)
+
+        new_ins.d = ida_hexrays.mop_t()
+
+        if self.left is not None:
+            new_ins.d.size = new_ins.l.size
+        if dest is not None:
+            new_ins.d = dest
+        return new_ins
+
+    def get_pattern(self) -> str:
+        nb_operands = OPCODES_INFO[self.opcode]["nb_operands"]
+        if nb_operands == 0:
+            return "AstNode({0})".format(OPCODES_INFO[self.opcode]["name"])
+        elif nb_operands == 1 and self.left is not None:
+            return "AstNode(m_{0}, {1})".format(
+                OPCODES_INFO[self.opcode]["name"], self.left.get_pattern()
+            )
+        elif nb_operands == 2 and self.left is not None and self.right is not None:
+            return "AstNode(m_{0}, {1}, {2})".format(
+                OPCODES_INFO[self.opcode]["name"],
+                self.left.get_pattern(),
+                self.right.get_pattern(),
+            )
+        else:
+            raise ValueError(f"Invalid number of operands: {nb_operands}")
+
+    def evaluate_with_leaf_info(
+        self, leafs_info: list[AstInfo], leafs_value: list[int]
+    ) -> int:
+        return _DEFAULT_AST_EVALUATOR.evaluate_with_leaf_info(self, leafs_info, leafs_value)
+
+    def evaluate(self, dict_index_to_value: dict[int, int]) -> int:
+        return _DEFAULT_AST_EVALUATOR.evaluate(self, dict_index_to_value)
+
+    def get_depth_signature(self, depth):
+        if depth == 1:
+            return ["{0}".format(self.opcode)]
+        tmp = []
+        nb_operands = OPCODES_INFO[self.opcode]["nb_operands"]
+        if (nb_operands >= 1) and self.left is not None:
+            tmp += self.left.get_depth_signature(depth - 1)
+        else:
+            tmp += ["N"] * (2 ** (depth - 2))
+        if (nb_operands >= 2) and self.right is not None:
+            tmp += self.right.get_depth_signature(depth - 1)
+        else:
+            tmp += ["N"] * (2 ** (depth - 2))
+        return tmp
+
+    def __str__(self):
+        try:
+            nb_operands = OPCODES_INFO[self.opcode]["nb_operands"]
+            if "symbol" in OPCODES_INFO[self.opcode].keys():
+                if nb_operands == 0:
+                    return "{0}()".format(OPCODES_INFO[self.opcode]["symbol"])
+                elif nb_operands == 1:
+                    return "{0}({1})".format(
+                        OPCODES_INFO[self.opcode]["symbol"], self.left
+                    )
+                elif nb_operands == 2:
+                    if OPCODES_INFO[self.opcode]["symbol"] not in Z3_SPECIAL_OPERANDS:
+                        return "({1} {0} {2})".format(
+                            OPCODES_INFO[self.opcode]["symbol"], self.left, self.right
+                        )
+                    else:
+                        return "{0}({1}, {2})".format(
+                            OPCODES_INFO[self.opcode]["symbol"], self.left, self.right
+                        )
+            else:
+                if nb_operands == 0:
+                    return "{0}()".format(OPCODES_INFO[self.opcode]["name"])
+                elif nb_operands == 1:
+                    return "{0}({1})".format(
+                        OPCODES_INFO[self.opcode]["name"], self.left
+                    )
+                elif nb_operands == 2:
+                    return "{0}({1}, {2})".format(
+                        OPCODES_INFO[self.opcode]["name"], self.left, self.right
+                    )
+        except RuntimeError as e:
+            logger.info("Error while calling __str__ on AstNode: {0}".format(e))
+        return "Error_AstNode"
+
+    def __repr__(self):
+        op_str = opcode_to_string(self.opcode) if self.opcode is not None else "None"
+        return f"AstNode({op_str}, left={self.left}, right={self.right})"
+
+    cpdef AstNode clone(self):
+        """Fast, C-level recursive implementation of clone."""
+        # Create a new instance without calling Python's __init__
+        cdef AstNode new_node = AstNode.__new__(AstNode)
+        # Call __init__ to set up all lists/dicts correctly
+        new_node.__init__(self.opcode)
+        
+        # Manually copy attributes
+        # new_node.opcode = self.opcode
+        new_node.mop = self.mop
+        new_node.dst_mop = self.dst_mop
+        new_node.dest_size = self.dest_size
+        new_node.ea = self.ea
+        new_node.ast_index = self.ast_index
+        new_node.func_name = self.func_name
+
+        # Create shallow copies of collections. This is safe because the
+        # items inside (other AST nodes) will be cloned recursively below.
+        new_node.opcodes = self.opcodes.copy()
+        new_node.leafs = self.leafs.copy()
+        new_node.leafs_by_name = self.leafs_by_name.copy()
+        # Recursively clone children using the fast path
+        # The `is not None` check is crucial
+        # if hasattr(self, "left") and self.left is not None:
+        #     new_node.left = self.left.clone()
+        # if hasattr(self, "right") and self.right is not None:
+        #     new_node.right = self.right.clone()
+        # if hasattr(self, "dst") and self.dst is not None:
+        #     new_node.dst = self.dst.clone()
+        new_node.left = self.left.clone() if self.left is not None else None
+        new_node.right = self.right.clone() if self.right is not None else None
+        new_node.dst = self.dst.clone() if self.dst is not None else None
+        
+        # Initialize transient state for the new clone
+        new_node.is_candidate_ok = <bint>False
+        new_node.leafs = []
+        new_node.leafs_by_name = {}
+        new_node.opcodes = []
+        new_node.sub_ast_info_by_index = {}
+        new_node._is_frozen = <bint>False
+        return new_node   
+
+    @_compat.override
+    def is_node(self):
+        return True
+
+    @_compat.override
+    def is_leaf(self):
+        # An AstNode is not a leaf, so returns False
+        return False
+
+    @_compat.override
+    def is_constant(self):
+        return False
+
+
+cdef class AstLeaf(AstBase):
+    cdef public object name
+    cdef public object z3_var
+    cdef public object z3_var_name
+    cdef public bint _is_frozen
+
+    def __init__(self, name):
+        self.name = name
+        self.ast_index: None
+
+        self.mop = None
+        self.z3_var = None
+        self.z3_var_name = NOT_GIVEN
+
+        self.dest_size = None
+        self.ea = None
+        self._is_frozen = <bint>False  # All newly created nodes are mutable by default
+        self.sub_ast_info_by_index = {}
+
+    @_compat.override
+    def is_frozen(self) -> bool:
+        return bool(self._is_frozen)
+
+    @_compat.override
+    def freeze(self):
+        """Recursively freezes this node and all its children."""
+        if self._is_frozen:
+            return
+        self._is_frozen = <bint>True
+
+    @_compat.override
+    def is_node(self):
+        return False
+
+    @_compat.override
+    def is_leaf(self):
+        return True
+
+    @_compat.override
+    def is_constant(self):
+        if self.mop is None:
+            return False
+        return self.mop.t == ida_hexrays.mop_n
+    
+    cpdef AstLeaf clone(self):
+        """Fast, C-level cloner for AstLeaf instances."""
+        cdef AstLeaf new_leaf = AstLeaf.__new__(AstLeaf)
+        # Call __init__ to set up name and defaults
+        new_leaf.__init__(self.name)
+        # Manually copy attribute
+        new_leaf.ast_index = self.ast_index
+        new_leaf.mop = self.mop
+        new_leaf.dest_size = self.dest_size
+        new_leaf.ea = self.ea
+
+        # Initialize transient state
+        new_leaf.z3_var = None
+        return new_leaf     
+
+    def __getitem__(self, name: str) -> AstLeaf:
+        if name == self.name:
+            return self
+        raise KeyError
+
+    @property
+    def size(self):
+        return self.mop.size if self.mop else 0
+
+    @property
+    def dst_mop(self):
+        return self.mop
+
+    @dst_mop.setter
+    def dst_mop(self, mop):
+        self.mop = mop
+
+    @property
+    def value(self):
+        if self.is_constant() and self.mop is not None:
+            return self.mop.nnn.value
+        else:
+            return None
+
+    def compute_sub_ast(self):
+        self.sub_ast_info_by_index = {}
+        assert self.ast_index is not None
+        self.sub_ast_info_by_index[self.ast_index] = AstInfo(self, 1)
+
+    def get_information(self):
+        # Just here to allow calling get_information on either a AstNode or AstLeaf
+        return [], [], []
+
+    def get_leaf_list(self):
+        return [self]
+
+    def create_mop(self, ea):
+        # 1. Constant operands can keep using the shared cache
+        if self.is_constant() and self.value is not None:
+            # TODO: is this right?
+            size = self.dest_size if self.dest_size is not None else self.size
+            if logger.debug_on:
+                logger.debug(
+                    "AstLeaf.create_mop: Constant operand @ 0x%x: %s, size: %s, dest_size: %s, equal? %s",
+                    ea,
+                    self.value,
+                    size,
+                    self.dest_size,
+                    size == self.dest_size,
+                )
+            val = get_constant_mop(self.value, size)
+            if logger.debug_on:
+                logger.debug(
+                    "AstLeaf.create_mop: Constant operand reused: %s",
+                    val,
+                    extra={"ea": hex(ea)},
+                )
+            return val
+
+        if self.mop is None:
+            logger.error(
+                "%r mop is None in create_mop for 0x%x",
+                self,
+                ea,
+            )
+            raise AstEvaluationException(
+                f"{repr(self)}'s mop is None in create_mop for {hex(ea)}"
+            )
+
+        # 2. Otherwise, we need to create a new mop
+        new_mop = ida_hexrays.mop_t()
+        new_mop.assign(self.mop)
+        return new_mop  # duplicates the C++ object
+
+    def update_leafs_mop(self, other: AstNode, other2: AstNode | None = None):
+        if other is not None and self.name in other.leafs_by_name:
+            self.mop = other.leafs_by_name[self.name].mop
+            return True
+        elif other2 is not None and self.name in other2.leafs_by_name:
+            self.mop = other2.leafs_by_name[self.name].mop
+            return True
+        return False
+
+    def check_pattern_and_copy_mops(self, ast, read_only: bool = False):
+        if not read_only:
+            self.reset_mops()
+        is_matching_shape = self._copy_mops_from_ast(ast, read_only)
+
+        if not is_matching_shape:
+            return False
+        return self._check_implicit_equalities()
+
+    def reset_mops(self):
+        self.z3_var = None
+        self.z3_var_name = NOT_GIVEN
+        self.mop = None
+
+    def _copy_mops_from_ast(self, other, read_only: bool = False):
+        if other.mop is None:
+            if logger.debug_on:
+                logger.debug(
+                    "AstLeaf._copy_mops_from_ast: other %r's mop is None",
+                    other,
+                )
+            return False
+        if logger.debug_on:
+            logger.debug(
+                "AstLeaf._copy_mops_from_ast: other %r's mop %s is not None",
+                other,
+                format_mop_t(other.mop),
+            )
+        if not read_only:
+            self.mop = other.mop
+        return True
+
+    @staticmethod
+    def _check_implicit_equalities():
+        # An AstLeaf does not have any implicit equalities to be checked, so we always returns True
+        return True
+
+    def get_pattern(self):
+        if self.is_constant() and self.mop is not None:
+            return "AstConstant('{0}', {0})".format(self.mop.nnn.value)
+        if self.ast_index is not None:
+            return "AstLeaf('x_{0}')".format(self.ast_index)
+        if self.name is not None:
+            return "AstLeaf('{0}')".format(self.name)
+
+    def evaluate_with_leaf_info(self, leafs_info, leafs_value):
+        return _DEFAULT_AST_EVALUATOR.evaluate_with_leaf_info(self, leafs_info, leafs_value)
+
+    def evaluate(self, dict_index_to_value):
+        return _DEFAULT_AST_EVALUATOR.evaluate(self, dict_index_to_value)
+
+    def get_depth_signature(self, depth):
+        if depth == 1:
+            if self.is_constant():
+                return ["C"]
+            return ["L"]
+        else:
+            return ["N"] * (2 ** (depth - 1))
+
+    def __str__(self):
+        try:
+            if self.is_constant() and self.mop is not None:
+                return "{0}".format(hex(self.mop.nnn.value))
+            if self.z3_var_name is not NOT_GIVEN:
+                return self.z3_var_name
+            if self.ast_index is not None:
+                return "x_{0}".format(self.ast_index)
+            if self.mop is not None:
+                return format_mop_t(self.mop)
+            return self.name
+        except RuntimeError as e:
+            logger.info("Error while calling __str__ on AstLeaf: {0}".format(e))
+            return "Error_AstLeaf"
+
+    def __repr__(self):
+        return f"AstLeaf('{str(self)}')"
+
+
+cdef class AstConstant(AstLeaf):
+    cdef public object expected_value
+    cdef public object expected_size
+    
+    def __init__(self, name, expected_value=None, expected_size=None):
+        super().__init__(name)
+        self.expected_value = expected_value
+        self.expected_size = expected_size
+
+    @property
+    def value(self):
+        assert self.mop is not None and self.mop.t == ida_hexrays.mop_n
+        return self.mop.nnn.value
+
+    @_compat.override
+    def is_constant(self) -> bool:
+        # An AstConstant is always constant, so return True
+        return True
+
+    def _copy_mops_from_ast(self, other, read_only: bool = False):
+        if other.mop is not None and other.mop.t != ida_hexrays.mop_n:
+            if logger.debug_on:
+                logger.debug(
+                    "AstConstant._copy_mops_from_ast: other.mop is not a constant: %r",
+                    other.mop,
+                )
+            return False
+
+        if logger.debug_on:
+            logger.debug(
+                "AstConstant._copy_mops_from_ast: other %r's mop %s is a constant",
+                other,
+                format_mop_t(other.mop),
+            )
+        if not read_only:
+            self.mop = other.mop
+        if self.expected_value is None:
+            if not read_only:
+                self.expected_value = other.mop.nnn.value
+                self.expected_size = other.mop.size
+            else:
+                return True
+        return self.expected_value == other.mop.nnn.value
+
+    def evaluate(self, dict_index_to_value=None):
+        if self.mop is not None and self.mop.t == ida_hexrays.mop_n:
+            return self.mop.nnn.value
+        return self.expected_value
+
+    def get_depth_signature(self, depth):
+        if depth == 1:
+            return ["C"]
+        else:
+            return ["N"] * (2 ** (depth - 1))
+
+    @_compat.override
+    def __str__(self):
+        try:
+            if self.mop is not None and self.mop.t == ida_hexrays.mop_n:
+                return "0x{0:x}".format(self.mop.nnn.value)
+            if getattr(self, "expected_value", None) is not None:
+                return "0x{0:x}".format(self.expected_value)
+            return self.name
+        except RuntimeError as e:
+            logger.info("Error while calling __str__ on AstConstant: {0}".format(e))
+            return "Error_AstConstant"
+
+    @_compat.override
+    def __repr__(self):
+        return f"AstConstant({str(self)})"
+        
+    cpdef AstConstant clone(self):
+        """Fast, C-level cloner for AstConstant instances."""
+        cdef AstConstant new_const = AstConstant.__new__(AstConstant)
+        # Call __init__ to set up name and expected values
+        new_const.__init__(self.name, self.expected_value, self.expected_size)
+        
+        # Copy AstLeaf attributes
+        # new_const.name = self.name
+        new_const.ast_index = self.ast_index
+        new_const.mop = self.mop
+        new_const.dest_size = self.dest_size
+        new_const.ea = self.ea
+
+        # Initialize transient state
+        new_const.z3_var = None
+        new_const.z3_var_name = NOT_GIVEN
+        new_const.sub_ast_info_by_index = {}
+        new_const._is_frozen = <bint>False
+        return new_const
+
+
+# class AstEvaluator:
+#     """
+#     Pure-Python evaluator for AST nodes. Extracted from AstNode/AstLeaf methods
+#     to centralize evaluation logic.
+#     """
+
+#     def evaluate_with_leaf_info(self, node, leafs_info, leafs_value):
+#         dict_index_to_value = {}
+#         for leaf_info, leaf_value in zip(leafs_info, leafs_value):
+#             if leaf_info.ast.ast_index is not None:
+#                 dict_index_to_value[leaf_info.ast.ast_index] = leaf_value
+#         return self.evaluate(node, dict_index_to_value)
+
+#     def evaluate(self, node, dict_index_to_value):
+#         if isinstance(node, AstNode):
+#             return self._eval_node(node, dict_index_to_value)
+#         if isinstance(node, AstLeaf):
+#             return self._eval_leaf(node, dict_index_to_value)
+#         if isinstance(node, AstProxy):
+#             return self.evaluate(node._target, dict_index_to_value)
+#         raise AstEvaluationException(f"Unsupported AST node type: {type(node).__name__}")
+
+#     def _eval_leaf(self, leaf, dict_index_to_value):
+#         # AstConstant: prefer concrete mop value, otherwise fall back to expected_value
+#         if isinstance(leaf, AstConstant):
+#             if leaf.mop is not None and leaf.mop.t == ida_hexrays.mop_n:
+#                 return leaf.mop.nnn.value
+#             return leaf.expected_value
+
+#         if leaf.is_constant() and leaf.mop is not None:
+#             return leaf.mop.nnn.value
+#         assert leaf.ast_index is not None
+#         return dict_index_to_value.get(leaf.ast_index)
+
+#     def _eval_node(self, node, dict_index_to_value):
+#         if node.ast_index in dict_index_to_value:
+#             return dict_index_to_value[node.ast_index]
+#         if node.dest_size is None:
+#             raise ValueError("dest_size is None")
+
+#         res_mask = AND_TABLE[node.dest_size]
+
+#         if node.left is None:
+#             raise ValueError(f"left is None for opcode: {node.opcode}")
+
+#         binary_opcodes = {
+#             ida_hexrays.m_add,
+#             ida_hexrays.m_sub,
+#             ida_hexrays.m_mul,
+#             ida_hexrays.m_udiv,
+#             ida_hexrays.m_sdiv,
+#             ida_hexrays.m_umod,
+#             ida_hexrays.m_smod,
+#             ida_hexrays.m_or,
+#             ida_hexrays.m_and,
+#             ida_hexrays.m_xor,
+#             ida_hexrays.m_shl,
+#             ida_hexrays.m_shr,
+#             ida_hexrays.m_sar,
+#             ida_hexrays.m_cfadd,
+#             ida_hexrays.m_ofadd,
+#             ida_hexrays.m_seto,
+#             ida_hexrays.m_setnz,
+#             ida_hexrays.m_setz,
+#             ida_hexrays.m_setae,
+#             ida_hexrays.m_setb,
+#             ida_hexrays.m_seta,
+#             ida_hexrays.m_setbe,
+#             ida_hexrays.m_setg,
+#             ida_hexrays.m_setge,
+#             ida_hexrays.m_setl,
+#             ida_hexrays.m_setle,
+#             ida_hexrays.m_setp,
+#         }
+
+#         if node.opcode in binary_opcodes and node.right is None:
+#             raise ValueError("right is None for binary opcode: {0}".format(node.opcode))
+
+#         if node.opcode == ida_hexrays.m_mov:
+#             return (self.evaluate(node.left, dict_index_to_value)) & res_mask
+#         elif node.opcode == ida_hexrays.m_neg:
+#             return (-self.evaluate(node.left, dict_index_to_value)) & res_mask
+#         elif node.opcode == ida_hexrays.m_lnot:
+#             return self.evaluate(node.left, dict_index_to_value) != 0
+#         elif node.opcode == ida_hexrays.m_bnot:
+#             return (self.evaluate(node.left, dict_index_to_value) ^ res_mask) & res_mask
+#         elif node.opcode == ida_hexrays.m_xds:
+#             left_value_signed = unsigned_to_signed(
+#                 self.evaluate(node.left, dict_index_to_value), node.left.dest_size
+#             )
+#             return signed_to_unsigned(left_value_signed, node.dest_size) & res_mask
+#         elif node.opcode == ida_hexrays.m_xdu:
+#             return (self.evaluate(node.left, dict_index_to_value)) & res_mask
+#         elif node.opcode == ida_hexrays.m_low:
+#             return (self.evaluate(node.left, dict_index_to_value)) & res_mask
+#         elif node.opcode == ida_hexrays.m_high:
+#             if node.left.dest_size is None:
+#                 raise ValueError("left.dest_size is None for m_high")
+#             shift_bits = node.dest_size * 8 if node.dest_size is not None else 0
+#             return (self.evaluate(node.left, dict_index_to_value) >> shift_bits) & res_mask
+#         elif node.opcode == ida_hexrays.m_add and node.right is not None:
+#             return (self.evaluate(node.left, dict_index_to_value) + self.evaluate(node.right, dict_index_to_value)) & res_mask
+#         elif node.opcode == ida_hexrays.m_sub and node.right is not None:
+#             return (self.evaluate(node.left, dict_index_to_value) - self.evaluate(node.right, dict_index_to_value)) & res_mask
+#         elif node.opcode == ida_hexrays.m_mul and node.right is not None:
+#             return (self.evaluate(node.left, dict_index_to_value) * self.evaluate(node.right, dict_index_to_value)) & res_mask
+#         elif node.opcode == ida_hexrays.m_udiv and node.right is not None:
+#             return (self.evaluate(node.left, dict_index_to_value) // self.evaluate(node.right, dict_index_to_value)) & res_mask
+#         elif node.opcode == ida_hexrays.m_sdiv and node.right is not None:
+#             return (self.evaluate(node.left, dict_index_to_value) // self.evaluate(node.right, dict_index_to_value)) & res_mask
+#         elif node.opcode == ida_hexrays.m_umod and node.right is not None:
+#             return (self.evaluate(node.left, dict_index_to_value) % self.evaluate(node.right, dict_index_to_value)) & res_mask
+#         elif node.opcode == ida_hexrays.m_smod and node.right is not None:
+#             return (self.evaluate(node.left, dict_index_to_value) % self.evaluate(node.right, dict_index_to_value)) & res_mask
+#         elif node.opcode == ida_hexrays.m_or and node.right is not None:
+#             return (self.evaluate(node.left, dict_index_to_value) | self.evaluate(node.right, dict_index_to_value)) & res_mask
+#         elif node.opcode == ida_hexrays.m_and and node.right is not None:
+#             return (self.evaluate(node.left, dict_index_to_value) & self.evaluate(node.right, dict_index_to_value)) & res_mask
+#         elif node.opcode == ida_hexrays.m_xor and node.right is not None:
+#             return (self.evaluate(node.left, dict_index_to_value) ^ self.evaluate(node.right, dict_index_to_value)) & res_mask
+#         elif node.opcode == ida_hexrays.m_shl and node.right is not None:
+#             return (self.evaluate(node.left, dict_index_to_value) << self.evaluate(node.right, dict_index_to_value)) & res_mask
+#         elif node.opcode == ida_hexrays.m_shr and node.right is not None:
+#             return (self.evaluate(node.left, dict_index_to_value) >> self.evaluate(node.right, dict_index_to_value)) & res_mask
+#         elif node.opcode == ida_hexrays.m_sar and node.right is not None:
+#             left_value_signed = unsigned_to_signed(
+#                 self.evaluate(node.left, dict_index_to_value), node.left.dest_size
+#             )
+#             res_signed = left_value_signed >> self.evaluate(node.right, dict_index_to_value)
+#             return signed_to_unsigned(res_signed, node.dest_size) & res_mask
+#         elif node.opcode == ida_hexrays.m_cfadd and node.right is not None:
+#             tmp = get_add_cf(
+#                 self.evaluate(node.left, dict_index_to_value),
+#                 self.evaluate(node.right, dict_index_to_value),
+#                 node.left.dest_size,
+#             )
+#             return tmp & res_mask
+#         elif node.opcode == ida_hexrays.m_ofadd and node.right is not None:
+#             tmp = get_add_of(
+#                 self.evaluate(node.left, dict_index_to_value),
+#                 self.evaluate(node.right, dict_index_to_value),
+#                 node.left.dest_size,
+#             )
+#             return tmp & res_mask
+#         elif node.opcode == ida_hexrays.m_sets:
+#             left_value_signed = unsigned_to_signed(
+#                 self.evaluate(node.left, dict_index_to_value), node.left.dest_size
+#             )
+#             res = 1 if left_value_signed < 0 else 0
+#             return res & res_mask
+#         elif node.opcode == ida_hexrays.m_seto and node.right is not None:
+#             left_value_signed = unsigned_to_signed(
+#                 self.evaluate(node.left, dict_index_to_value), node.left.dest_size
+#             )
+#             right_value_signed = unsigned_to_signed(
+#                 self.evaluate(node.right, dict_index_to_value), node.right.dest_size
+#             )
+#             sub_overflow = get_sub_of(
+#                 left_value_signed, right_value_signed, node.left.dest_size
+#             )
+#             return sub_overflow & res_mask
+#         elif node.opcode == ida_hexrays.m_setnz and node.right is not None:
+#             res = 1 if self.evaluate(node.left, dict_index_to_value) != self.evaluate(node.right, dict_index_to_value) else 0
+#             return res & res_mask
+#         elif node.opcode == ida_hexrays.m_setz and node.right is not None:
+#             res = 1 if self.evaluate(node.left, dict_index_to_value) == self.evaluate(node.right, dict_index_to_value) else 0
+#             return res & res_mask
+#         elif node.opcode == ida_hexrays.m_setae and node.right is not None:
+#             res = 1 if self.evaluate(node.left, dict_index_to_value) >= self.evaluate(node.right, dict_index_to_value) else 0
+#             return res & res_mask
+#         elif node.opcode == ida_hexrays.m_setb and node.right is not None:
+#             res = 1 if self.evaluate(node.left, dict_index_to_value) < self.evaluate(node.right, dict_index_to_value) else 0
+#             return res & res_mask
+#         elif node.opcode == ida_hexrays.m_seta and node.right is not None:
+#             res = 1 if self.evaluate(node.left, dict_index_to_value) > self.evaluate(node.right, dict_index_to_value) else 0
+#             return res & res_mask
+#         elif node.opcode == ida_hexrays.m_setbe and node.right is not None:
+#             res = 1 if self.evaluate(node.left, dict_index_to_value) <= self.evaluate(node.right, dict_index_to_value) else 0
+#             return res & res_mask
+#         elif node.opcode == ida_hexrays.m_setg and node.right is not None:
+#             left_value_signed = unsigned_to_signed(
+#                 self.evaluate(node.left, dict_index_to_value), node.left.dest_size
+#             )
+#             right_value_signed = unsigned_to_signed(
+#                 self.evaluate(node.right, dict_index_to_value), node.right.dest_size
+#             )
+#             res = 1 if left_value_signed > right_value_signed else 0
+#             return res & res_mask
+#         elif node.opcode == ida_hexrays.m_setge and node.right is not None:
+#             left_value_signed = unsigned_to_signed(
+#                 self.evaluate(node.left, dict_index_to_value), node.left.dest_size
+#             )
+#             right_value_signed = unsigned_to_signed(
+#                 self.evaluate(node.right, dict_index_to_value), node.right.dest_size
+#             )
+#             res = 1 if left_value_signed >= right_value_signed else 0
+#             return res & res_mask
+#         elif node.opcode == ida_hexrays.m_setl and node.right is not None:
+#             left_value_signed = unsigned_to_signed(
+#                 self.evaluate(node.left, dict_index_to_value), node.left.dest_size
+#             )
+#             right_value_signed = unsigned_to_signed(
+#                 self.evaluate(node.right, dict_index_to_value), node.right.dest_size
+#             )
+#             res = 1 if left_value_signed < right_value_signed else 0
+#             return res & res_mask
+#         elif node.opcode == ida_hexrays.m_setle and node.right is not None:
+#             left_value_signed = unsigned_to_signed(
+#                 self.evaluate(node.left, dict_index_to_value), node.left.dest_size
+#             )
+#             right_value_signed = unsigned_to_signed(
+#                 self.evaluate(node.right, dict_index_to_value), node.right.dest_size
+#             )
+#             res = 1 if left_value_signed <= right_value_signed else 0
+#             return res & res_mask
+#         elif node.opcode == ida_hexrays.m_setp and node.right is not None:
+#             res = get_parity_flag(
+#                 self.evaluate(node.left, dict_index_to_value),
+#                 self.evaluate(node.right, dict_index_to_value),
+#                 node.left.dest_size,
+#             )
+#             return res & res_mask
+#         elif node.opcode == ida_hexrays.m_call:
+#             if logger.debug_on:
+#                 logger.debug(
+#                     "evaluate m_call: ast_index=%s, dest_size=%s, callee=%s, args=%s",
+#                     node.ast_index,
+#                     node.dest_size,
+#                     node.left,
+#                     node.right,
+#                 )
+#             return 0 & res_mask
+#         else:
+#             raise AstEvaluationException(
+#                 "Can't evaluate opcode: {0}".format(node.opcode)
+#             )
+
+
+_DEFAULT_AST_EVALUATOR = AstEvaluator()
+
+
+cdef class AstProxy(AstBase):
+    cdef public AstBase _target
+    cdef public bint _mutable
+
+    def __init__(self, target_ast: AstBase):
+        self._target = target_ast
+        self._mutable = <bint>False
+
+    def _ensure_mutable(self):
+        """
+        Ensures the target is mutable. If the target is frozen, clone it and
+        replace our internal reference with the new, mutable clone.
+        Skips cloning if already mutable.
+        """
+        if not self._mutable:
+            if self._target.is_frozen():
+                # Clone only if frozen (i.e., shared)
+                self._target = self._target.clone()
+            # Mark as mutable to prevent future clones
+            self._mutable = <bint>True
+
+    def __getattr__(self, name):
+        """
+        Handles all read access to attributes (e.g., proxy.opcode, proxy.left).
+        """
+        # Forward read requests directly to the target (shared or cloned).
+        return getattr(self._target, name)
+
+    def __setattr__(self, name, value):
+        """
+        Handles all write access to attributes (e.g., proxy.ast_index = 5).
+        """
+        if name == "_target" or name == "_mutable":
+            # Special case to allow initialization of the proxy itself.
+            self.__dict__["_target"] = value
+            return
+
+        # 1. Trigger the clone-on-write check.
+        self._ensure_mutable()
+
+        # 2. Perform the write on the (now guaranteed to be mutable) target.
+        setattr(self._target, name, value)
+
+    # You might need to proxy other magic methods if your code uses them
+    # For example, if you use AstNode as a dict:
+    def __getitem__(self, key):
+        getitem = getattr(self._target, "__getitem__", None)
+        if getitem is None:
+            raise AttributeError(
+                f"Object of type {type(self._target)} does not support __getitem__"
+            )
+        return getitem(key)
+
+    def __setitem__(self, key, value):
+        setitem = getattr(self._target, "__setitem__", None)
+        if setitem is None:
+            raise AttributeError(
+                f"Object of type {type(self._target)} does not support __setitem__"
+            )
+        self._ensure_mutable()
+        setitem(key, value)
+
+    # ------------------------------------------------------------------
+    # Transparent attribute forwarding with sane fallback.
+    # ------------------------------------------------------------------
+
+    def __getattribute__(self, name):  # noqa: D401, ANN001
+        """Forward *all* attribute access to the wrapped target when:
+        1) the attribute is not private to the proxy itself, and
+        2) the value obtained from the proxy's own namespace is *None*.
+
+        This retains the cheap class-level default attributes coming from
+        AstBase (all set to None) while still exposing the real runtime
+        values stored in the wrapped AST object.
+        """
+
+        # Fast-path: internal/private attributes stay local.
+        if name.startswith("_"):
+            return super().__getattribute__(name)
+
+        try:
+            val = super().__getattribute__(name)
+        except AttributeError:
+            # Attribute not present on proxy → delegate unconditionally.
+            return getattr(super().__getattribute__("_target"), name)
+
+        # If the proxy's value is a meaningless placeholder (None) but the
+        # underlying object has a better value, return the latter instead.
+        if val is None:
+            target = super().__getattribute__("_target")
+            return getattr(target, name)
+        return val
+
+    def is_frozen(self) -> bool:
+        return self._target.is_frozen()
+
+    @_compat.override
+    def clone(self) -> AstBase:
+        return AstProxy(self._target.clone())
+
+    @_compat.override
+    def freeze(self) -> None:
+        self._target.freeze()
+
+    @_compat.override
+    def is_node(self) -> bool:
+        return self._target.is_node()
+
+    @_compat.override
+    def is_leaf(self) -> bool:
+        return self._target.is_leaf()
+
+    @_compat.override
+    def is_constant(self) -> bool:
+        return self._target.is_constant()
+
+    @_compat.override
+    def compute_sub_ast(self) -> None:
+        self._target.compute_sub_ast()
+
+    @_compat.override
+    def get_leaf_list(self) -> list[AstLeaf]:
+        return self._target.get_leaf_list()
+
+    @_compat.override
+    def reset_mops(self) -> None:
+        self._target.reset_mops()
+
+    @_compat.override
+    def _copy_mops_from_ast(self, other: AstBase) -> bool:
+        return self._target._copy_mops_from_ast(other)
+
+    @_compat.override
+    def create_mop(self, ea: int) -> ida_hexrays.mop_t:
+        return self._target.create_mop(ea)
+
+    @_compat.override
+    def get_pattern(self) -> str:
+        return self._target.get_pattern()
+
+    @_compat.override
+    def evaluate(self, dict_index_to_value: dict[int, int]) -> int:
+        return self._target.evaluate(dict_index_to_value)
+
+    @_compat.override
+    def get_depth_signature(self, depth: int) -> list[str]:
+        return self._target.get_depth_signature(depth)
+
+    @_compat.override
+    def __str__(self):
+        return f"AstProxy({self._target.__class__.__name__}({str(self._target)}))"
+
+    @_compat.override
+    def __repr__(self):
+        return f"AstProxy({repr(self._target)})"
+
+    # Explicitly forward critical leaf data that callers expect to access
+    # directly.  Without these properties Python finds the *class*-level
+    # attribute defined in AstBase (value = None) and never triggers
+    # __getattr__, so evaluators see a leaf with no mop.
+
+    # ‑-- Mop -------------------------------------------------------------
+    @property
+    def mop(self):  # type: ignore[override]
+        return self._target.mop
+
+    @mop.setter
+    def mop(self, value):  # noqa: ANN001
+        self._ensure_mutable()
+        self._target.mop = value
+
+    # Convenience setters for a few commonly mutated fields
+    @property
+    def dest_size(self):  # type: ignore[override]
+        return self._target.dest_size
+
+    @dest_size.setter
+    def dest_size(self, value):  # noqa: ANN001
+        self._ensure_mutable()
+        self._target.dest_size = value
+
+    @property
+    def ea(self):  # type: ignore[override]
+        return self._target.ea
+
+    @ea.setter
+    def ea(self, value):  # noqa: ANN001
+        self._ensure_mutable()
+        self._target.ea = value
+
+    @property
+    def ast_index(self):  # type: ignore[override]
+        return self._target.ast_index
+
+    @ast_index.setter
+    def ast_index(self, value):  # noqa: ANN001
+        self._ensure_mutable()
+        self._target.ast_index = value
+
+
+cdef class AstBuilderContext:
+    """
+    Manages the state during the recursive construction of an AST.
+    This avoids passing multiple related arguments through the recursion
+    and provides a clean way to store the lookup dictionary.
+    """    
+    cdef public list[AstBase] unique_asts
+    cdef public dict[tuple[int, str], int] mop_key_to_index
+
+    def __cinit__(self):
+        self.unique_asts = []
+        self.mop_key_to_index = {}
+
+
+def get_mop_key(mop: ida_hexrays.mop_t) -> tuple:
+    """
+    Generates a fast, hashable key for a `mop_t` using the structural hash.
+    Avoids `dstr()` entirely and ignores SSA valnum differences.
+    """
+    t = mop.t
+    sz = mop.size
+    try:
+        h = int(structural_mop_hash(mop, 0))
+        return (t, sz, h)
+    except Exception:
+        # Fallback: rely on cheap structural fields only; still avoid dstr().
+        if t == ida_hexrays.mop_n:
+            return (t, sz, mop.valnum, mop.nnn.value)
+        elif t == ida_hexrays.mop_r:
+            return (t, sz, mop.r)
+        elif t == ida_hexrays.mop_d:
+            # Use EA if available; do not call dstr()
+            return (t, sz, mop.d.ea if mop.d else idaapi.BADADDR)
+        elif t == ida_hexrays.mop_S:
+            return (t, sz, mop.s.off)
+        elif t == ida_hexrays.mop_v:
+            return (t, sz, mop.g)
+        elif t == ida_hexrays.mop_l:
+            return (t, sz, mop.l.idx, mop.l.off)
+        elif t == ida_hexrays.mop_b:
+            return (t, sz, mop.b)
+        elif t == ida_hexrays.mop_h:
+            return (t, sz, mop.helper)
+        elif t == ida_hexrays.mop_str:
+            return (t, sz, mop.cstr)
+        else:
+            # Last resort: identity-based key
+            return (t, sz, id(mop))
+
+
+# def get_mop_key_cy(mop):
+#     """
+#     Generates a fast, hashable key from a mop_t's essential attributes.
+#     Cython version that takes Python mop_t for compatibility.
+#     """
+#     cdef int t = mop.t
+    
+#     # Build base key - same logic as Python
+#     key = (t, mop.size) if t != ida_hexrays.mop_n else (t, mop.size, mop.valnum)
+    
+#     if t == ida_hexrays.mop_n:
+#         return key + (mop.nnn.value,)
+#     elif t == ida_hexrays.mop_r:
+#         return key + (mop.r,)
+#     elif t == ida_hexrays.mop_d:
+#         try:
+#             return key + (mop.dstr(),)
+#         except:
+#             if mop.d:
+#                 return key + (mop.d.ea,)
+#             else:
+#                 return key + (idaapi.BADADDR,)
+#     elif t == ida_hexrays.mop_S:
+#         return key + (mop.s.off,)
+#     elif t == ida_hexrays.mop_v:
+#         return key + (mop.g,)
+#     elif t == ida_hexrays.mop_l:
+#         return key + (mop.l.idx, mop.l.off)
+#     elif t == ida_hexrays.mop_b:
+#         return key + (mop.b,)
+#     elif t == ida_hexrays.mop_h:
+#         return key + (mop.helper,)
+#     elif t == ida_hexrays.mop_str:
+#         return key + (mop.cstr,)
+#     else:
+#         try:
+#             return key + (mop.dstr(),)
+#         except:
+#             return key + (f"unsupported_mop_t_{t}",)
+
+
+def mop_to_ast_internal(
+    mop: ida_hexrays.mop_t, context: AstBuilderContext, root: bool = False
+) -> AstBase | None:
+    # Only log at root
+    if root and logger.debug_on:
+        logger.debug(
+            "[mop_to_ast_internal] Processing root mop: %s",
+            str(mop.dstr()) if hasattr(mop, "dstr") else str(mop),
+        )
+
+    # Early filter at root: only process if supported, with one exception:
+    # If the root is an m_call that has no argument list (r is mop_z) we treat it
+    # as transparent and attempt to build an AST from its destination operand.
+    if root:
+        if hasattr(mop, "d") and hasattr(mop.d, "opcode"):
+            root_opcode = mop.d.opcode
+
+            # Transparent helper call wrappers are now normalised by a
+            # peephole pass (TransparentCallUnwrapRule).  No special handling
+            # needed here anymore.
+
+            if root_opcode not in MBA_RELATED_OPCODES and not is_rotate_helper_call(
+                mop.d
+            ):
+                if logger.debug_on:
+                    logger.debug(
+                        "Skipping AST build for unsupported root opcode: %s",
+                        opcode_to_string(root_opcode),
+                    )
+                return None
+
+    # 1. Create the unique, hashable key for the current mop.
+    key = get_mop_key(mop)
+
+    # 2. Thread-local deduplication: if we've already built an AST for *this*
+    #    mop during the current recursive walk, return the existing instance to
+    #    avoid exponential explosion.
+    if key in context.mop_key_to_index:
+        existing_index = context.mop_key_to_index[key]
+        return context.unique_asts[existing_index]
+
+    # Rotate helper calls (__ROL*/__ROR*) are now inlined into plain shift/or
+    # instructions by RotateHelperInlineRule (peephole, MMAT_GLBOPT1).
+    # No special handling required here.
+
+    # Helper calls that evaluate to constants are now canonicalised by
+    # ConstantCallResultFoldRule (peephole GLBOPT1).
+
+    # NEW: Build AST nodes for MBA-related opcodes (binary or unary)
+    if mop.t == ida_hexrays.mop_d and mop.d.opcode in MBA_RELATED_OPCODES:
+        nb_ops = OPCODES_INFO[mop.d.opcode]["nb_operands"]
+
+        # Gather children ASTs based on operand count
+        left_ast = (
+            mop_to_ast_internal(mop.d.l, context) if mop.d.l is not None else None
+        )
+        right_ast = (
+            mop_to_ast_internal(mop.d.r, context)
+            if (nb_ops >= 2 and mop.d.r is not None)
+            else None
+        )
+
+        # Require at least the mandatory operands; if missing, fall back to leaf
+        if left_ast is None:
+            # Can't build meaningful node - fallback later to leaf
+            if logger.debug_on:
+                logger.debug(
+                    "[mop_to_ast_internal] Missing mandatory operand(s) for opcode %s, will treat as leaf",
+                    opcode_to_string(mop.d.opcode),
+                )
+        else:
+            # Only use dst_ast if destination present (ternary ops like m_stx etc.)
+            dst_ast = (
+                mop_to_ast_internal(mop.d.d, context) if mop.d.d is not None else None
+            )
+            tree = AstNode(mop.d.opcode, left_ast, right_ast, dst_ast)
+
+            # Set dest_size robustly
+            if hasattr(mop, "size") and mop.size:
+                tree.dest_size = mop.size
+            elif hasattr(mop.d, "size") and mop.d.size:
+                tree.dest_size = mop.d.size
+            elif mop.d.l is not None and hasattr(mop.d.l, "size"):
+                tree.dest_size = mop.d.l.size
+            else:
+                tree.dest_size = None
+
+            tree.mop = mop
+            tree.ea = sanitize_ea(mop.d.ea)
+
+            if logger.debug_on:
+                logger.debug(
+                    "[mop_to_ast_internal] Created AstNode for opcode %s (ea=0x%X): %s",
+                    opcode_to_string(mop.d.opcode),
+                    mop.d.ea if hasattr(mop.d, "ea") else -1,
+                    tree,
+                )
+            new_index = len(context.unique_asts)
+            tree.ast_index = new_index
+            context.unique_asts.append(tree)
+            context.mop_key_to_index[key] = new_index
+            return tree
+
+    # Special handling for mop_d that wraps an m_ldc as a constant leaf
+    if (
+        mop.t == ida_hexrays.mop_d
+        and mop.d is not None
+        and mop.d.opcode == ida_hexrays.m_ldc
+    ):
+        # Only treat it as constant if the *source* of the ldc is itself a
+        # numeric constant.  Otherwise we ignore the ldc wrapper and fall
+        # back to the generic leaf logic below.
+        ldc_src = mop.d.l
+        if ldc_src is not None and ldc_src.t == ida_hexrays.mop_n:
+            const_val = int(ldc_src.nnn.value)
+            const_size = ldc_src.size
+
+            const_leaf = AstConstant(hex(const_val), const_val, const_size)
+            # Clone numeric mop to detach from Hex-Rays internal storage
+            cloned_mop = ida_hexrays.mop_t()
+            cloned_mop.make_number(const_val, const_size)
+            const_leaf.mop = cloned_mop
+            const_leaf.dest_size = const_size
+
+            new_index = len(context.unique_asts)
+            const_leaf.ast_index = new_index
+            context.unique_asts.append(const_leaf)
+            context.mop_key_to_index[key] = new_index
+            return const_leaf
+
+    # Fallback for any unhandled mop: treat as a leaf.
+    # This is for simple operands (registers, stack vars) or complex
+    # instructions that are not part of our MBA analysis.
+    if (
+        mop.t != ida_hexrays.mop_d
+        or (mop.d.opcode not in MBA_RELATED_OPCODES)
+        or mop.d.l is None
+        or mop.d.r is None
+    ):
+        tree: AstBase | None
+        if mop.t == ida_hexrays.mop_n:
+            const_val = int(mop.nnn.value)
+            const_size = mop.size
+            tree = AstConstant(hex(const_val), const_val, const_size)
+            # Re-use a shared constant mop_t from the global cache to avoid the
+            # overhead of allocating a fresh object for every identical literal.
+            tree.mop = get_constant_mop(const_val, const_size)
+            tree.dest_size = const_size  # detached copy
+        # Typed-immediate wrappers (mop_f) are now normalised by the
+        # TypedImmediateCanonicaliseRule peephole pass.  If we still see one
+        # here it means it holds *no* literal value, therefore fall through to
+        # generic leaf creation.
+        elif mop.t == ida_hexrays.mop_f:
+            tree = None
+        else:
+            tree = None
+
+        # ------------------------------------------------------------------
+        # If we still haven't built a node, create a generic AstLeaf now.  This
+        # guarantees that *tree* is always defined even if new mop_t kinds are
+        # introduced in future IDA versions.
+        # ------------------------------------------------------------------
+        if tree is None:
+            tree = AstLeaf(format_mop_t(mop))
+            if logger.debug_on:
+                logger.debug(
+                    "[mop_to_ast_internal] Tree is NONE! Defaulting to AstLeaf for mop type %s dstr=%s",
+                    mop_type_to_string(mop.t),
+                    str(mop.dstr()) if hasattr(mop, "dstr") else str(mop),
+                )
+            tree.dest_size = mop.size
+
+        # For non-constant leaves we deliberately *do not* keep a reference
+        # to the original mop_t object, because Hex-Rays may free or reuse
+        # it after micro-optimisations, leading to use-after-free crashes.
+        # Only constant leaves benefit from holding the numeric mop to
+        # speed up further evaluations.
+        if tree.is_constant():
+            tree.mop = getattr(tree, "mop", None) or mop
+        else:
+            tree = AstLeaf(format_mop_t(mop))
+            if logger.debug_on:
+                logger.debug(
+                    "[mop_to_ast_internal] Fallback to AstLeaf for mop type %s dstr=%s",
+                    mop_type_to_string(mop.t),
+                    str(mop.dstr()) if hasattr(mop, "dstr") else str(mop),
+                )
+            tree.dest_size = mop.size
+
+        # Preserve previously assigned mop (e.g., inner numeric mop) unless
+        # it is still unset.  This prevents clobbering the constant `mop_n`
+        # we stored above with the wrapper operand, which would break
+        # constant detection later in the pipeline.
+        if getattr(tree, "mop", None) is None:
+            tree.mop = mop
+        dest_size = (
+            mop.size
+            if mop.t != ida_hexrays.mop_d
+            else mop.d.d.size if mop.d.d is not None else mop.size
+        )
+        tree.dest_size = dest_size
+        new_index = len(context.unique_asts)
+        tree.ast_index = new_index
+        context.unique_asts.append(tree)
+        context.mop_key_to_index[key] = new_index
+        return tree
+
+    # If we reach here, we failed to build an AST. Log the full mop tree.
+    logger.error("[mop_to_ast_internal] Could not build AST for mop. Dumping mop tree:")
+    mop_tree(mop)
+    return None
+
+
+
+# # Port mop_to_ast_internal as a cdef function
+# # Use 'object' for mop initially for compatibility with SWIG wrapper
+# cdef object mop_to_ast_internal_cy(object mop, AstBuilderContext bldr_ctx, object ctx, bint root=False):
+#     # Use project-specific debug flag
+#     if root and logger.debug_on:
+#         logger.debug(
+#             "[mop_to_ast_internal] Processing root mop: %s",
+#             str(mop.dstr()) if hasattr(mop, "dstr") else str(mop),
+#         )
+
+#     # Early filter at root - MATCH PYTHON VERSION EXACTLY
+#     if root:
+#         if hasattr(mop, "d") and hasattr(mop.d, "opcode"):
+#             root_opcode = mop.d.opcode
+            
+#             # Check against MBA_RELATED_OPCODES and rotate helpers
+#             if root_opcode not in MBA_RELATED_OPCODES and not is_rotate_helper_call(mop.d):
+#                 if logger.debug_on:
+#                     logger.debug(
+#                         "Skipping AST build for unsupported root opcode: %s",
+#                         opcode_to_string(root_opcode),
+#                     )
+#                 return None
+
+#     # 1. Create the unique, hashable key for the current mop.
+#     key = get_mop_key_cy(mop)
+    
+#     # 2. Thread-local deduplication
+#     if key in bldr_ctx.mop_key_to_index:
+#         existing_index = bldr_ctx.mop_key_to_index[key]
+#         return bldr_ctx.unique_asts[existing_index]
+
+#     # NEW: Build AST nodes for MBA-related opcodes
+#     # In the MBA_RELATED_OPCODES section:
+#     if (mop.t == ida_hexrays.mop_d and 
+#         mop.d is not None and  # Add this check
+#         hasattr(mop.d, 'opcode') and  # Add this check
+#         mop.d.opcode in MBA_RELATED_OPCODES):
+        
+#         # Additional safety checks
+#         if mop.d.l is None:
+#             if logger.debug_on:
+#                 logger.debug("[mop_to_ast_internal] Missing left operand for opcode %s", 
+#                             opcode_to_string(mop.d.opcode))
+#         nb_ops = OPCODES_INFO[mop.d.opcode]["nb_operands"]
+#         # Gather children ASTs based on operand count
+#         # Recursive calls to the Cython version
+#         left_ast = mop_to_ast_internal_cy(mop.d.l, bldr_ctx, ctx) if mop.d.l is not None else None
+#         right_ast = (
+#             mop_to_ast_internal_cy(mop.d.r, bldr_ctx, ctx)
+#             if (nb_ops >= 2 and mop.d.r is not None)
+#             else None
+#         )
+
+#         # Require at least the mandatory operands
+#         if left_ast is None:
+#             # Can't build meaningful node - fallback later to leaf
+#             if logger.debug_on:
+#                 logger.debug(
+#                     "[mop_to_ast_internal] Missing mandatory operand(s) for opcode %s, will treat as leaf",
+#                     opcode_to_string(mop.d.opcode),
+#                 )
+#         else:
+#             # Only use dst_ast if destination present
+#             dst_ast = mop_to_ast_internal_cy(mop.d.d, bldr_ctx, ctx) if mop.d.d is not None else None
+
+#             # Create AstNode (Python object)
+#             tree = AstNode(mop.d.opcode, left_ast, right_ast, dst_ast)
+
+#             # Set dest_size robustly
+#             if hasattr(mop, "size") and mop.size:
+#                 tree.dest_size = mop.size
+#             elif hasattr(mop.d, "size") and mop.d.size:
+#                 tree.dest_size = mop.d.size
+#             elif mop.d.l is not None and hasattr(mop.d.l, "size"):
+#                 tree.dest_size = mop.d.l.size
+#             else:
+#                 tree.dest_size = None
+
+#             tree.mop = mop
+#             tree.ea = sanitize_ea(mop.d.ea)
+
+#             if logger.debug_on:
+#                 logger.debug(
+#                     "[mop_to_ast_internal] Created AstNode for opcode %s (ea=0x%X): %s",
+#                     opcode_to_string(mop.d.opcode),
+#                     mop.d.ea if hasattr(mop.d, "ea") else -1,
+#                     tree,
+#                 )
+
+#             new_index = len(bldr_ctx.unique_asts)
+#             tree.ast_index = new_index
+#             bldr_ctx.unique_asts.append(tree)
+#             bldr_ctx.mop_key_to_index[key] = new_index
+#             return tree
+
+#     # Special handling for mop_d that wraps an m_ldc as a constant leaf
+#     if (
+#         mop.t == ida_hexrays.mop_d
+#         and mop.d is not None
+#         and mop.d.opcode == ida_hexrays.m_ldc
+#     ):
+#         ldc_src = mop.d.l
+#         if ldc_src is not None and ldc_src.t == ida_hexrays.mop_n:
+#             const_val = int(ldc_src.nnn.value)
+#             const_size = ldc_src.size
+#             const_leaf = AstConstant(hex(const_val), const_val, const_size)
+
+#             # Use get_constant_mop (Python function) - Reuse shared mop
+#             const_leaf.mop = ctx["get_constant_mop_py"](const_val, const_size)
+#             const_leaf.dest_size = const_size
+
+#             new_index = len(bldr_ctx.unique_asts)
+#             const_leaf.ast_index = new_index
+#             bldr_ctx.unique_asts.append(const_leaf)
+#             bldr_ctx.mop_key_to_index[key] = new_index
+#             return const_leaf
+
+#     # Fallback for any unhandled mop: treat as a leaf.
+#     tree = None
+#     if mop.t == ida_hexrays.mop_n:
+#         const_val = int(mop.nnn.value)
+#         const_size = mop.size
+#         tree = AstConstant(hex(const_val), const_val, const_size)
+#         # Re-use a shared constant mop_t
+#         tree.mop = ctx["get_constant_mop_py"](const_val, const_size)
+#         tree.dest_size = const_size
+
+#     elif mop.t == ida_hexrays.mop_f:
+#         tree = None
+#     else:
+#         tree = None
+
+#     # ------------------------------------------------------------------
+#     # If we still haven't built a node, create a generic AstLeaf now.
+#     # ------------------------------------------------------------------
+#     if tree is None:
+#         tree = AstLeaf(format_mop_t(mop))
+#         if logger.debug_on:
+#             logger.debug(
+#                 "[mop_to_ast_internal] Tree is NONE! Defaulting to AstLeaf for mop type %s dstr=%s",
+#                 mop_type_to_string(mop.t),
+#                 str(mop.dstr()) if hasattr(mop, "dstr") else str(mop),
+#             )
+#         tree.dest_size = mop.size
+
+#     # For non-constant leaves, create a new AstLeaf (as per original logic)
+#     # Note: The original Python logic seems to have a duplicate/overwriting section here.
+#     # The ported version reflects the final outcome of that logic.
+#     if not tree.is_constant():
+#         tree = AstLeaf(format_mop_t(mop)) # Overwrite with generic leaf
+#         if logger.debug_on:
+#             logger.debug(
+#                 "[mop_to_ast_internal] Fallback to AstLeaf for mop type %s dstr=%s",
+#                 mop_type_to_string(mop.t),
+#                 str(mop.dstr()) if hasattr(mop, "dstr") else str(mop),
+#             )
+#         tree.dest_size = mop.size
+
+#     # Preserve previously assigned mop for constants, assign mop for others
+#     if getattr(tree, "mop", None) is None:
+#         tree.mop = mop
+
+#     # Determine dest_size (mirroring original logic's attempt)
+#     dest_size = (
+#         mop.size
+#         if mop.t != ida_hexrays.mop_d
+#         else (mop.d.d.size if mop.d.d is not None else mop.size)
+#     )
+#     tree.dest_size = dest_size
+
+#     new_index = len(bldr_ctx.unique_asts)
+#     tree.ast_index = new_index
+#     bldr_ctx.unique_asts.append(tree)
+#     bldr_ctx.mop_key_to_index[key] = new_index
+#     return tree
+
+#     # If we reach here, we failed to build an AST.
+#     # logger.error("[mop_to_ast_internal] Could not build AST for mop. Dumping mop tree (not implemented in Cython port yet)")
+#     # mop_tree(mop) # Assuming mop_tree is a Python function, might need special handling or commenting out for now
+#     # return None # Implicit return None at end of function
+
+
+def mop_to_ast(mop: ida_hexrays.mop_t) -> AstProxy | None:
+    """
+    Converts a mop_t to an AST node, with caching to avoid re-computation.
+
+    Returns a deep copy of the cached AST to prevent side-effects from
+    mutations by the caller.
+    """
+
+    # 1. Create a stable, hashable key from the mop_t object.
+    cache_key = get_mop_key(mop)
+
+    # 2. Global template cache: return a proxy if we already know the template
+    if cache_key in MOP_TO_AST_CACHE:
+        cached_template = MOP_TO_AST_CACHE[cache_key]
+        if cached_template is None:
+            return None  # Previously determined unconvertible.
+        return AstProxy(cached_template)
+
+    builder_context = AstBuilderContext()
+    # Start the optimized recursive build.
+
+    if not (mop_ast := mop_to_ast_internal(mop, builder_context, root=True)):
+        # Cache the failure to avoid re-computing it.
+        MOP_TO_AST_CACHE[cache_key] = None
+        return None
+
+    # This mutates the mop_ast object, populating its sub_ast_info.
+    # We do this ONCE before caching the "template" object, then we
+    # freeze the object to prevent mutations.
+    mop_ast.compute_sub_ast()
+    mop_ast.freeze()
+
+    # 4. Store the newly computed "template" object in the cache.
+    MOP_TO_AST_CACHE[cache_key] = mop_ast
+
+    # 5. Return a proxy to the caller for safety.
+    return AstProxy(mop_ast)
+
+
+
+# # Port mop_to_ast as a cdef function
+# cdef object mop_to_ast_cy(object mop, object ctx):
+#     """
+#     Converts a mop_t to an AST node, with caching to avoid re-computation.
+#     Returns a deep copy of the cached AST to prevent side-effects from
+#     mutations by the caller.
+#     """
+#     # 1. Create a stable, hashable key from the mop_t object.
+#     # Call the Python get_mop_key function
+#     cache_key = get_mop_key_cy(mop)
+
+#     # 2. Global template cache: return a proxy if we already know the template
+#     # Access the global Python MOP_TO_AST_CACHE dict
+#     if cache_key in ctx["mop_to_ast_cache_py"]:
+#         cached_template = ctx["mop_to_ast_cache_py"][cache_key]
+#         if cached_template is None:
+#             return None  # Previously determined unconvertible.
+#         # Return ctx["ast_proxy_py"] (Python object)
+#         return ctx["ast_proxy_py"](cached_template)
+
+#     # Create builder context
+#     builder_context = AstBuilderContext()
+
+#     # Start the optimized recursive build.
+#     # Call the Cython version
+#     mop_ast = mop_to_ast_internal_cy(mop, builder_context, ctx, root=True)
+#     if not mop_ast:
+#         # Cache the failure to avoid re-computing it.
+#         ctx["mop_to_ast_cache_py"][cache_key] = None
+#         return None
+
+#     # This mutates the mop_ast object, populating its sub_ast_info.
+#     mop_ast.compute_sub_ast()
+#     mop_ast.freeze()
+
+#     # 4. Store the newly computed "template" object in the cache.
+#     ctx["mop_to_ast_cache_py"][cache_key] = mop_ast
+
+#     # 5. Return a proxy to the caller for safety.
+#     return ctx["ast_proxy_py"](mop_ast)
+
+
+def minsn_to_ast(instruction: ida_hexrays.minsn_t) -> AstProxy | None:
+    try:
+        # Early filter: forbidden opcodes
+        if instruction.opcode in MINSN_TO_AST_FORBIDDEN_OPCODES:
+            if logger.debug_on:
+                logger.debug(
+                    "Skipping AST build for forbidden opcode: %s @ 0x%x %s",
+                    opcode_to_string(instruction.opcode),
+                    instruction.ea,
+                    (
+                        "({0})".format(instruction.dstr())
+                        if instruction.opcode != ida_hexrays.m_jtbl
+                        else ""
+                    ),
+                )
+            return None
+
+        # Early filter: unsupported opcodes (not in MBA_RELATED_OPCODES)
+        # Allow rotate helper calls ("__ROL*" / "__ROR*") even though m_call
+        # is normally filtered out - they can be constant-folded later.
+        if instruction.opcode not in MBA_RELATED_OPCODES and not is_rotate_helper_call(
+            instruction
+        ):
+            if logger.debug_on:
+                logger.debug(
+                    "Skipping AST build for unsupported opcode: %s @ 0x%x %s",
+                    opcode_to_string(instruction.opcode),
+                    instruction.ea,
+                    (
+                        "({0})".format(instruction.dstr())
+                        if instruction.opcode != ida_hexrays.m_jtbl
+                        else ""
+                    ),
+                )
+            return None
+
+        # Constant-returning helper calls are folded to m_ldc by the peephole
+        # pass ConstantCallResultFoldRule.  No need for AST special case.
+
+        # Transparent-call shortcut: no args, computation stored in destination mop_d
+        if (
+            instruction.opcode == ida_hexrays.m_call
+            and (instruction.r is None or instruction.r.t == ida_hexrays.mop_z)
+            and instruction.d is not None
+            and instruction.d.t == ida_hexrays.mop_d
+        ):
+            if logger.debug_on:
+                logger.debug(
+                    "[minsn_to_ast] Unwrapping call with empty args; using destination expression for AST",
+                )
+            dest_ast = mop_to_ast(instruction.d)
+            if dest_ast is not None:
+                return dest_ast
+
+        ins_mop = ida_hexrays.mop_t()
+        ins_mop.create_from_insn(instruction)
+
+        # if instruction.opcode == ida_hexrays.m_mov:
+        #     tmp = AstNode(ida_hexrays.m_mov, mop_to_ast(ins_mop))
+        #     tmp.mop = ins_mop
+        #     tmp.dest_size = instruction.d.size
+        #     tmp.ea = instruction.ea
+        #     tmp.dst_mop = instruction.d
+        #     return tmp
+
+        tmp = mop_to_ast(ins_mop)
+        if tmp is None:
+            if logger.debug_on:
+                logger.debug(
+                    "Skipping AST build for unsupported or nop instruction: %s @ 0x%x %s",
+                    opcode_to_string(instruction.opcode),
+                    instruction.ea,
+                    (
+                        "({0})".format(instruction.dstr())
+                        if instruction.opcode != ida_hexrays.m_jtbl
+                        else ""
+                    ),
+                )
+        else:
+            tmp.dst_mop = instruction.d
+        return tmp
+    except RuntimeError as e:
+        logger.error(
+            "Error while transforming instruction %s: %s",
+            format_minsn_t(instruction),
+            e,
+        )
+
+
+
+# def minsn_to_ast_cy(
+#     object ins_py,
+#     object mop_to_ast_cache_py,
+#     object ast_proxy_py,
+#     object ast_node_py,
+#     object ast_leaf_py,
+#     object ast_constant_py,
+#     object get_constant_mop_py,
+#     object mba_related_opcodes_py,
+# ):
+#     """
+#     Public entry point matching the signature expected by the original ast.py fast path.
+#     """
+#     cdef:
+#         object ins_mop
+#         object dest_ast
+#         object result
+#     try:
+#         # Early filter: forbidden opcodes
+#         if ins_py.opcode in MINSN_TO_AST_FORBIDDEN_OPCODES:
+#             if logger.debug_on:
+#                 logger.debug(
+#                     "Skipping AST build for forbidden opcode: %s @ 0x%x",
+#                     opcode_to_string(ins_py.opcode),
+#                     ins_py.ea,
+#                 )
+#             return None
+
+#         # Early filter: unsupported opcodes (not in MBA_RELATED_OPCODES)
+#         # Allow rotate helper calls ("__ROL*" / "__ROR*") 
+#         if ins_py.opcode not in MBA_RELATED_OPCODES and not is_rotate_helper_call(ins_py):
+#             if logger.debug_on:
+#                 logger.debug(
+#                     "Skipping AST build for unsupported opcode: %s @ 0x%x",
+#                     opcode_to_string(ins_py.opcode),
+#                     ins_py.ea,
+#                 )
+#             return None
+
+#         # Transparent-call shortcut: no args, computation stored in destination mop_d
+#         if (
+#             ins_py.opcode == ida_hexrays.m_call
+#             and (ins_py.r is None or ins_py.r.t == ida_hexrays.mop_z)
+#             and ins_py.d is not None
+#             and ins_py.d.t == ida_hexrays.mop_d
+#         ):
+#             if logger.debug_on:
+#                 logger.debug(
+#                     "[minsn_to_ast] Unwrapping call with empty args; using destination expression for AST",
+#                 )
+#             # For this case, we need to process the destination mop
+#             ins_mop = ida_hexrays.mop_t()
+#             ins_mop.create_from_insn(ins_py)
+            
+#             kwargs = {
+#                 "mop_to_ast_cache_py": mop_to_ast_cache_py,
+#                 "ast_proxy_py": ast_proxy_py,
+#                 "ast_node_py": ast_node_py,
+#                 "ast_leaf_py": ast_leaf_py,
+#                 "ast_constant_py": ast_constant_py,
+#                 "get_constant_mop_py": get_constant_mop_py,
+#                 "mba_related_opcodes_py": mba_related_opcodes_py,
+#             }
+            
+#             dest_ast = mop_to_ast_cy(ins_py.d, kwargs)
+#             return dest_ast
+
+#         # Create mop from instruction
+#         ins_mop = ida_hexrays.mop_t()
+#         ins_mop.create_from_insn(ins_py)
+        
+#         kwargs = {
+#             "mop_to_ast_cache_py": mop_to_ast_cache_py,
+#             "ast_proxy_py": ast_proxy_py,
+#             "ast_node_py": ast_node_py,
+#             "ast_leaf_py": ast_leaf_py,
+#             "ast_constant_py": ast_constant_py,
+#             "get_constant_mop_py": get_constant_mop_py,
+#             "mba_related_opcodes_py": mba_related_opcodes_py,
+#         }
+        
+#         result = mop_to_ast_cy(ins_mop, kwargs)
+        
+#         # Set dst_mop if result exists (to match Python behavior)
+#         if result is not None:
+#             result.dst_mop = ins_py.d
+            
+#         return result
+        
+#     except RuntimeError as e:
+#         logger.error(
+#             "Error while transforming instruction %s: %s",
+#             str(ins_py.dstr()) if hasattr(ins_py, "dstr") else str(ins_py),
+#             e,
+#             exc_info=True,
+#         )
+#         raise
+#     except Exception as e:
+#         logger.error(
+#             "Unexpected error while transforming instruction %s: %s",
+#             str(ins_py.dstr()) if hasattr(ins_py, "dstr") else str(ins_py),
+#             e,
+#             exc_info=True,
+#         )
+#         raise
diff --git a/src/d810/expr/_ast_evaluate.pyx b/src/d810/expr/_ast_evaluate.pyx
new file mode 100644
index 0000000..b43bfb6
--- /dev/null
+++ b/src/d810/expr/_ast_evaluate.pyx
@@ -0,0 +1,334 @@
+# distutils: language = c++
+# cython: language_level=3, embedsignature=True
+# cython: cdivision=True
+# distutils: define_macros=__EA64__=1
+from __future__ import annotations
+
+import ida_hexrays
+
+from d810.conf.loggers import getLogger
+from d810.errors import AstEvaluationException
+from d810.expr.utils import (
+    get_add_cf,
+    get_add_of,
+    get_parity_flag,
+    get_sub_of,
+    signed_to_unsigned,
+    unsigned_to_signed,
+)
+from d810.hexrays.hexrays_helpers import AND_TABLE
+
+logger = getLogger(__name__)
+
+from d810.expr._ast import AstConstant, AstLeaf, AstNode, AstProxy
+
+
+cdef object _BINARY_OPCODES = frozenset((
+    ida_hexrays.m_add,
+    ida_hexrays.m_sub,
+    ida_hexrays.m_mul,
+    ida_hexrays.m_udiv,
+    ida_hexrays.m_sdiv,
+    ida_hexrays.m_umod,
+    ida_hexrays.m_smod,
+    ida_hexrays.m_or,
+    ida_hexrays.m_and,
+    ida_hexrays.m_xor,
+    ida_hexrays.m_shl,
+    ida_hexrays.m_shr,
+    ida_hexrays.m_sar,
+    ida_hexrays.m_cfadd,
+    ida_hexrays.m_ofadd,
+    ida_hexrays.m_seto,
+    ida_hexrays.m_setnz,
+    ida_hexrays.m_setz,
+    ida_hexrays.m_setae,
+    ida_hexrays.m_setb,
+    ida_hexrays.m_seta,
+    ida_hexrays.m_setbe,
+    ida_hexrays.m_setg,
+    ida_hexrays.m_setge,
+    ida_hexrays.m_setl,
+    ida_hexrays.m_setle,
+    ida_hexrays.m_setp,
+))
+
+
+cdef class AstEvaluator:
+    """
+    Pure-Python evaluator for AST nodes. Extracted from AstNode/AstLeaf methods
+    to centralize evaluation logic.
+    """
+
+    cpdef object evaluate_with_leaf_info(self, object node, object leafs_info, object leafs_value):
+        dict_index_to_value = {}
+        for leaf_info, leaf_value in zip(leafs_info, leafs_value):
+            if leaf_info.ast.ast_index is not None:
+                dict_index_to_value[leaf_info.ast.ast_index] = leaf_value
+        return self.evaluate(node, dict_index_to_value)
+
+    cpdef object evaluate(self, object node, dict dict_index_to_value):
+        if isinstance(node, AstNode):
+            return self._eval_node(node, dict_index_to_value)
+        if isinstance(node, AstLeaf):
+            return self._eval_leaf(node, dict_index_to_value)
+        if isinstance(node, AstProxy):
+            return self.evaluate(node._target, dict_index_to_value)
+        raise AstEvaluationException(
+            f"Unsupported AST node type: {type(node).__name__}"
+        )
+
+    cdef inline object _eval_leaf(self, object leaf, dict dict_index_to_value):
+        # AstConstant: prefer concrete mop value, otherwise fall back to expected_value
+        if isinstance(leaf, AstConstant):
+            if leaf.mop is not None and leaf.mop.t == ida_hexrays.mop_n:
+                return leaf.mop.nnn.value
+            return leaf.expected_value
+
+        if leaf.is_constant() and leaf.mop is not None:
+            return leaf.mop.nnn.value
+        assert leaf.ast_index is not None
+        return dict_index_to_value.get(leaf.ast_index)
+
+    cdef inline object _eval_node(self, object node, dict dict_index_to_value):
+        if node.ast_index in dict_index_to_value:
+            return dict_index_to_value[node.ast_index]
+        if node.dest_size is None:
+            raise ValueError("dest_size is None")
+
+        res_mask = AND_TABLE[node.dest_size]
+
+        if node.left is None:
+            raise ValueError(f"left is None for opcode: {node.opcode}")
+
+        if node.opcode in _BINARY_OPCODES and node.right is None:
+            raise ValueError("right is None for binary opcode: {0}".format(node.opcode))
+
+        if node.opcode == ida_hexrays.m_mov:
+            return (self.evaluate(node.left, dict_index_to_value)) & res_mask
+        elif node.opcode == ida_hexrays.m_neg:
+            return (-self.evaluate(node.left, dict_index_to_value)) & res_mask
+        elif node.opcode == ida_hexrays.m_lnot:
+            return self.evaluate(node.left, dict_index_to_value) != 0
+        elif node.opcode == ida_hexrays.m_bnot:
+            return (self.evaluate(node.left, dict_index_to_value) ^ res_mask) & res_mask
+        elif node.opcode == ida_hexrays.m_xds:
+            left_value_signed = unsigned_to_signed(
+                self.evaluate(node.left, dict_index_to_value), node.left.dest_size
+            )
+            return signed_to_unsigned(left_value_signed, node.dest_size) & res_mask
+        elif node.opcode == ida_hexrays.m_xdu:
+            return (self.evaluate(node.left, dict_index_to_value)) & res_mask
+        elif node.opcode == ida_hexrays.m_low:
+            return (self.evaluate(node.left, dict_index_to_value)) & res_mask
+        elif node.opcode == ida_hexrays.m_high:
+            if node.left.dest_size is None:
+                raise ValueError("left.dest_size is None for m_high")
+            shift_bits = node.dest_size * 8 if node.dest_size is not None else 0
+            return (
+                self.evaluate(node.left, dict_index_to_value) >> shift_bits
+            ) & res_mask
+        elif node.opcode == ida_hexrays.m_add and node.right is not None:
+            return (
+                self.evaluate(node.left, dict_index_to_value)
+                + self.evaluate(node.right, dict_index_to_value)
+            ) & res_mask
+        elif node.opcode == ida_hexrays.m_sub and node.right is not None:
+            return (
+                self.evaluate(node.left, dict_index_to_value)
+                - self.evaluate(node.right, dict_index_to_value)
+            ) & res_mask
+        elif node.opcode == ida_hexrays.m_mul and node.right is not None:
+            return (
+                self.evaluate(node.left, dict_index_to_value)
+                * self.evaluate(node.right, dict_index_to_value)
+            ) & res_mask
+        elif node.opcode == ida_hexrays.m_udiv and node.right is not None:
+            return (
+                self.evaluate(node.left, dict_index_to_value)
+                // self.evaluate(node.right, dict_index_to_value)
+            ) & res_mask
+        elif node.opcode == ida_hexrays.m_sdiv and node.right is not None:
+            return (
+                self.evaluate(node.left, dict_index_to_value)
+                // self.evaluate(node.right, dict_index_to_value)
+            ) & res_mask
+        elif node.opcode == ida_hexrays.m_umod and node.right is not None:
+            return (
+                self.evaluate(node.left, dict_index_to_value)
+                % self.evaluate(node.right, dict_index_to_value)
+            ) & res_mask
+        elif node.opcode == ida_hexrays.m_smod and node.right is not None:
+            return (
+                self.evaluate(node.left, dict_index_to_value)
+                % self.evaluate(node.right, dict_index_to_value)
+            ) & res_mask
+        elif node.opcode == ida_hexrays.m_or and node.right is not None:
+            return (
+                self.evaluate(node.left, dict_index_to_value)
+                | self.evaluate(node.right, dict_index_to_value)
+            ) & res_mask
+        elif node.opcode == ida_hexrays.m_and and node.right is not None:
+            return (
+                self.evaluate(node.left, dict_index_to_value)
+                & self.evaluate(node.right, dict_index_to_value)
+            ) & res_mask
+        elif node.opcode == ida_hexrays.m_xor and node.right is not None:
+            return (
+                self.evaluate(node.left, dict_index_to_value)
+                ^ self.evaluate(node.right, dict_index_to_value)
+            ) & res_mask
+        elif node.opcode == ida_hexrays.m_shl and node.right is not None:
+            return (
+                self.evaluate(node.left, dict_index_to_value)
+                << self.evaluate(node.right, dict_index_to_value)
+            ) & res_mask
+        elif node.opcode == ida_hexrays.m_shr and node.right is not None:
+            return (
+                self.evaluate(node.left, dict_index_to_value)
+                >> self.evaluate(node.right, dict_index_to_value)
+            ) & res_mask
+        elif node.opcode == ida_hexrays.m_sar and node.right is not None:
+            left_value_signed = unsigned_to_signed(
+                self.evaluate(node.left, dict_index_to_value), node.left.dest_size
+            )
+            res_signed = left_value_signed >> self.evaluate(
+                node.right, dict_index_to_value
+            )
+            return signed_to_unsigned(res_signed, node.dest_size) & res_mask
+        elif node.opcode == ida_hexrays.m_cfadd and node.right is not None:
+            tmp = get_add_cf(
+                self.evaluate(node.left, dict_index_to_value),
+                self.evaluate(node.right, dict_index_to_value),
+                node.left.dest_size,
+            )
+            return tmp & res_mask
+        elif node.opcode == ida_hexrays.m_ofadd and node.right is not None:
+            tmp = get_add_of(
+                self.evaluate(node.left, dict_index_to_value),
+                self.evaluate(node.right, dict_index_to_value),
+                node.left.dest_size,
+            )
+            return tmp & res_mask
+        elif node.opcode == ida_hexrays.m_sets:
+            left_value_signed = unsigned_to_signed(
+                self.evaluate(node.left, dict_index_to_value), node.left.dest_size
+            )
+            res = 1 if left_value_signed < 0 else 0
+            return res & res_mask
+        elif node.opcode == ida_hexrays.m_seto and node.right is not None:
+            left_value_signed = unsigned_to_signed(
+                self.evaluate(node.left, dict_index_to_value), node.left.dest_size
+            )
+            right_value_signed = unsigned_to_signed(
+                self.evaluate(node.right, dict_index_to_value), node.right.dest_size
+            )
+            sub_overflow = get_sub_of(
+                left_value_signed, right_value_signed, node.left.dest_size
+            )
+            return sub_overflow & res_mask
+        elif node.opcode == ida_hexrays.m_setnz and node.right is not None:
+            res = (
+                1
+                if self.evaluate(node.left, dict_index_to_value)
+                != self.evaluate(node.right, dict_index_to_value)
+                else 0
+            )
+            return res & res_mask
+        elif node.opcode == ida_hexrays.m_setz and node.right is not None:
+            res = (
+                1
+                if self.evaluate(node.left, dict_index_to_value)
+                == self.evaluate(node.right, dict_index_to_value)
+                else 0
+            )
+            return res & res_mask
+        elif node.opcode == ida_hexrays.m_setae and node.right is not None:
+            res = (
+                1
+                if self.evaluate(node.left, dict_index_to_value)
+                >= self.evaluate(node.right, dict_index_to_value)
+                else 0
+            )
+            return res & res_mask
+        elif node.opcode == ida_hexrays.m_setb and node.right is not None:
+            res = (
+                1
+                if self.evaluate(node.left, dict_index_to_value)
+                < self.evaluate(node.right, dict_index_to_value)
+                else 0
+            )
+            return res & res_mask
+        elif node.opcode == ida_hexrays.m_seta and node.right is not None:
+            res = (
+                1
+                if self.evaluate(node.left, dict_index_to_value)
+                > self.evaluate(node.right, dict_index_to_value)
+                else 0
+            )
+            return res & res_mask
+        elif node.opcode == ida_hexrays.m_setbe and node.right is not None:
+            res = (
+                1
+                if self.evaluate(node.left, dict_index_to_value)
+                <= self.evaluate(node.right, dict_index_to_value)
+                else 0
+            )
+            return res & res_mask
+        elif node.opcode == ida_hexrays.m_setg and node.right is not None:
+            left_value_signed = unsigned_to_signed(
+                self.evaluate(node.left, dict_index_to_value), node.left.dest_size
+            )
+            right_value_signed = unsigned_to_signed(
+                self.evaluate(node.right, dict_index_to_value), node.right.dest_size
+            )
+            res = 1 if left_value_signed > right_value_signed else 0
+            return res & res_mask
+        elif node.opcode == ida_hexrays.m_setge and node.right is not None:
+            left_value_signed = unsigned_to_signed(
+                self.evaluate(node.left, dict_index_to_value), node.left.dest_size
+            )
+            right_value_signed = unsigned_to_signed(
+                self.evaluate(node.right, dict_index_to_value), node.right.dest_size
+            )
+            res = 1 if left_value_signed >= right_value_signed else 0
+            return res & res_mask
+        elif node.opcode == ida_hexrays.m_setl and node.right is not None:
+            left_value_signed = unsigned_to_signed(
+                self.evaluate(node.left, dict_index_to_value), node.left.dest_size
+            )
+            right_value_signed = unsigned_to_signed(
+                self.evaluate(node.right, dict_index_to_value), node.right.dest_size
+            )
+            res = 1 if left_value_signed < right_value_signed else 0
+            return res & res_mask
+        elif node.opcode == ida_hexrays.m_setle and node.right is not None:
+            left_value_signed = unsigned_to_signed(
+                self.evaluate(node.left, dict_index_to_value), node.left.dest_size
+            )
+            right_value_signed = unsigned_to_signed(
+                self.evaluate(node.right, dict_index_to_value), node.right.dest_size
+            )
+            res = 1 if left_value_signed <= right_value_signed else 0
+            return res & res_mask
+        elif node.opcode == ida_hexrays.m_setp and node.right is not None:
+            res = get_parity_flag(
+                self.evaluate(node.left, dict_index_to_value),
+                self.evaluate(node.right, dict_index_to_value),
+                node.left.dest_size,
+            )
+            return res & res_mask
+        elif node.opcode == ida_hexrays.m_call:
+            if logger.debug_on:
+                logger.debug(
+                    "evaluate m_call: ast_index=%s, dest_size=%s, callee=%s, args=%s",
+                    node.ast_index,
+                    node.dest_size,
+                    node.left,
+                    node.right,
+                )
+            return 0 & res_mask
+        else:
+            raise AstEvaluationException(
+                "Can't evaluate opcode: {0}".format(node.opcode)
+            )
diff --git a/src/d810/expr/_rly_fast_ast.pyx b/src/d810/expr/_rly_fast_ast.pyx
new file mode 100644
index 0000000..4bdfb89
--- /dev/null
+++ b/src/d810/expr/_rly_fast_ast.pyx
@@ -0,0 +1,480 @@
+# distutils: language = c++
+# cython: language_level=3, embedsignature=True
+# distutils: define_macros=__EA64__=1
+
+from cython.operator cimport dereference as deref
+from libcpp.memory cimport unique_ptr
+from libcpp.utility cimport move
+from cpython.ref cimport PyObject
+from libcpp.unordered_map cimport unordered_map
+
+import ida_hexrays  
+
+from d810.conf.loggers import getLogger
+import d810.hexrays.hexrays_formatters as fmt
+from d810.cythxr._chexrays cimport (
+    ea_t, 
+    mcode_t, 
+    minsn_t,
+    mop_t_ptr,
+    mop_t,
+    mopt_t,
+    MOPT,
+    qstring, 
+    qvector, 
+    SwigPyObject,
+    SwigPyObject,
+    SwigPyObjectPtr,
+    swigtocpp,
+    uint64,
+)
+
+logger = getLogger(__name__)
+
+# C++ struct definition
+cdef extern from *:
+    """
+    namespace d810 {
+    struct TempAstNode {
+        mcode_t opcode;
+        TempAstNode* left;
+        TempAstNode* right;
+        TempAstNode* dst;
+        bool is_leaf;
+        bool is_const;
+        PyObject* py_mop;  // Use PyObject* for proper refcounting
+        
+        int ast_index;
+        int dest_size;
+        ea_t ea;
+
+        TempAstNode()
+          : opcode(m_nop),
+            left(nullptr), right(nullptr), dst(nullptr),
+            is_leaf(false), is_const(false),
+            py_mop(nullptr),
+            ast_index(-1), dest_size(0), ea(0) {}
+            
+        ~TempAstNode() {
+            Py_XDECREF(py_mop);
+        }
+        
+        void set_py_mop(PyObject* obj) {
+            Py_XINCREF(obj);
+            Py_XDECREF(py_mop);
+            py_mop = obj;
+        }
+    };
+    } // namespace d810
+    """
+    pass
+
+# C++ class declaration for Cython
+cdef extern from * namespace "d810":
+    cdef cppclass TempAstNode:
+        mcode_t opcode
+        TempAstNode* left
+        TempAstNode* right
+        TempAstNode* dst
+        bint is_leaf
+        bint is_const
+        PyObject* py_mop
+        int ast_index
+        int dest_size
+        ea_t ea
+        
+        TempAstNode() except +
+        void set_py_mop(PyObject* obj)
+
+
+ctypedef TempAstNode* TempAstNodePtr
+ctypedef unordered_map[qstring, TempAstNodePtr] qstr2node_map
+ctypedef qvector[TempAstNodePtr] node_vec
+
+cdef class AstBuildContext:
+    cdef:
+        qvector[unique_ptr[TempAstNode]] node_pool
+        qstr2node_map mop_key_to_node
+        node_vec unique_nodes
+        size_t pool_size
+        object mba_opcodes
+    
+    def __cinit__(self, object mba_opcodes, size_t pool_size=1024): # type: ignore
+        self.mba_opcodes = mba_opcodes
+        self.pool_size = pool_size
+        self.node_pool.reserve(pool_size) # type: ignore
+        self.reset()
+    
+    def __dealloc__(self):
+        # RAII: node_pool will be cleared by the destructor of unique_ptr
+        self.node_pool.clear()
+        self.mop_key_to_node.clear()
+        self.unique_nodes.clear()
+    
+    cdef TempAstNodePtr new_node(self):
+        cdef:
+            # 1. Create a new node on the heap, wrapped in a unique_ptr
+            unique_ptr[TempAstNode] node_ptr = unique_ptr[TempAstNode](new TempAstNode()) # type: ignore
+            # 2. Get the raw pointer to return and use for inter-node links
+            TempAstNodePtr raw_node_ptr = node_ptr.get()
+
+        self.node_pool.push_back_move(move(node_ptr)) # type: ignore
+
+        # The default constructor for TempAstNode already initialized it.
+        return raw_node_ptr
+
+    cdef void reset(self):
+        # Clearing the pool in __dealloc__ is enough.
+        # For a reset during lifetime, you'd clear and reserve again.
+        self.node_pool.clear()
+        self.node_pool.reserve(self.pool_size) # type: ignore
+        self.mop_key_to_node.clear()
+        self.unique_nodes.clear()
+        self.mba_opcodes = None
+
+# --------------------------
+# Key helpers
+# --------------------------
+
+cdef qstring _mop_key(mop_t* mop):
+    cdef:
+        qstring key
+        mopt_t t = mop.t
+
+    # t, size, valnum header
+    if t != MOPT.NUMBER:
+        key.sprnt("%d,%d,", t, mop.size)
+    else:
+        key.sprnt("%d,%d,%d,", t, mop.size, mop.valnum)
+
+    if t == MOPT.NUMBER:
+        key.cat_sprnt("%llx", mop.nnn.value)
+    elif t == MOPT.REGISTER:
+        key.cat_sprnt("%d", mop.r)
+    elif t == MOPT.DEST_RESULT and mop.d:
+        # instruction string as identity
+        key.cat_sprnt("%s", mop.d.dstr())
+    elif t == MOPT.STACK:
+        key.cat_sprnt("%llx", mop.s.off)
+    elif t == MOPT.GLOBAL:
+        key.cat_sprnt("%llx", <ea_t>mop.g)  # ea_t
+    elif t == MOPT.LOCAL:
+        key.cat_sprnt("%d,%lld", mop.l.idx, mop.l.off)
+    elif t == MOPT.MBLOCK:
+        key.cat_sprnt("%d", mop.b)
+    elif t == MOPT.HELPER:
+        key.cat_sprnt("%s", mop.helper)
+    elif t == MOPT.STRING:
+        key.cat_sprnt("%s", mop.cstr)
+    else:
+        key.cat_sprnt("%s", mop.dstr())
+    return key
+
+cdef qstring _ins_key(minsn_t* ins):
+    # A simple, stable cache key: text form of instruction
+    return qstring(ins.dstr())
+
+# --------------------------
+# Recursive builders
+# --------------------------
+
+cdef TempAstNodePtr _build_from_mop(AstBuildContext ctx, mop_t* mop):
+    cdef:
+        qstring key
+        qstr2node_map.iterator it
+        TempAstNodePtr node
+        bint is_mba
+        object new_mop_obj
+        mop_t_ptr new_mop_obj_swig
+
+    key = _mop_key(mop)
+    it = ctx.mop_key_to_node.find(key)
+
+    if it != ctx.mop_key_to_node.end():
+        if logger.debug_on:
+            # Avoid calling dstr() from Cython; just log type/size.
+            try:
+                logger.debug(
+                    "[fast_ast] Reusing cached node for mop t=%d size=%d",
+                    <int>mop.t,
+                    <int>mop.size,
+                )
+            except Exception:
+                pass
+        return deref(it).second
+
+    node = ctx.new_node()
+    node.ast_index = ctx.unique_nodes.size()
+    ctx.unique_nodes.push_back(node)
+
+    # NB: don't rely on mop.ea (not always meaningful); leave 0 here.
+    node.dest_size = mop.size
+
+    if mop.t == MOPT.DEST_RESULT and mop.d:
+        # Only build structured nodes for MBA-related opcodes, otherwise
+        # treat the whole expression as a leaf (matches Python builder).
+        is_mba = <bint>False
+        if ctx.mba_opcodes is not None:
+            try:
+                # Cast to int to avoid Python int(mcode_t) issues in Cython
+                is_mba = (<int>mop.d.opcode) in ctx.mba_opcodes
+            except Exception:
+                is_mba = <bint>False
+
+        if is_mba:
+            node.is_leaf = <bint>False
+            node.opcode  = mop.d.opcode
+            node.left  = _build_from_mop(ctx, &mop.d.l)
+            node.right = _build_from_mop(ctx, &mop.d.r)
+            node.dst   = _build_from_mop(ctx, &mop.d.d)
+            if logger.debug_on:
+                try:
+                    logger.debug(
+                        "[fast_ast] Building structured node opcode=%s size=%d",
+                        fmt.opcode_to_string(<int>node.opcode),
+                        <int>node.dest_size,
+                    )
+                except Exception:
+                    pass
+            new_mop_obj = ida_hexrays.mop_t()
+            new_mop_obj_swig = swigtocpp[mop_t_ptr](<PyObject *>(new_mop_obj.this))
+            new_mop_obj_swig.assign(deref(mop))
+            node.set_py_mop(<PyObject*>new_mop_obj)
+            # best-effort size if missing
+            if node.dest_size == 0:
+                if mop.d.d.size != 0:
+                    node.dest_size = mop.d.d.size
+                elif mop.d.l.size != 0:
+                    node.dest_size = mop.d.l.size
+            if logger.debug_on:
+                try:
+                    logger.debug(
+                        "[fast_ast] Structured node finalized opcode=%s dest_size=%d",
+                        fmt.opcode_to_string(<int>node.opcode),
+                        <int>node.dest_size,
+                    )
+                except Exception:
+                    pass
+        else:
+            node.is_leaf  = <bint>True
+            node.is_const = <bint>(mop.t == MOPT.NUMBER)
+            # node.pmop = make_shared[mop_t](deref(mop)) # type: ignore
+            # --- FIX: Safely store a Python-managed copy of the leaf mop ---
+            new_mop_obj = ida_hexrays.mop_t()
+            new_mop_obj_swig = swigtocpp[mop_t_ptr](<PyObject *>(new_mop_obj.this))
+            new_mop_obj_swig.assign(deref(mop))
+            node.set_py_mop(<PyObject*>new_mop_obj)       
+            if logger.debug_on:
+                try:
+                    logger.debug(
+                        "[fast_ast] Treating DEST_RESULT as leaf (non-MBA): %s",
+                        fmt.format_mop_t(new_mop_obj),
+                    )
+                except Exception:
+                    pass
+    else:
+        node.is_leaf  = <bint>True
+        node.is_const = <bint>(mop.t == MOPT.NUMBER)
+        #node.pmop = make_shared[mop_t](deref(mop)) # type: ignore
+        new_mop_obj = ida_hexrays.mop_t()
+        new_mop_obj_swig = swigtocpp[mop_t_ptr](<PyObject *>(new_mop_obj.this))
+        new_mop_obj_swig.assign(deref(mop))
+        node.set_py_mop(<PyObject*>new_mop_obj)
+        if logger.debug_on:
+            try:
+                logger.debug(
+                    "[fast_ast] Leaf node: %s",
+                    fmt.format_mop_t(new_mop_obj),
+                )
+            except Exception:
+                pass
+    ctx.mop_key_to_node[key] = node
+    return node
+
+cdef TempAstNodePtr _build_from_ins(AstBuildContext ctx, minsn_t* ins):
+    # Build directly from the instruction; no temporary mop_t.
+    cdef TempAstNodePtr root = ctx.new_node()
+    if root == NULL:
+        return NULL
+
+    root.is_leaf = <bint>False
+    root.opcode  = ins.opcode
+    # best-effort size if missing
+    root.dest_size = ins.d.size if ins.d.size != 0 else ins.l.size
+
+    # Recurse on operands
+    root.left  = _build_from_mop(ctx, &ins.l)
+    root.right = _build_from_mop(ctx, &ins.r)
+    root.dst   = _build_from_mop(ctx, &ins.d)
+
+    root.ea = ins.ea
+    if logger.debug_on:
+        try:
+            logger.debug(
+                "[fast_ast] Built instruction root opcode=%d ea=0x%x dest_size=%d",
+                <int>root.opcode,
+                <uint64>root.ea,
+                <int>root.dest_size,
+            )
+        except Exception:
+            pass
+    return root
+
+
+cdef object _to_py(TempAstNodePtr n,
+                   object AstNode, 
+                   object AstLeaf, 
+                   object AstConstant,
+                   object get_constant_mop):
+    cdef:
+        uint64 v
+        object py
+        str hex_v
+        int size
+        object mop_obj
+
+    if not n: 
+        return None
+
+    if not n.py_mop:
+        mop_obj = None
+    else:
+        mop_obj = <object>(n.py_mop)
+    if n.is_leaf:
+        if n.is_const:
+            # v = n.pmop.get().nnn.value
+            v = mop_obj.nnn.value
+            size = mop_obj.size
+            hex_v = hex(v)
+            py = AstConstant(hex_v, v, size)
+            # only constants get a Python mop, via provided callback
+            py.mop = get_constant_mop(v, size)
+        else:
+            # non-const leaf: just give it a readable label
+            py = AstLeaf(mop_obj.dstr())
+            py.mop = mop_obj
+            # py.mop = mop_t()
+            # py.mop.assign(deref(n.pmop.get()))
+    else:
+        py = AstNode(
+            n.opcode,
+            _to_py(n.left,  AstNode, AstLeaf, AstConstant, get_constant_mop),
+            _to_py(n.right, AstNode, AstLeaf, AstConstant, get_constant_mop),
+            _to_py(n.dst,   AstNode, AstLeaf, AstConstant, get_constant_mop),
+        )
+        py.mop = mop_obj        
+
+    py.dest_size = n.dest_size
+    py.ea        = n.ea
+    py.ast_index = n.ast_index
+    return py
+
+# --------------------------
+# Public entry
+# --------------------------
+
+cpdef object fast_minsn_to_ast(object ins_py,
+                               object MOP_TO_AST_CACHE,
+                               object AstProxy,
+                               object AstNode,
+                               object AstLeaf,
+                               object AstConstant,
+                               object get_constant_mop,
+                               object MBA_RELATED_OPCODES):
+    cdef:
+        SwigPyObject* swig_obj
+        minsn_t* ins
+        qstring qk
+        bytes key
+        object tmpl
+        AstBuildContext ctx
+        TempAstNodePtr root
+        object py_tmpl
+    
+
+    swig_obj = swigtocpp[SwigPyObjectPtr](<PyObject *>(ins_py.this))
+    
+    ins = <minsn_t*>swig_obj.ptr
+
+    if ins == NULL:
+        if logger.debug_on:
+            try:
+                logger.debug(
+                    "[fast_ast] Null instruction pointer; py=%s",
+                    fmt.format_minsn_t(<object>ins_py),
+                )
+            except Exception:
+                pass
+        return None
+
+    # very light filter: only real ops or instruction results
+    if ins.opcode == mcode_t.m_nop:
+        if logger.debug_on:
+            try:
+                logger.debug(
+                    "[fast_ast] Skipping m_nop: %s",
+                    fmt.format_minsn_t(<object>ins_py),
+                )
+            except Exception:
+                pass
+        return None
+
+    # cache
+    qk = _ins_key(ins)
+    key = qk.c_str()
+    if key in MOP_TO_AST_CACHE:
+        tmpl = MOP_TO_AST_CACHE[key]
+        if logger.debug_on:
+            try:
+                logger.debug(
+                    "[fast_ast] Cache hit: %s",
+                    fmt.format_minsn_t(<object>ins_py),
+                )
+            except Exception:
+                pass
+        return None if tmpl is None else AstProxy(tmpl)
+
+    ctx = AstBuildContext(MBA_RELATED_OPCODES)
+    root = _build_from_ins(ctx, ins)
+    if root == NULL:
+        MOP_TO_AST_CACHE[key] = None
+        if logger.debug_on:
+            try:
+                logger.debug(
+                    "[fast_ast] Failed to build AST root; caching miss: %s",
+                    fmt.format_minsn_t(<object>ins_py),
+                )
+            except Exception:
+                pass
+        return None
+
+    py_tmpl = _to_py(root, AstNode, AstLeaf, AstConstant, get_constant_mop)
+    if py_tmpl is None:
+        MOP_TO_AST_CACHE[key] = None
+        if logger.debug_on:
+            try:
+                logger.debug(
+                    "[fast_ast] _to_py returned None; caching miss: %s",
+                    fmt.format_minsn_t(<object>ins_py),
+                )
+            except Exception:
+                pass
+        return None
+
+    # let your Python side finalize the template
+    if hasattr(py_tmpl, "compute_sub_ast"):
+        py_tmpl.compute_sub_ast()
+    if hasattr(py_tmpl, "freeze"):
+        py_tmpl.freeze()
+
+    MOP_TO_AST_CACHE[key] = py_tmpl
+    if logger.debug_on:
+        try:
+            logger.debug(
+                "[fast_ast] Cached AST template: %s (opcode=%s)",
+                fmt.format_minsn_t(<object>ins_py),
+                fmt.opcode_to_string(getattr(<object>ins_py, "opcode", -1)),
+            )
+        except Exception:
+            pass
+    return AstProxy(py_tmpl)
diff --git a/src/d810/expr/_slow_ast.py b/src/d810/expr/_slow_ast.py
new file mode 100644
index 0000000..ef5f923
--- /dev/null
+++ b/src/d810/expr/_slow_ast.py
@@ -0,0 +1,1707 @@
+from __future__ import annotations
+
+import abc
+import dataclasses
+import typing
+
+import ida_hexrays
+import idaapi
+
+import d810._compat as _compat
+from d810.conf.loggers import getLogger
+from d810.errors import AstEvaluationException
+from d810.expr.utils import (
+    MOP_CONSTANT_CACHE,
+    MOP_TO_AST_CACHE,
+    get_add_cf,
+    get_add_of,
+    get_parity_flag,
+    get_sub_of,
+    signed_to_unsigned,
+    unsigned_to_signed,
+)
+from d810.hexrays.hexrays_formatters import (
+    format_minsn_t,
+    format_mop_t,
+    mop_tree,
+    mop_type_to_string,
+    opcode_to_string,
+    sanitize_ea,
+)
+from d810.hexrays.hexrays_helpers import (
+    AND_TABLE,
+    MBA_RELATED_OPCODES,
+    MINSN_TO_AST_FORBIDDEN_OPCODES,
+    OPCODES_INFO,
+    Z3_SPECIAL_OPERANDS,
+    equal_mops_ignore_size,
+    is_rotate_helper_call,
+)
+from d810.registry import NOT_GIVEN, NotGiven
+
+logger = getLogger(__name__)
+
+
+def get_constant_mop(value: int, size: int) -> ida_hexrays.mop_t:
+    """
+    Returns a cached or new mop_t for a constant value.
+    This avoids repeated calls to mop_t.__init__ and make_number.
+    """
+    key = (value, size)
+    if key in MOP_CONSTANT_CACHE:
+        return MOP_CONSTANT_CACHE[key]
+
+    # Not in cache, create it once and store it.
+    cst_mop = ida_hexrays.mop_t()
+    cst_mop.make_number(value, size)
+    MOP_CONSTANT_CACHE[key] = cst_mop
+    return cst_mop
+
+
+@dataclasses.dataclass(slots=True)
+class AstInfo:
+    ast: AstNode | AstLeaf
+    number_of_use: int
+
+    def __str__(self):
+        return f"{self.ast} used {self.number_of_use} times: {format_mop_t(self.ast.mop) if self.ast.mop else 0}"
+
+
+class AstBase:
+
+    sub_ast_info_by_index: dict[int, AstInfo] = {}
+    mop: ida_hexrays.mop_t | None = None
+    dest_size: int | None = None
+    ea: int | None = None
+    ast_index: int | None = None
+
+    @property
+    @abc.abstractmethod
+    def is_frozen(self) -> bool: ...
+
+    @abc.abstractmethod
+    def clone(self) -> AstBase: ...
+
+    @abc.abstractmethod
+    def freeze(self) -> None: ...
+
+    @abc.abstractmethod
+    def is_node(self) -> bool: ...
+
+    @abc.abstractmethod
+    def is_leaf(self) -> bool: ...
+
+    @abc.abstractmethod
+    def is_constant(self) -> bool: ...
+
+    @abc.abstractmethod
+    def compute_sub_ast(self) -> None: ...
+
+    @abc.abstractmethod
+    def get_leaf_list(self) -> list[AstLeaf]: ...
+
+    @abc.abstractmethod
+    def reset_mops(self) -> None: ...
+
+    @abc.abstractmethod
+    def _copy_mops_from_ast(self, other: AstBase, read_only: bool = False) -> bool: ...
+
+    @abc.abstractmethod
+    def create_mop(self, ea: int) -> ida_hexrays.mop_t: ...
+
+    @abc.abstractmethod
+    def get_pattern(self) -> str: ...
+
+    @abc.abstractmethod
+    def evaluate(self, dict_index_to_value: dict[int, int]) -> int: ...
+
+    @abc.abstractmethod
+    def get_depth_signature(self, depth: int) -> list[str]: ...
+
+    def __bool__(self) -> bool:
+        return True
+
+
+class AstNode(AstBase):
+    def __init__(
+        self,
+        opcode: int | None = None,
+        left: AstBase | None = None,
+        right: AstBase | None = None,
+        dst: AstBase | None = None,
+    ):
+        super().__init__()
+        self.opcode = opcode
+        self.left = left
+        self.right = right
+        self.dst = dst
+        self.dst_mop = None
+
+        self.opcodes = []
+        self.is_candidate_ok = False
+
+        self.leafs = []
+        self.leafs_by_name = {}
+
+        self.ast_index = 0
+        self.sub_ast_info_by_index = {}
+
+        self.func_name: str = ""
+        self._is_frozen = False  # All newly created nodes are mutable by default
+
+    @property
+    @_compat.override
+    def is_frozen(self) -> bool:
+        return self._is_frozen
+
+    @_compat.override
+    def freeze(self):
+        """Recursively freezes this node and all its children."""
+        if self._is_frozen:
+            return
+        self._is_frozen = True
+        if hasattr(self, "left") and self.left:
+            self.left.freeze()
+        if hasattr(self, "right") and self.right:
+            self.right.freeze()
+        if hasattr(self, "dst") and self.dst:
+            self.dst.freeze()
+
+    @property
+    def size(self):
+        return self.mop.d.d.size if self.mop else 0
+
+    def compute_sub_ast(self):
+        self.sub_ast_info_by_index = {}
+        assert self.ast_index is not None
+        self.sub_ast_info_by_index[self.ast_index] = AstInfo(self, 1)
+
+        if self.left is not None:
+            self.left.compute_sub_ast()
+            for ast_index, ast_info in self.left.sub_ast_info_by_index.items():
+                if ast_index not in self.sub_ast_info_by_index.keys():
+                    self.sub_ast_info_by_index[ast_index] = AstInfo(ast_info.ast, 0)
+                self.sub_ast_info_by_index[
+                    ast_index
+                ].number_of_use += ast_info.number_of_use
+
+        if self.right is not None:
+            self.right.compute_sub_ast()
+            for ast_index, ast_info in self.right.sub_ast_info_by_index.items():
+                if ast_index not in self.sub_ast_info_by_index.keys():
+                    self.sub_ast_info_by_index[ast_index] = AstInfo(ast_info.ast, 0)
+                self.sub_ast_info_by_index[
+                    ast_index
+                ].number_of_use += ast_info.number_of_use
+
+    def get_information(self):
+        leaf_info_list = []
+        cst_list = []
+        opcode_list = []
+        self.compute_sub_ast()
+
+        for _, ast_info in self.sub_ast_info_by_index.items():
+            if (ast_info.ast.mop is not None) and (
+                ast_info.ast.mop.t != ida_hexrays.mop_z
+            ):
+                if ast_info.ast.is_leaf():
+                    if ast_info.ast.is_constant():
+                        cst_list.append(ast_info.ast.mop.nnn.value)
+                    else:
+                        leaf_info_list.append(ast_info)
+                else:
+                    ast_node = typing.cast(AstNode, ast_info.ast)
+                    opcode_list += [ast_node.opcode] * ast_info.number_of_use
+
+        return leaf_info_list, cst_list, opcode_list
+
+    def __getitem__(self, k: str) -> AstLeaf:
+        return self.leafs_by_name[k]
+
+    def get_leaf_list(self) -> list[AstLeaf]:
+        leafs = []
+        if self.left is not None:
+            leafs += self.left.get_leaf_list()
+        if self.right is not None:
+            leafs += self.right.get_leaf_list()
+        return leafs
+
+    def add_leaf(self, leaf_name: str, leaf_mop: ida_hexrays.mop_t):
+        leaf = AstLeaf(leaf_name)
+        leaf.mop = leaf_mop
+        self.leafs.append(leaf)
+        self.leafs_by_name[leaf_name] = leaf
+
+    def add_constant_leaf(self, leaf_name: str, cst_value: int, cst_size: int):
+        masked_value = cst_value & AND_TABLE[cst_size]
+        cst_mop = get_constant_mop(masked_value, cst_size)
+        self.add_leaf(leaf_name, cst_mop)
+
+    def check_pattern_and_copy_mops(
+        self, ast: AstNode | AstLeaf, read_only: bool = False
+    ) -> bool:
+        if not read_only:
+            self.reset_mops()
+        if logger.debug_on:
+            logger.debug(
+                "AstNode.check_pattern_and_copy_mops from %r",
+                ast,
+            )
+        is_matching_shape = self._copy_mops_from_ast(ast, read_only)
+        if not is_matching_shape:
+            return False
+        return self._check_implicit_equalities()
+
+    def reset_mops(self):
+        self.mop = None
+        if self.left is not None:
+            self.left.reset_mops()
+        if self.right is not None:
+            self.right.reset_mops()
+
+    def _copy_mops_from_ast(
+        self, other: AstNode | AstLeaf, read_only: bool = False
+    ) -> bool:
+        if not other.is_node():
+            return False
+        other = typing.cast(AstNode, other)
+        if self.opcode != other.opcode:
+            return False
+
+        if not read_only:
+            self.mop = other.mop
+            self.dst_mop = other.dst_mop
+            self.dest_size = other.dest_size
+            self.ea = other.ea
+
+        if logger.debug_on:
+            logger.debug(
+                "AstNode._copy_mops_from_ast: self.left: %r, other.left: %r",
+                self.left,
+                other.left,
+            )
+        if self.left is not None and other.left is not None:
+            if not self.left._copy_mops_from_ast(other.left, read_only):
+                return False
+        if logger.debug_on:
+            logger.debug(
+                "AstNode._copy_mops_from_ast: self.right: %r, other.right: %r",
+                self.right,
+                other.right,
+            )
+        if self.right is not None and other.right is not None:
+            if not self.right._copy_mops_from_ast(other.right, read_only):
+                return False
+        return True
+
+    def _check_implicit_equalities(self) -> bool:
+        self.leafs = self.get_leaf_list()
+        self.leafs_by_name = {}
+        self.is_candidate_ok = True
+
+        for leaf in self.leafs:
+            ref_leaf = self.leafs_by_name.get(leaf.name)
+            if ref_leaf is not None and leaf.mop is not None:
+                if not equal_mops_ignore_size(ref_leaf.mop, leaf.mop):
+                    self.is_candidate_ok = False
+            self.leafs_by_name[leaf.name] = leaf
+        return self.is_candidate_ok
+
+    def update_leafs_mop(
+        self,
+        other: AstNode,
+        other2: AstNode | None = None,
+    ) -> bool:
+        self.leafs = self.get_leaf_list()
+        all_leafs_found = True
+        for leaf in self.leafs:
+            if other is not None and leaf.name in other.leafs_by_name:
+                leaf.mop = other.leafs_by_name[leaf.name].mop
+            elif other2 is not None and leaf.name in other2.leafs_by_name:
+                leaf.mop = other2.leafs_by_name[leaf.name].mop
+            else:
+                all_leafs_found = False
+        return all_leafs_found
+
+    def create_mop(self, ea: int) -> ida_hexrays.mop_t:
+        new_ins = self.create_minsn(ea)
+        new_ins_mop = ida_hexrays.mop_t()
+        new_ins_mop.create_from_insn(new_ins)
+        return new_ins_mop
+
+    def create_minsn(self, ea: int, dest=None) -> ida_hexrays.minsn_t:
+        new_ins = ida_hexrays.minsn_t(ea)
+        new_ins.opcode = self.opcode
+
+        if self.left is not None:
+            new_ins.l = self.left.create_mop(ea)
+            if self.right is not None:
+                new_ins.r = self.right.create_mop(ea)
+
+        new_ins.d = ida_hexrays.mop_t()
+
+        if self.left is not None:
+            new_ins.d.size = new_ins.l.size
+        if dest is not None:
+            new_ins.d = dest
+        return new_ins
+
+    def get_pattern(self) -> str:
+        nb_operands = OPCODES_INFO[self.opcode]["nb_operands"]
+        if nb_operands == 0:
+            return "AstNode({0})".format(OPCODES_INFO[self.opcode]["name"])
+        elif nb_operands == 1 and self.left is not None:
+            return "AstNode(m_{0}, {1})".format(
+                OPCODES_INFO[self.opcode]["name"], self.left.get_pattern()
+            )
+        elif nb_operands == 2 and self.left is not None and self.right is not None:
+            return "AstNode(m_{0}, {1}, {2})".format(
+                OPCODES_INFO[self.opcode]["name"],
+                self.left.get_pattern(),
+                self.right.get_pattern(),
+            )
+        else:
+            raise ValueError(f"Invalid number of operands: {nb_operands}")
+
+    def evaluate_with_leaf_info(
+        self, leafs_info: list[AstInfo], leafs_value: list[int]
+    ) -> int:
+        dict_index_to_value: dict[int, int] = {}
+        for leaf_info, leaf_value in zip(leafs_info, leafs_value):
+            if leaf_info.ast.ast_index is not None:
+                dict_index_to_value[leaf_info.ast.ast_index] = leaf_value
+        res = self.evaluate(dict_index_to_value)
+        return res
+
+    def evaluate(self, dict_index_to_value: dict[int, int]) -> int:
+        if self.ast_index in dict_index_to_value:
+            return dict_index_to_value[self.ast_index]
+        if self.dest_size is None:
+            raise ValueError("dest_size is None")
+
+        res_mask = AND_TABLE[self.dest_size]
+
+        if self.left is None:
+            raise ValueError(f"left is None for opcode: {self.opcode}")
+
+        binary_opcodes = {
+            ida_hexrays.m_add,
+            ida_hexrays.m_sub,
+            ida_hexrays.m_mul,
+            ida_hexrays.m_udiv,
+            ida_hexrays.m_sdiv,
+            ida_hexrays.m_umod,
+            ida_hexrays.m_smod,
+            ida_hexrays.m_or,
+            ida_hexrays.m_and,
+            ida_hexrays.m_xor,
+            ida_hexrays.m_shl,
+            ida_hexrays.m_shr,
+            ida_hexrays.m_sar,
+            ida_hexrays.m_cfadd,
+            ida_hexrays.m_ofadd,
+            ida_hexrays.m_seto,
+            ida_hexrays.m_setnz,
+            ida_hexrays.m_setz,
+            ida_hexrays.m_setae,
+            ida_hexrays.m_setb,
+            ida_hexrays.m_seta,
+            ida_hexrays.m_setbe,
+            ida_hexrays.m_setg,
+            ida_hexrays.m_setge,
+            ida_hexrays.m_setl,
+            ida_hexrays.m_setle,
+            ida_hexrays.m_setp,
+        }
+
+        if self.opcode in binary_opcodes and self.right is None:
+            raise ValueError("right is None for binary opcode: {0}".format(self.opcode))
+
+        match self.opcode:
+            case ida_hexrays.m_mov:
+                return (self.left.evaluate(dict_index_to_value)) & res_mask
+            case ida_hexrays.m_neg:
+                return (-self.left.evaluate(dict_index_to_value)) & res_mask
+            case ida_hexrays.m_lnot:
+                return self.left.evaluate(dict_index_to_value) != 0
+            case ida_hexrays.m_bnot:
+                return (self.left.evaluate(dict_index_to_value) ^ res_mask) & res_mask
+            case ida_hexrays.m_xds:
+                left_value_signed = unsigned_to_signed(
+                    self.left.evaluate(dict_index_to_value), self.left.dest_size
+                )
+                return signed_to_unsigned(left_value_signed, self.dest_size) & res_mask
+            case ida_hexrays.m_xdu:
+                return (self.left.evaluate(dict_index_to_value)) & res_mask
+            case ida_hexrays.m_low:
+                return (self.left.evaluate(dict_index_to_value)) & res_mask
+            case ida_hexrays.m_high:
+                # Extract the upper half of the operand. We shift right by the
+                # size (in bits) of the current destination. For example, when
+                # evaluating a 32-bit "high" of a 64-bit operand we shift by
+                # 32 bits, then mask the result to the destination size.
+                if self.left.dest_size is None:
+                    raise ValueError("left.dest_size is None for m_high")
+                shift_bits = self.dest_size * 8 if self.dest_size is not None else 0
+                return (
+                    self.left.evaluate(dict_index_to_value) >> shift_bits
+                ) & res_mask
+            case ida_hexrays.m_add if self.right is not None:
+                return (
+                    self.left.evaluate(dict_index_to_value)
+                    + self.right.evaluate(dict_index_to_value)
+                ) & res_mask
+            case ida_hexrays.m_sub if self.right is not None:
+                return (
+                    self.left.evaluate(dict_index_to_value)
+                    - self.right.evaluate(dict_index_to_value)
+                ) & res_mask
+            case ida_hexrays.m_mul if self.right is not None:
+                return (
+                    self.left.evaluate(dict_index_to_value)
+                    * self.right.evaluate(dict_index_to_value)
+                ) & res_mask
+            case ida_hexrays.m_udiv if self.right is not None:
+                return (
+                    self.left.evaluate(dict_index_to_value)
+                    // self.right.evaluate(dict_index_to_value)
+                ) & res_mask
+            case ida_hexrays.m_sdiv if self.right is not None:
+                return (
+                    self.left.evaluate(dict_index_to_value)
+                    // self.right.evaluate(dict_index_to_value)
+                ) & res_mask
+            case ida_hexrays.m_umod if self.right is not None:
+                return (
+                    self.left.evaluate(dict_index_to_value)
+                    % self.right.evaluate(dict_index_to_value)
+                ) & res_mask
+            case ida_hexrays.m_smod if self.right is not None:
+                return (
+                    self.left.evaluate(dict_index_to_value)
+                    % self.right.evaluate(dict_index_to_value)
+                ) & res_mask
+            case ida_hexrays.m_or if self.right is not None:
+                return (
+                    self.left.evaluate(dict_index_to_value)
+                    | self.right.evaluate(dict_index_to_value)
+                ) & res_mask
+            case ida_hexrays.m_and if self.right is not None:
+                return (
+                    self.left.evaluate(dict_index_to_value)
+                    & self.right.evaluate(dict_index_to_value)
+                ) & res_mask
+            case ida_hexrays.m_xor if self.right is not None:
+                return (
+                    self.left.evaluate(dict_index_to_value)
+                    ^ self.right.evaluate(dict_index_to_value)
+                ) & res_mask
+            case ida_hexrays.m_shl if self.right is not None:
+                return (
+                    self.left.evaluate(dict_index_to_value)
+                    << self.right.evaluate(dict_index_to_value)
+                ) & res_mask
+            case ida_hexrays.m_shr if self.right is not None:
+                return (
+                    self.left.evaluate(dict_index_to_value)
+                    >> self.right.evaluate(dict_index_to_value)
+                ) & res_mask
+            case ida_hexrays.m_sar if self.right is not None:
+                left_value_signed = unsigned_to_signed(
+                    self.left.evaluate(dict_index_to_value), self.left.dest_size
+                )
+                res_signed = left_value_signed >> self.right.evaluate(
+                    dict_index_to_value
+                )
+                return signed_to_unsigned(res_signed, self.dest_size) & res_mask
+            case ida_hexrays.m_cfadd if self.right is not None:
+                tmp = get_add_cf(
+                    self.left.evaluate(dict_index_to_value),
+                    self.right.evaluate(dict_index_to_value),
+                    self.left.dest_size,
+                )
+                return tmp & res_mask
+            case ida_hexrays.m_ofadd if self.right is not None:
+                tmp = get_add_of(
+                    self.left.evaluate(dict_index_to_value),
+                    self.right.evaluate(dict_index_to_value),
+                    self.left.dest_size,
+                )
+                return tmp & res_mask
+            case ida_hexrays.m_sets:
+                left_value_signed = unsigned_to_signed(
+                    self.left.evaluate(dict_index_to_value), self.left.dest_size
+                )
+                res = 1 if left_value_signed < 0 else 0
+                return res & res_mask
+            case ida_hexrays.m_seto if self.right is not None:
+                left_value_signed = unsigned_to_signed(
+                    self.left.evaluate(dict_index_to_value), self.left.dest_size
+                )
+                right_value_signed = unsigned_to_signed(
+                    self.right.evaluate(dict_index_to_value), self.right.dest_size
+                )
+                sub_overflow = get_sub_of(
+                    left_value_signed, right_value_signed, self.left.dest_size
+                )
+                return sub_overflow & res_mask
+            case ida_hexrays.m_setnz if self.right is not None:
+                res = (
+                    1
+                    if self.left.evaluate(dict_index_to_value)
+                    != self.right.evaluate(dict_index_to_value)
+                    else 0
+                )
+                return res & res_mask
+            case ida_hexrays.m_setz if self.right is not None:
+                res = (
+                    1
+                    if self.left.evaluate(dict_index_to_value)
+                    == self.right.evaluate(dict_index_to_value)
+                    else 0
+                )
+                return res & res_mask
+            case ida_hexrays.m_setae if self.right is not None:
+                res = (
+                    1
+                    if self.left.evaluate(dict_index_to_value)
+                    >= self.right.evaluate(dict_index_to_value)
+                    else 0
+                )
+                return res & res_mask
+            case ida_hexrays.m_setb if self.right is not None:
+                res = (
+                    1
+                    if self.left.evaluate(dict_index_to_value)
+                    < self.right.evaluate(dict_index_to_value)
+                    else 0
+                )
+                return res & res_mask
+            case ida_hexrays.m_seta if self.right is not None:
+                res = (
+                    1
+                    if self.left.evaluate(dict_index_to_value)
+                    > self.right.evaluate(dict_index_to_value)
+                    else 0
+                )
+                return res & res_mask
+            case ida_hexrays.m_setbe if self.right is not None:
+                res = (
+                    1
+                    if self.left.evaluate(dict_index_to_value)
+                    <= self.right.evaluate(dict_index_to_value)
+                    else 0
+                )
+                return res & res_mask
+            case ida_hexrays.m_setg if self.right is not None:
+                left_value_signed = unsigned_to_signed(
+                    self.left.evaluate(dict_index_to_value), self.left.dest_size
+                )
+                right_value_signed = unsigned_to_signed(
+                    self.right.evaluate(dict_index_to_value), self.right.dest_size
+                )
+                res = 1 if left_value_signed > right_value_signed else 0
+                return res & res_mask
+            case ida_hexrays.m_setge if self.right is not None:
+                left_value_signed = unsigned_to_signed(
+                    self.left.evaluate(dict_index_to_value), self.left.dest_size
+                )
+                right_value_signed = unsigned_to_signed(
+                    self.right.evaluate(dict_index_to_value), self.right.dest_size
+                )
+                res = 1 if left_value_signed >= right_value_signed else 0
+                return res & res_mask
+            case ida_hexrays.m_setl if self.right is not None:
+                left_value_signed = unsigned_to_signed(
+                    self.left.evaluate(dict_index_to_value), self.left.dest_size
+                )
+                right_value_signed = unsigned_to_signed(
+                    self.right.evaluate(dict_index_to_value), self.right.dest_size
+                )
+                res = 1 if left_value_signed < right_value_signed else 0
+                return res & res_mask
+            case ida_hexrays.m_setle if self.right is not None:
+                left_value_signed = unsigned_to_signed(
+                    self.left.evaluate(dict_index_to_value), self.left.dest_size
+                )
+                right_value_signed = unsigned_to_signed(
+                    self.right.evaluate(dict_index_to_value), self.right.dest_size
+                )
+                res = 1 if left_value_signed <= right_value_signed else 0
+                return res & res_mask
+            case ida_hexrays.m_setp if self.right is not None:
+                res = get_parity_flag(
+                    self.left.evaluate(dict_index_to_value),
+                    self.right.evaluate(dict_index_to_value),
+                    self.left.dest_size,
+                )
+                return res & res_mask
+            case ida_hexrays.m_call:
+                if logger.debug_on:
+                    logger.debug(
+                        "evaluate m_call: ast_index=%s, dest_size=%s, callee=%s, args=%s",
+                        self.ast_index,
+                        self.dest_size,
+                        self.left,
+                        self.right,
+                    )
+                # Unknown runtime value – treat as 0 to let constant evaluation proceed.
+                return 0 & res_mask
+            case _:
+                raise AstEvaluationException(
+                    "Can't evaluate opcode: {0}".format(self.opcode)
+                )
+
+    def get_depth_signature(self, depth):
+        if depth == 1:
+            return ["{0}".format(self.opcode)]
+        tmp = []
+        nb_operands = OPCODES_INFO[self.opcode]["nb_operands"]
+        if (nb_operands >= 1) and self.left is not None:
+            tmp += self.left.get_depth_signature(depth - 1)
+        else:
+            tmp += ["N"] * (2 ** (depth - 2))
+        if (nb_operands >= 2) and self.right is not None:
+            tmp += self.right.get_depth_signature(depth - 1)
+        else:
+            tmp += ["N"] * (2 ** (depth - 2))
+        return tmp
+
+    def __str__(self):
+        try:
+            nb_operands = OPCODES_INFO[self.opcode]["nb_operands"]
+            if "symbol" in OPCODES_INFO[self.opcode].keys():
+                if nb_operands == 0:
+                    return "{0}()".format(OPCODES_INFO[self.opcode]["symbol"])
+                elif nb_operands == 1:
+                    return "{0}({1})".format(
+                        OPCODES_INFO[self.opcode]["symbol"], self.left
+                    )
+                elif nb_operands == 2:
+                    if OPCODES_INFO[self.opcode]["symbol"] not in Z3_SPECIAL_OPERANDS:
+                        return "({1} {0} {2})".format(
+                            OPCODES_INFO[self.opcode]["symbol"], self.left, self.right
+                        )
+                    else:
+                        return "{0}({1}, {2})".format(
+                            OPCODES_INFO[self.opcode]["symbol"], self.left, self.right
+                        )
+            else:
+                if nb_operands == 0:
+                    return "{0}()".format(OPCODES_INFO[self.opcode]["name"])
+                elif nb_operands == 1:
+                    return "{0}({1})".format(
+                        OPCODES_INFO[self.opcode]["name"], self.left
+                    )
+                elif nb_operands == 2:
+                    return "{0}({1}, {2})".format(
+                        OPCODES_INFO[self.opcode]["name"], self.left, self.right
+                    )
+        except RuntimeError as e:
+            logger.info("Error while calling __str__ on AstNode: {0}".format(e))
+        return "Error_AstNode"
+
+    def __repr__(self):
+        op_str = opcode_to_string(self.opcode) if self.opcode is not None else "None"
+        return f"AstNode({op_str}, left={self.left}, right={self.right})"
+
+    @_compat.override
+    def clone(self):
+        # Use __new__ to bypass __init__ for speed
+        new_node = self.__class__.__new__(self.__class__)
+        super(AstNode, new_node).__init__()  # Initialize the dict part
+
+        # Manually copy attributes and clone children
+        new_node.opcode = self.opcode
+        new_node.left = self.left.clone() if self.left else None
+        new_node.right = self.right.clone() if self.right else None
+        new_node.dst = self.dst.clone() if self.dst else None
+
+        new_node.mop = self.mop
+        new_node.dst_mop = self.dst_mop
+        new_node.dest_size = self.dest_size
+        new_node.ea = self.ea
+        new_node.ast_index = self.ast_index
+
+        # Initialize transient state
+        new_node.is_candidate_ok = False
+        new_node.leafs = []
+        new_node.leafs_by_name = {}
+        new_node.opcodes = []
+        new_node.sub_ast_info_by_index = {}  # Start fresh
+
+        # Cloned objects start mutable
+        new_node._is_frozen = False
+
+        return new_node
+
+    @_compat.override
+    def is_node(self):
+        return True
+
+    @_compat.override
+    def is_leaf(self):
+        # An AstNode is not a leaf, so returns False
+        return False
+
+    @_compat.override
+    def is_constant(self):
+        return False
+
+
+class AstLeaf(AstBase):
+    def __init__(self, name):
+        self.name = name
+        self.ast_index: int | None = None
+
+        self.mop = None
+        self.z3_var = None
+        self.z3_var_name: str | NotGiven = NOT_GIVEN
+
+        self.dest_size = None
+        self.ea = None
+        self._is_frozen = False  # All newly created nodes are mutable by default
+        self.sub_ast_info_by_index = {}
+
+    @property
+    @_compat.override
+    def is_frozen(self) -> bool:
+        return self._is_frozen
+
+    @_compat.override
+    def freeze(self):
+        """Recursively freezes this node and all its children."""
+        if self._is_frozen:
+            return
+        self._is_frozen = True
+
+    @_compat.override
+    def is_node(self):
+        return False
+
+    @_compat.override
+    def is_leaf(self):
+        return True
+
+    @_compat.override
+    def is_constant(self):
+        if self.mop is None:
+            return False
+        return self.mop.t == ida_hexrays.mop_n
+
+    @_compat.override
+    def clone(self):
+        # Use __new__ to bypass __init__ for speed
+        new_leaf = self.__class__.__new__(self.__class__)
+
+        # Manually copy attributes. This is faster than generic deepcopy.
+        new_leaf.name = self.name
+        new_leaf.ast_index = self.ast_index
+        new_leaf.mop = self.mop
+        new_leaf.dest_size = self.dest_size
+        new_leaf.ea = self.ea
+
+        # Initialize transient state
+        new_leaf.z3_var = None
+        new_leaf.z3_var_name = NOT_GIVEN
+        new_leaf.sub_ast_info_by_index = {}  # Start fresh
+
+        # Cloned objects start mutable by definition
+        new_leaf._is_frozen = False
+
+        return new_leaf
+
+    def __getitem__(self, name: str) -> AstLeaf:
+        if name == self.name:
+            return self
+        raise KeyError
+
+    @property
+    def size(self):
+        return self.mop.size if self.mop else 0
+
+    @property
+    def dst_mop(self):
+        return self.mop
+
+    @dst_mop.setter
+    def dst_mop(self, mop):
+        self.mop = mop
+
+    @property
+    def value(self):
+        if self.is_constant() and self.mop is not None:
+            return self.mop.nnn.value
+        else:
+            return None
+
+    def compute_sub_ast(self):
+        self.sub_ast_info_by_index = {}
+        assert self.ast_index is not None
+        self.sub_ast_info_by_index[self.ast_index] = AstInfo(self, 1)
+
+    def get_information(self):
+        # Just here to allow calling get_information on either a AstNode or AstLeaf
+        return [], [], []
+
+    def get_leaf_list(self):
+        return [self]
+
+    def create_mop(self, ea):
+        # 1. Constant operands can keep using the shared cache
+        if self.is_constant() and self.value is not None:
+            # TODO: is this right?
+            size = self.dest_size if self.dest_size is not None else self.size
+            if logger.debug_on:
+                logger.debug(
+                    "AstLeaf.create_mop: Constant operand @ 0x%x: %s, size: %s, dest_size: %s, equal? %s",
+                    ea,
+                    self.value,
+                    size,
+                    self.dest_size,
+                    size == self.dest_size,
+                )
+            val = get_constant_mop(self.value, size)
+            if logger.debug_on:
+                logger.debug(
+                    "AstLeaf.create_mop: Constant operand reused: %s",
+                    val,
+                    extra={"ea": hex(ea)},
+                )
+            return val
+
+        if self.mop is None:
+            logger.error(
+                "%r mop is None in create_mop for 0x%x",
+                self,
+                ea,
+            )
+            raise AstEvaluationException(
+                f"{repr(self)}'s mop is None in create_mop for {hex(ea)}"
+            )
+
+        # 2. Otherwise, we need to create a new mop
+        new_mop = ida_hexrays.mop_t()
+        new_mop.assign(self.mop)
+        return new_mop  # duplicates the C++ object
+
+    def update_leafs_mop(self, other: AstNode, other2: AstNode | None = None):
+        if other is not None and self.name in other.leafs_by_name:
+            self.mop = other.leafs_by_name[self.name].mop
+            return True
+        elif other2 is not None and self.name in other2.leafs_by_name:
+            self.mop = other2.leafs_by_name[self.name].mop
+            return True
+        return False
+
+    def check_pattern_and_copy_mops(self, ast, read_only: bool = False):
+        if not read_only:
+            self.reset_mops()
+        is_matching_shape = self._copy_mops_from_ast(ast, read_only)
+
+        if not is_matching_shape:
+            return False
+        return self._check_implicit_equalities()
+
+    def reset_mops(self):
+        self.z3_var = None
+        self.z3_var_name = NOT_GIVEN
+        self.mop = None
+
+    def _copy_mops_from_ast(self, other, read_only: bool = False):
+        if other.mop is None:
+            if logger.debug_on:
+                logger.debug(
+                    "AstLeaf._copy_mops_from_ast: other %r's mop is None",
+                    other,
+                )
+            return False
+        if logger.debug_on:
+            logger.debug(
+                "AstLeaf._copy_mops_from_ast: other %r's mop %s is not None",
+                other,
+                format_mop_t(other.mop),
+            )
+        if not read_only:
+            self.mop = other.mop
+        return True
+
+    @staticmethod
+    def _check_implicit_equalities():
+        # An AstLeaf does not have any implicit equalities to be checked, so we always returns True
+        return True
+
+    def get_pattern(self):
+        if self.is_constant() and self.mop is not None:
+            return "AstConstant('{0}', {0})".format(self.mop.nnn.value)
+        if self.ast_index is not None:
+            return "AstLeaf('x_{0}')".format(self.ast_index)
+        if self.name is not None:
+            return "AstLeaf('{0}')".format(self.name)
+
+    def evaluate_with_leaf_info(self, leafs_info, leafs_value):
+        dict_index_to_value = {
+            leaf_info.ast.ast_index: leaf_value
+            for leaf_info, leaf_value in zip(leafs_info, leafs_value)
+        }
+        res = self.evaluate(dict_index_to_value)
+        return res
+
+    def evaluate(self, dict_index_to_value):
+        if self.is_constant() and self.mop is not None:
+            return self.mop.nnn.value
+        assert self.ast_index is not None
+        return dict_index_to_value.get(self.ast_index)
+
+    def get_depth_signature(self, depth):
+        if depth == 1:
+            if self.is_constant():
+                return ["C"]
+            return ["L"]
+        else:
+            return ["N"] * (2 ** (depth - 1))
+
+    def __str__(self):
+        try:
+            if self.is_constant() and self.mop is not None:
+                return "{0}".format(hex(self.mop.nnn.value))
+            if self.z3_var_name is not NOT_GIVEN:
+                return self.z3_var_name
+            if self.ast_index is not None:
+                return "x_{0}".format(self.ast_index)
+            if self.mop is not None:
+                return format_mop_t(self.mop)
+            return self.name
+        except RuntimeError as e:
+            logger.info("Error while calling __str__ on AstLeaf: {0}".format(e))
+            return "Error_AstLeaf"
+
+    def __repr__(self):
+        return f"AstLeaf('{str(self)}')"
+
+
+class AstConstant(AstLeaf):
+    def __init__(self, name, expected_value=None, expected_size=None):
+        super().__init__(name)
+        self.expected_value = expected_value
+        self.expected_size = expected_size
+
+    @property
+    def value(self):
+        assert self.mop is not None and self.mop.t == ida_hexrays.mop_n
+        return self.mop.nnn.value
+
+    @_compat.override
+    def is_constant(self) -> bool:
+        # An AstConstant is always constant, so return True
+        return True
+
+    def _copy_mops_from_ast(self, other, read_only: bool = False):
+        if other.mop is not None and other.mop.t != ida_hexrays.mop_n:
+            if logger.debug_on:
+                logger.debug(
+                    "AstConstant._copy_mops_from_ast: other.mop is not a constant: %r",
+                    other.mop,
+                )
+            return False
+
+        if logger.debug_on:
+            logger.debug(
+                "AstConstant._copy_mops_from_ast: other %r's mop %s is a constant",
+                other,
+                format_mop_t(other.mop),
+            )
+        if not read_only:
+            self.mop = other.mop
+        if self.expected_value is None:
+            if not read_only:
+                self.expected_value = other.mop.nnn.value
+                self.expected_size = other.mop.size
+            else:
+                return True
+        return self.expected_value == other.mop.nnn.value
+
+    def evaluate(self, dict_index_to_value=None):
+        if self.mop is not None and self.mop.t == ida_hexrays.mop_n:
+            return self.mop.nnn.value
+        return self.expected_value
+
+    def get_depth_signature(self, depth):
+        if depth == 1:
+            return ["C"]
+        else:
+            return ["N"] * (2 ** (depth - 1))
+
+    @_compat.override
+    def __str__(self):
+        try:
+            if self.mop is not None and self.mop.t == ida_hexrays.mop_n:
+                return "0x{0:x}".format(self.mop.nnn.value)
+            if getattr(self, "expected_value", None) is not None:
+                return "0x{0:x}".format(self.expected_value)
+            return self.name
+        except RuntimeError as e:
+            logger.info("Error while calling __str__ on AstConstant: {0}".format(e))
+            return "Error_AstConstant"
+
+    @_compat.override
+    def __repr__(self):
+        return f"AstConstant({str(self)})"
+
+
+class AstProxy(AstBase):
+
+    def __init__(self, target_ast: AstBase):
+        # The proxy initially holds a reference to the shared, frozen template
+        self._target = target_ast
+
+    def _ensure_mutable(self):
+        """
+        Ensures the target is mutable. If the target is frozen, clone it and
+        replace our internal reference with the new, mutable clone.
+        Skips cloning if already mutable.
+        """
+        if self._target.is_frozen:
+            # Clone only if frozen (i.e., shared)
+            self._target = self._target.clone()
+
+    def __getattr__(self, name):
+        """
+        Handles all read access to attributes (e.g., proxy.opcode, proxy.left).
+        """
+        # Forward read requests directly to the target (shared or cloned).
+        return getattr(self._target, name)
+
+    def __setattr__(self, name, value):
+        """
+        Handles all write access to attributes (e.g., proxy.ast_index = 5).
+        """
+        if name == "_target":
+            # Special case to allow initialization of the proxy itself.
+            self.__dict__["_target"] = value
+            return
+
+        # 1. Trigger the clone-on-write check.
+        self._ensure_mutable()
+
+        # 2. Perform the write on the (now guaranteed to be mutable) target.
+        setattr(self._target, name, value)
+
+    # You might need to proxy other magic methods if your code uses them
+    # For example, if you use AstNode as a dict:
+    def __getitem__(self, key):
+        getitem = getattr(self._target, "__getitem__", None)
+        if getitem is None:
+            raise AttributeError(
+                f"Object of type {type(self._target)} does not support __getitem__"
+            )
+        return getitem(key)
+
+    def __setitem__(self, key, value):
+        setitem = getattr(self._target, "__setitem__", None)
+        if setitem is None:
+            raise AttributeError(
+                f"Object of type {type(self._target)} does not support __setitem__"
+            )
+        self._ensure_mutable()
+        setitem(key, value)
+
+    # ------------------------------------------------------------------
+    # Transparent attribute forwarding with sane fallback.
+    # ------------------------------------------------------------------
+
+    def __getattribute__(self, name):  # noqa: D401, ANN001
+        """Forward *all* attribute access to the wrapped target when:
+        1) the attribute is not private to the proxy itself, and
+        2) the value obtained from the proxy's own namespace is *None*.
+
+        This retains the cheap class-level default attributes coming from
+        AstBase (all set to None) while still exposing the real runtime
+        values stored in the wrapped AST object.
+        """
+
+        # Fast-path: internal/private attributes stay local.
+        if name.startswith("_"):
+            return super().__getattribute__(name)
+
+        try:
+            val = super().__getattribute__(name)
+        except AttributeError:
+            # Attribute not present on proxy → delegate unconditionally.
+            return getattr(super().__getattribute__("_target"), name)
+
+        # If the proxy's value is a meaningless placeholder (None) but the
+        # underlying object has a better value, return the latter instead.
+        if val is None:
+            target = super().__getattribute__("_target")
+            return getattr(target, name)
+        return val
+
+    @property
+    @_compat.override
+    def is_frozen(self) -> bool:
+        return self._target.is_frozen
+
+    @_compat.override
+    def clone(self) -> AstBase:
+        return AstProxy(self._target.clone())
+
+    @_compat.override
+    def freeze(self) -> None:
+        self._target.freeze()
+
+    @_compat.override
+    def is_node(self) -> bool:
+        return self._target.is_node()
+
+    @_compat.override
+    def is_leaf(self) -> bool:
+        return self._target.is_leaf()
+
+    @_compat.override
+    def is_constant(self) -> bool:
+        return self._target.is_constant()
+
+    @_compat.override
+    def compute_sub_ast(self) -> None:
+        self._target.compute_sub_ast()
+
+    @_compat.override
+    def get_leaf_list(self) -> list[AstLeaf]:
+        return self._target.get_leaf_list()
+
+    @_compat.override
+    def reset_mops(self) -> None:
+        self._target.reset_mops()
+
+    @_compat.override
+    def _copy_mops_from_ast(self, other: AstBase) -> bool:
+        return self._target._copy_mops_from_ast(other)
+
+    @_compat.override
+    def create_mop(self, ea: int) -> ida_hexrays.mop_t:
+        return self._target.create_mop(ea)
+
+    @_compat.override
+    def get_pattern(self) -> str:
+        return self._target.get_pattern()
+
+    @_compat.override
+    def evaluate(self, dict_index_to_value: dict[int, int]) -> int:
+        return self._target.evaluate(dict_index_to_value)
+
+    @_compat.override
+    def get_depth_signature(self, depth: int) -> list[str]:
+        return self._target.get_depth_signature(depth)
+
+    @_compat.override
+    def __str__(self):
+        return f"AstProxy({self._target.__class__.__name__}({str(self._target)}))"
+
+    @_compat.override
+    def __repr__(self):
+        return f"AstProxy({repr(self._target)})"
+
+    # Explicitly forward critical leaf data that callers expect to access
+    # directly.  Without these properties Python finds the *class*-level
+    # attribute defined in AstBase (value = None) and never triggers
+    # __getattr__, so evaluators see a leaf with no mop.
+
+    # ‑-- Mop -------------------------------------------------------------
+    @property
+    def mop(self):  # type: ignore[override]
+        return self._target.mop
+
+    @mop.setter
+    def mop(self, value):  # noqa: ANN001
+        self._ensure_mutable()
+        self._target.mop = value
+
+    # Convenience setters for a few commonly mutated fields
+    @property
+    def dest_size(self):  # type: ignore[override]
+        return self._target.dest_size
+
+    @dest_size.setter
+    def dest_size(self, value):  # noqa: ANN001
+        self._ensure_mutable()
+        self._target.dest_size = value
+
+    @property
+    def ea(self):  # type: ignore[override]
+        return self._target.ea
+
+    @ea.setter
+    def ea(self, value):  # noqa: ANN001
+        self._ensure_mutable()
+        self._target.ea = value
+
+    @property
+    def ast_index(self):  # type: ignore[override]
+        return self._target.ast_index
+
+    @ast_index.setter
+    def ast_index(self, value):  # noqa: ANN001
+        self._ensure_mutable()
+        self._target.ast_index = value
+
+
+class AstBuilderContext:
+    """
+    Manages the state during the recursive construction of an AST.
+    This avoids passing multiple related arguments through the recursion
+    and provides a clean way to store the lookup dictionary.
+    """
+
+    def __init__(self):
+        # The list of unique AST nodes. The index in this list is the ast_index.
+        self.unique_asts: list[AstBase] = []
+
+        # The fast lookup dictionary.
+        # Maps a mop's unique key to its index in the unique_asts list.
+        self.mop_key_to_index: dict[tuple[int, str], int] = {}
+
+
+def get_mop_key(mop: ida_hexrays.mop_t) -> tuple:
+    """
+    Generates a fast, hashable key from a mop_t's essential attributes.
+    This is significantly faster than using mop.dstr().
+    """
+    t = mop.t
+
+    # Hex-Rays assigns a new SSA value number (valnum) every time an operand is
+    # produced, even when it represents the *same* memory/register location.
+    # Including valnum in the cache key therefore forces the AST builder to
+    # create a distinct AstLeaf for each SSA instance (x_0, x_6, …), which
+    # breaks pattern rules that expect a single variable.
+
+    # We drop valnum from the key for all operand kinds except plain numeric
+    # constants, where valnum is useful to avoid collisions when two literals
+    # share the same size.
+
+    key = (t, mop.size) if t != ida_hexrays.mop_n else (t, mop.size, mop.valnum)
+    match t:
+        case ida_hexrays.mop_n:
+            return key + (mop.nnn.value,)
+        case ida_hexrays.mop_r:
+            return key + (mop.r,)
+        case ida_hexrays.mop_d:
+            # Using the micro-instruction EA differentiates identical loads that
+            # happen at different addresses, producing multiple leaves for the
+            # same logical value.  Instead we rely on the operand text (dstr),
+            # which is identical for identical expressions regardless of SSA
+            # copy location.
+            try:
+                return key + (mop.dstr(),)
+            except Exception:
+                # As a last resort fall back to EA.
+                return key + (mop.d.ea if mop.d else idaapi.BADADDR,)
+        case ida_hexrays.mop_S:
+            return key + (mop.s.off,)
+        case ida_hexrays.mop_v:
+            return key + (mop.g,)
+        case ida_hexrays.mop_l:
+            return key + (mop.l.idx, mop.l.off)
+        case ida_hexrays.mop_b:
+            return key + (mop.b,)
+        case ida_hexrays.mop_h:
+            return key + (mop.helper,)
+        case ida_hexrays.mop_str:
+            return key + (mop.cstr,)
+        case _:
+            # For other types, including complex ones like mop_f, mop_a, etc.,
+            # and mop_z, we fall back to the slower but safer dstr().
+            # This is a deliberate trade-off for robustness.
+            try:
+                return key + (mop.dstr(),)
+            except Exception:
+                # As a last resort, if dstr() fails, use a placeholder.
+                # This can happen for uninitialized or unusual mop_t instances.
+                logger.warning(
+                    "get_mop_key: Unsupported mop_t type: %s, returning placeholder",
+                    mop_type_to_string(t),
+                )
+                return key + (f"unsupported_mop_t_{t}",)
+
+
+def mop_to_ast_internal(
+    mop: ida_hexrays.mop_t, context: AstBuilderContext, root: bool = False
+) -> AstBase | None:
+    # Only log at root
+    if root and logger.debug_on:
+        logger.debug(
+            "[mop_to_ast_internal] Processing root mop: %s",
+            str(mop.dstr()) if hasattr(mop, "dstr") else str(mop),
+        )
+
+    # Early filter at root: only process if supported, with one exception:
+    # If the root is an m_call that has no argument list (r is mop_z) we treat it
+    # as transparent and attempt to build an AST from its destination operand.
+    if root:
+        if hasattr(mop, "d") and hasattr(mop.d, "opcode"):
+            root_opcode = mop.d.opcode
+
+            # Transparent helper call wrappers are now normalised by a
+            # peephole pass (TransparentCallUnwrapRule).  No special handling
+            # needed here anymore.
+
+            if root_opcode not in MBA_RELATED_OPCODES and not is_rotate_helper_call(
+                mop.d
+            ):
+                if logger.debug_on:
+                    logger.debug(
+                        "Skipping AST build for unsupported root opcode: %s",
+                        opcode_to_string(root_opcode),
+                    )
+                return None
+
+    # 1. Create the unique, hashable key for the current mop.
+    key = get_mop_key(mop)
+
+    # 2. Thread-local deduplication: if we've already built an AST for *this*
+    #    mop during the current recursive walk, return the existing instance to
+    #    avoid exponential explosion.
+    if key in context.mop_key_to_index:
+        existing_index = context.mop_key_to_index[key]
+        return context.unique_asts[existing_index]
+
+    # Rotate helper calls (__ROL*/__ROR*) are now inlined into plain shift/or
+    # instructions by RotateHelperInlineRule (peephole, MMAT_GLBOPT1).
+    # No special handling required here.
+
+    # Helper calls that evaluate to constants are now canonicalised by
+    # ConstantCallResultFoldRule (peephole GLBOPT1).
+
+    # NEW: Build AST nodes for MBA-related opcodes (binary or unary)
+    if mop.t == ida_hexrays.mop_d and mop.d.opcode in MBA_RELATED_OPCODES:
+        nb_ops = OPCODES_INFO[mop.d.opcode]["nb_operands"]
+
+        # Gather children ASTs based on operand count
+        left_ast = (
+            mop_to_ast_internal(mop.d.l, context) if mop.d.l is not None else None
+        )
+        right_ast = (
+            mop_to_ast_internal(mop.d.r, context)
+            if (nb_ops >= 2 and mop.d.r is not None)
+            else None
+        )
+
+        # Require at least the mandatory operands; if missing, fall back to leaf
+        if left_ast is None:
+            # Can't build meaningful node - fallback later to leaf
+            if logger.debug_on:
+                logger.debug(
+                    "[mop_to_ast_internal] Missing mandatory operand(s) for opcode %s, will treat as leaf",
+                    opcode_to_string(mop.d.opcode),
+                )
+        else:
+            # Only use dst_ast if destination present (ternary ops like m_stx etc.)
+            dst_ast = (
+                mop_to_ast_internal(mop.d.d, context) if mop.d.d is not None else None
+            )
+            tree = AstNode(mop.d.opcode, left_ast, right_ast, dst_ast)
+
+            # Set dest_size robustly
+            if hasattr(mop, "size") and mop.size:
+                tree.dest_size = mop.size
+            elif hasattr(mop.d, "size") and mop.d.size:
+                tree.dest_size = mop.d.size
+            elif mop.d.l is not None and hasattr(mop.d.l, "size"):
+                tree.dest_size = mop.d.l.size
+            else:
+                tree.dest_size = None
+
+            tree.mop = mop
+            tree.ea = sanitize_ea(mop.d.ea)
+
+            if logger.debug_on:
+                logger.debug(
+                    "[mop_to_ast_internal] Created AstNode for opcode %s (ea=0x%X): %s",
+                    opcode_to_string(mop.d.opcode),
+                    mop.d.ea if hasattr(mop.d, "ea") else -1,
+                    tree,
+                )
+            new_index = len(context.unique_asts)
+            tree.ast_index = new_index
+            context.unique_asts.append(tree)
+            context.mop_key_to_index[key] = new_index
+            return tree
+
+    # Special handling for mop_d that wraps an m_ldc as a constant leaf
+    if (
+        mop.t == ida_hexrays.mop_d
+        and mop.d is not None
+        and mop.d.opcode == ida_hexrays.m_ldc
+    ):
+        # Only treat it as constant if the *source* of the ldc is itself a
+        # numeric constant.  Otherwise we ignore the ldc wrapper and fall
+        # back to the generic leaf logic below.
+        ldc_src = mop.d.l
+        if ldc_src is not None and ldc_src.t == ida_hexrays.mop_n:
+            const_val = int(ldc_src.nnn.value)
+            const_size = ldc_src.size
+
+            const_leaf = AstConstant(hex(const_val), const_val, const_size)
+            # Clone numeric mop to detach from Hex-Rays internal storage
+            cloned_mop = ida_hexrays.mop_t()
+            cloned_mop.make_number(const_val, const_size)
+            const_leaf.mop = cloned_mop
+            const_leaf.dest_size = const_size
+
+            new_index = len(context.unique_asts)
+            const_leaf.ast_index = new_index
+            context.unique_asts.append(const_leaf)
+            context.mop_key_to_index[key] = new_index
+            return const_leaf
+
+    # Fallback for any unhandled mop: treat as a leaf.
+    # This is for simple operands (registers, stack vars) or complex
+    # instructions that are not part of our MBA analysis.
+    if (
+        mop.t != ida_hexrays.mop_d
+        or (mop.d.opcode not in MBA_RELATED_OPCODES)
+        or mop.d.l is None
+        or mop.d.r is None
+    ):
+        tree: AstBase | None
+        if mop.t == ida_hexrays.mop_n:
+            const_val = int(mop.nnn.value)
+            const_size = mop.size
+            tree = AstConstant(hex(const_val), const_val, const_size)
+            # Re-use a shared constant mop_t from the global cache to avoid the
+            # overhead of allocating a fresh object for every identical literal.
+            tree.mop = get_constant_mop(const_val, const_size)
+            tree.dest_size = const_size  # detached copy
+        # Typed-immediate wrappers (mop_f) are now normalised by the
+        # TypedImmediateCanonicaliseRule peephole pass.  If we still see one
+        # here it means it holds *no* literal value, therefore fall through to
+        # generic leaf creation.
+        elif mop.t == ida_hexrays.mop_f:
+            tree = None
+        else:
+            tree = None
+
+        # ------------------------------------------------------------------
+        # If we still haven't built a node, create a generic AstLeaf now.  This
+        # guarantees that *tree* is always defined even if new mop_t kinds are
+        # introduced in future IDA versions.
+        # ------------------------------------------------------------------
+        if tree is None:
+            tree = AstLeaf(format_mop_t(mop))
+            if logger.debug_on:
+                logger.debug(
+                    "[mop_to_ast_internal] Tree is NONE! Defaulting to AstLeaf for mop type %s dstr=%s",
+                    mop_type_to_string(mop.t),
+                    str(mop.dstr()) if hasattr(mop, "dstr") else str(mop),
+                )
+            tree.dest_size = mop.size
+
+        # For non-constant leaves we deliberately *do not* keep a reference
+        # to the original mop_t object, because Hex-Rays may free or reuse
+        # it after micro-optimisations, leading to use-after-free crashes.
+        # Only constant leaves benefit from holding the numeric mop to
+        # speed up further evaluations.
+        if tree.is_constant():
+            tree.mop = getattr(tree, "mop", None) or mop
+        else:
+            tree = AstLeaf(format_mop_t(mop))
+            if logger.debug_on:
+                logger.debug(
+                    "[mop_to_ast_internal] Fallback to AstLeaf for mop type %s dstr=%s",
+                    mop_type_to_string(mop.t),
+                    str(mop.dstr()) if hasattr(mop, "dstr") else str(mop),
+                )
+            tree.dest_size = mop.size
+
+        # Preserve previously assigned mop (e.g., inner numeric mop) unless
+        # it is still unset.  This prevents clobbering the constant `mop_n`
+        # we stored above with the wrapper operand, which would break
+        # constant detection later in the pipeline.
+        if getattr(tree, "mop", None) is None:
+            tree.mop = mop
+        dest_size = (
+            mop.size
+            if mop.t != ida_hexrays.mop_d
+            else mop.d.d.size if mop.d.d is not None else mop.size
+        )
+        tree.dest_size = dest_size
+        new_index = len(context.unique_asts)
+        tree.ast_index = new_index
+        context.unique_asts.append(tree)
+        context.mop_key_to_index[key] = new_index
+        return tree
+
+    # If we reach here, we failed to build an AST. Log the full mop tree.
+    logger.error("[mop_to_ast_internal] Could not build AST for mop. Dumping mop tree:")
+    mop_tree(mop)
+    return None
+
+
+def mop_to_ast(mop: ida_hexrays.mop_t) -> AstProxy | None:
+    """
+    Converts a mop_t to an AST node, with caching to avoid re-computation.
+
+    Returns a deep copy of the cached AST to prevent side-effects from
+    mutations by the caller.
+    """
+
+    # 1. Create a stable, hashable key from the mop_t object.
+    cache_key = get_mop_key(mop)
+
+    # 2. Global template cache: return a proxy if we already know the template
+    if cache_key in MOP_TO_AST_CACHE:
+        cached_template = MOP_TO_AST_CACHE[cache_key]
+        if cached_template is None:
+            return None  # Previously determined unconvertible.
+        return AstProxy(cached_template)
+
+    builder_context = AstBuilderContext()
+    # Start the optimized recursive build.
+
+    if not (mop_ast := mop_to_ast_internal(mop, builder_context, root=True)):
+        # Cache the failure to avoid re-computing it.
+        MOP_TO_AST_CACHE[cache_key] = None
+        return None
+
+    # This mutates the mop_ast object, populating its sub_ast_info.
+    # We do this ONCE before caching the "template" object, then we
+    # freeze the object to prevent mutations.
+    mop_ast.compute_sub_ast()
+    mop_ast.freeze()
+
+    # 4. Store the newly computed "template" object in the cache.
+    MOP_TO_AST_CACHE[cache_key] = mop_ast
+
+    # 5. Return a proxy to the caller for safety.
+    return AstProxy(mop_ast)
+
+
+def _py_slow_minsn_to_ast(instruction: ida_hexrays.minsn_t) -> AstProxy | None:
+    try:
+        # Early filter: forbidden opcodes
+        if instruction.opcode in MINSN_TO_AST_FORBIDDEN_OPCODES:
+            if logger.debug_on:
+                logger.debug(
+                    "Skipping AST build for forbidden opcode: %s @ 0x%x %s",
+                    opcode_to_string(instruction.opcode),
+                    instruction.ea,
+                    (
+                        "({0})".format(instruction.dstr())
+                        if instruction.opcode != ida_hexrays.m_jtbl
+                        else ""
+                    ),
+                )
+            return None
+
+        # Early filter: unsupported opcodes (not in MBA_RELATED_OPCODES)
+        # Allow rotate helper calls ("__ROL*" / "__ROR*") even though m_call
+        # is normally filtered out - they can be constant-folded later.
+        if instruction.opcode not in MBA_RELATED_OPCODES and not is_rotate_helper_call(
+            instruction
+        ):
+            if logger.debug_on:
+                logger.debug(
+                    "Skipping AST build for unsupported opcode: %s @ 0x%x %s",
+                    opcode_to_string(instruction.opcode),
+                    instruction.ea,
+                    (
+                        "({0})".format(instruction.dstr())
+                        if instruction.opcode != ida_hexrays.m_jtbl
+                        else ""
+                    ),
+                )
+            return None
+
+        # Constant-returning helper calls are folded to m_ldc by the peephole
+        # pass ConstantCallResultFoldRule.  No need for AST special case.
+
+        # Transparent-call shortcut: no args, computation stored in destination mop_d
+        if (
+            instruction.opcode == ida_hexrays.m_call
+            and (instruction.r is None or instruction.r.t == ida_hexrays.mop_z)
+            and instruction.d is not None
+            and instruction.d.t == ida_hexrays.mop_d
+        ):
+            if logger.debug_on:
+                logger.debug(
+                    "[minsn_to_ast] Unwrapping call with empty args; using destination expression for AST",
+                )
+            dest_ast = mop_to_ast(instruction.d)
+            if dest_ast is not None:
+                return dest_ast
+
+        ins_mop = ida_hexrays.mop_t()
+        ins_mop.create_from_insn(instruction)
+
+        # if instruction.opcode == ida_hexrays.m_mov:
+        #     tmp = AstNode(ida_hexrays.m_mov, mop_to_ast(ins_mop))
+        #     tmp.mop = ins_mop
+        #     tmp.dest_size = instruction.d.size
+        #     tmp.ea = instruction.ea
+        #     tmp.dst_mop = instruction.d
+        #     return tmp
+
+        tmp = mop_to_ast(ins_mop)
+        if tmp is None:
+            if logger.debug_on:
+                logger.debug(
+                    "Skipping AST build for unsupported or nop instruction: %s @ 0x%x %s",
+                    opcode_to_string(instruction.opcode),
+                    instruction.ea,
+                    (
+                        "({0})".format(instruction.dstr())
+                        if instruction.opcode != ida_hexrays.m_jtbl
+                        else ""
+                    ),
+                )
+        else:
+            tmp.dst_mop = instruction.d
+        return tmp
+    except RuntimeError as e:
+        logger.error(
+            "Error while transforming instruction %s: %s",
+            format_minsn_t(instruction),
+            e,
+        )
+
+
+# Public, unified entrypoint that callers can use instead of reaching into
+# the Cython module directly. If `use_cython` is True and the extension is
+# available, we delegate to `_cy_fast_minsn_to_ast`; otherwise we call the
+# pure-Python builder (defined below or elsewhere in this module).
+def minsn_to_ast(ins: ida_hexrays.minsn_t) -> typing.Any | None:
+    # # Fast path
+    # if use_cython and _CYTHON_FAST_AST_OK:
+    #     return _cy_fast_minsn_to_ast(
+    #         ins,
+    #         MOP_TO_AST_CACHE,
+    #         AstProxy,
+    #         AstNode,
+    #         AstLeaf,
+    #         AstConstant,
+    #         get_constant_mop,
+    #         MBA_RELATED_OPCODES,
+    #     )
+    # Slow path
+    return _py_slow_minsn_to_ast(ins)
+
+
+# # Side-by-side checker to compare the two implementations on the same input.
+# # Returns (py_ast, cy_ast). Caller can diff patterns, shapes, etc.
+# def compare_cython_vs_python(
+#     ins: ida_hexrays.minsn_t,
+# ) -> tuple[typing.Any | None, typing.Any | None]:
+#     py_ast = _py_slow_minsn_to_ast(ins)
+#     cy_ast = None
+#     if _CYTHON_FAST_AST_OK:
+#         cy_ast = _cy_fast_minsn_to_ast(
+#             ins,
+#             MOP_TO_AST_CACHE,
+#             AstProxy,
+#             AstNode,
+#             AstLeaf,
+#             AstConstant,
+#             get_constant_mop,
+#             MBA_RELATED_OPCODES,
+#         )
+#     return py_ast, cy_ast
diff --git a/src/d810/expr/ast.pxd b/src/d810/expr/ast.pxd
new file mode 100644
index 0000000..dc59a42
--- /dev/null
+++ b/src/d810/expr/ast.pxd
@@ -0,0 +1,32 @@
+
+
+cdef class AstBase:
+    cdef public dict sub_ast_info_by_index
+    cdef public object mop
+    cdef public object dst_mop
+    cdef public object dest_size
+    cdef public object ea
+    cdef public object ast_index
+
+cdef class AstNode(AstBase):
+    cdef public object opcode
+    cdef public object left
+    cdef public object right
+    cdef public object dst
+    cdef public object dst_mop
+    cdef public list opcodes
+    cdef public bint is_candidate_ok
+    cdef public list leafs
+    cdef public dict leafs_by_name
+    cdef public object func_name
+    cdef public bint _is_frozen
+
+cdef class AstLeaf(AstBase):
+    cdef public object name
+    cdef public object z3_var
+    cdef public object z3_var_name
+    cdef public bint _is_frozen
+
+cdef class AstConstant(AstLeaf):
+    cdef public object expected_value
+    cdef public object expected_size
\ No newline at end of file
diff --git a/src/d810/expr/ast.py b/src/d810/expr/ast.py
index 6a70cd5..e2efd15 100644
--- a/src/d810/expr/ast.py
+++ b/src/d810/expr/ast.py
@@ -1,1674 +1,22 @@
-from __future__ import annotations
-
-import abc
-import dataclasses
-import typing
-
-import ida_hexrays
-import idaapi
-
-import d810._compat as _compat
-from d810.conf.loggers import getLogger
-from d810.errors import AstEvaluationException
-from d810.expr.utils import (
-    MOP_CONSTANT_CACHE,
-    MOP_TO_AST_CACHE,
-    get_add_cf,
-    get_add_of,
-    get_parity_flag,
-    get_sub_of,
-    signed_to_unsigned,
-    unsigned_to_signed,
-)
-from d810.hexrays.hexrays_formatters import (
-    format_minsn_t,
-    format_mop_t,
-    mop_tree,
-    mop_type_to_string,
-    opcode_to_string,
-    sanitize_ea,
-)
-from d810.hexrays.hexrays_helpers import (
-    AND_TABLE,
-    MBA_RELATED_OPCODES,
-    MINSN_TO_AST_FORBIDDEN_OPCODES,
-    OPCODES_INFO,
-    Z3_SPECIAL_OPERANDS,
-    equal_mops_ignore_size,
-    is_rotate_helper_call,
-)
-from d810.registry import NOT_GIVEN, NotGiven
-
-logger = getLogger(__name__)
-
-
-def get_constant_mop(value: int, size: int) -> ida_hexrays.mop_t:
-    """
-    Returns a cached or new mop_t for a constant value.
-    This avoids repeated calls to mop_t.__init__ and make_number.
-    """
-    key = (value, size)
-    if key in MOP_CONSTANT_CACHE:
-        return MOP_CONSTANT_CACHE[key]
-
-    # Not in cache, create it once and store it.
-    cst_mop = ida_hexrays.mop_t()
-    cst_mop.make_number(value, size)
-    MOP_CONSTANT_CACHE[key] = cst_mop
-    return cst_mop
-
-
-def clear_mop_to_ast_cache():
-    """
-    Call this when the analysis context changes (e.g., new function)
-    to prevent using stale data.
-    """
-    MOP_TO_AST_CACHE.clear()
-
-
-@dataclasses.dataclass(slots=True)
-class AstInfo:
-    ast: AstNode | AstLeaf
-    number_of_use: int
-
-    def __str__(self):
-        return f"{self.ast} used {self.number_of_use} times: {format_mop_t(self.ast.mop) if self.ast.mop else 0}"
-
-
-class AstBase(abc.ABC):
-
-    sub_ast_info_by_index: dict[int, AstInfo] = {}
-    mop: ida_hexrays.mop_t | None = None
-    dest_size: int | None = None
-    ea: int | None = None
-    ast_index: int | None = None
-
-    @property
-    @abc.abstractmethod
-    def is_frozen(self) -> bool: ...
-
-    @abc.abstractmethod
-    def clone(self) -> AstBase: ...
-
-    @abc.abstractmethod
-    def freeze(self) -> None: ...
-
-    @abc.abstractmethod
-    def is_node(self) -> bool: ...
-
-    @abc.abstractmethod
-    def is_leaf(self) -> bool: ...
-
-    @abc.abstractmethod
-    def is_constant(self) -> bool: ...
-
-    @abc.abstractmethod
-    def compute_sub_ast(self) -> None: ...
-
-    @abc.abstractmethod
-    def get_leaf_list(self) -> list[AstLeaf]: ...
-
-    @abc.abstractmethod
-    def reset_mops(self) -> None: ...
-
-    @abc.abstractmethod
-    def _copy_mops_from_ast(self, other: AstBase, read_only: bool = False) -> bool: ...
-
-    @abc.abstractmethod
-    def create_mop(self, ea: int) -> ida_hexrays.mop_t: ...
-
-    @abc.abstractmethod
-    def get_pattern(self) -> str: ...
-
-    @abc.abstractmethod
-    def evaluate(self, dict_index_to_value: dict[int, int]) -> int: ...
-
-    @abc.abstractmethod
-    def get_depth_signature(self, depth: int) -> list[str]: ...
-
-    def __bool__(self) -> bool:
-        return True
-
-
-class AstNode(AstBase, dict):
-    def __init__(
-        self,
-        opcode: int,
-        left: AstBase | None = None,
-        right: AstBase | None = None,
-        dst: AstBase | None = None,
-    ):
-        super().__init__()
-        self.opcode = opcode
-        self.left = left
-        self.right = right
-        self.dst = dst
-        self.dst_mop = None
-
-        self.opcodes = []
-        self.mop = None
-        self.is_candidate_ok = False
-
-        self.leafs = []
-        self.leafs_by_name = {}
-
-        self.ast_index = 0
-        self.sub_ast_info_by_index = {}
-
-        self.dest_size = None
-        self.ea = None
-        self.func_name: str = ""
-        self._is_frozen = False  # All newly created nodes are mutable by default
-
-    @property
-    @_compat.override
-    def is_frozen(self) -> bool:
-        return self._is_frozen
-
-    @_compat.override
-    def freeze(self):
-        """Recursively freezes this node and all its children."""
-        if self._is_frozen:
-            return
-        self._is_frozen = True
-        if hasattr(self, "left") and self.left:
-            self.left.freeze()
-        if hasattr(self, "right") and self.right:
-            self.right.freeze()
-        if hasattr(self, "dst") and self.dst:
-            self.dst.freeze()
-
-    @property
-    def size(self):
-        return self.mop.d.d.size if self.mop else 0
-
-    def compute_sub_ast(self):
-        self.sub_ast_info_by_index = {}
-        assert self.ast_index is not None
-        self.sub_ast_info_by_index[self.ast_index] = AstInfo(self, 1)
-
-        if self.left is not None:
-            self.left.compute_sub_ast()
-            for ast_index, ast_info in self.left.sub_ast_info_by_index.items():
-                if ast_index not in self.sub_ast_info_by_index.keys():
-                    self.sub_ast_info_by_index[ast_index] = AstInfo(ast_info.ast, 0)
-                self.sub_ast_info_by_index[
-                    ast_index
-                ].number_of_use += ast_info.number_of_use
-
-        if self.right is not None:
-            self.right.compute_sub_ast()
-            for ast_index, ast_info in self.right.sub_ast_info_by_index.items():
-                if ast_index not in self.sub_ast_info_by_index.keys():
-                    self.sub_ast_info_by_index[ast_index] = AstInfo(ast_info.ast, 0)
-                self.sub_ast_info_by_index[
-                    ast_index
-                ].number_of_use += ast_info.number_of_use
-
-    def get_information(self):
-        leaf_info_list = []
-        cst_list = []
-        opcode_list = []
-        self.compute_sub_ast()
-
-        for _, ast_info in self.sub_ast_info_by_index.items():
-            if (ast_info.ast.mop is not None) and (
-                ast_info.ast.mop.t != ida_hexrays.mop_z
-            ):
-                if ast_info.ast.is_leaf():
-                    if ast_info.ast.is_constant():
-                        cst_list.append(ast_info.ast.mop.nnn.value)
-                    else:
-                        leaf_info_list.append(ast_info)
-                else:
-                    ast_node = typing.cast(AstNode, ast_info.ast)
-                    opcode_list += [ast_node.opcode] * ast_info.number_of_use
-
-        return leaf_info_list, cst_list, opcode_list
-
-    def __getitem__(self, k: str) -> AstLeaf:
-        return self.leafs_by_name[k]
-
-    def get_leaf_list(self) -> list[AstLeaf]:
-        leafs = []
-        if self.left is not None:
-            leafs += self.left.get_leaf_list()
-        if self.right is not None:
-            leafs += self.right.get_leaf_list()
-        return leafs
-
-    def add_leaf(self, leaf_name: str, leaf_mop: ida_hexrays.mop_t):
-        leaf = AstLeaf(leaf_name)
-        leaf.mop = leaf_mop
-        self.leafs.append(leaf)
-        self.leafs_by_name[leaf_name] = leaf
-
-    def add_constant_leaf(self, leaf_name: str, cst_value: int, cst_size: int):
-        masked_value = cst_value & AND_TABLE[cst_size]
-        cst_mop = get_constant_mop(masked_value, cst_size)
-        self.add_leaf(leaf_name, cst_mop)
-
-    def check_pattern_and_copy_mops(
-        self, ast: AstNode | AstLeaf, read_only: bool = False
-    ) -> bool:
-        if not read_only:
-            self.reset_mops()
-        if logger.debug_on:
-            logger.debug(
-                "AstNode.check_pattern_and_copy_mops from %r",
-                ast,
-            )
-        is_matching_shape = self._copy_mops_from_ast(ast, read_only)
-        if not is_matching_shape:
-            return False
-        return self._check_implicit_equalities()
-
-    def reset_mops(self):
-        self.mop = None
-        if self.left is not None:
-            self.left.reset_mops()
-        if self.right is not None:
-            self.right.reset_mops()
-
-    def _copy_mops_from_ast(
-        self, other: AstNode | AstLeaf, read_only: bool = False
-    ) -> bool:
-        if not other.is_node():
-            return False
-        other = typing.cast(AstNode, other)
-        if self.opcode != other.opcode:
-            return False
-
-        if not read_only:
-            self.mop = other.mop
-            self.dst_mop = other.dst_mop
-            self.dest_size = other.dest_size
-            self.ea = other.ea
-
-        if logger.debug_on:
-            logger.debug(
-                "AstNode._copy_mops_from_ast: self.left: %r, other.left: %r",
-                self.left,
-                other.left,
-            )
-        if self.left is not None and other.left is not None:
-            if not self.left._copy_mops_from_ast(other.left, read_only):
-                return False
-        if logger.debug_on:
-            logger.debug(
-                "AstNode._copy_mops_from_ast: self.right: %r, other.right: %r",
-                self.right,
-                other.right,
-            )
-        if self.right is not None and other.right is not None:
-            if not self.right._copy_mops_from_ast(other.right, read_only):
-                return False
-        return True
-
-    def _check_implicit_equalities(self) -> bool:
-        self.leafs = self.get_leaf_list()
-        self.leafs_by_name = {}
-        self.is_candidate_ok = True
-
-        for leaf in self.leafs:
-            ref_leaf = self.leafs_by_name.get(leaf.name)
-            if ref_leaf is not None and leaf.mop is not None:
-                if not equal_mops_ignore_size(ref_leaf.mop, leaf.mop):
-                    self.is_candidate_ok = False
-            self.leafs_by_name[leaf.name] = leaf
-        return self.is_candidate_ok
-
-    def update_leafs_mop(
-        self,
-        other: AstNode,
-        other2: AstNode | None = None,
-    ) -> bool:
-        self.leafs = self.get_leaf_list()
-        all_leafs_found = True
-        for leaf in self.leafs:
-            if other is not None and leaf.name in other.leafs_by_name:
-                leaf.mop = other.leafs_by_name[leaf.name].mop
-            elif other2 is not None and leaf.name in other2.leafs_by_name:
-                leaf.mop = other2.leafs_by_name[leaf.name].mop
-            else:
-                all_leafs_found = False
-        return all_leafs_found
-
-    def create_mop(self, ea: int) -> ida_hexrays.mop_t:
-        new_ins = self.create_minsn(ea)
-        new_ins_mop = ida_hexrays.mop_t()
-        new_ins_mop.create_from_insn(new_ins)
-        return new_ins_mop
-
-    def create_minsn(self, ea: int, dest=None) -> ida_hexrays.minsn_t:
-        new_ins = ida_hexrays.minsn_t(ea)
-        new_ins.opcode = self.opcode
-
-        if self.left is not None:
-            new_ins.l = self.left.create_mop(ea)
-            if self.right is not None:
-                new_ins.r = self.right.create_mop(ea)
-
-        new_ins.d = ida_hexrays.mop_t()
-
-        if self.left is not None:
-            new_ins.d.size = new_ins.l.size
-        if dest is not None:
-            new_ins.d = dest
-        return new_ins
-
-    def get_pattern(self) -> str:
-        nb_operands = OPCODES_INFO[self.opcode]["nb_operands"]
-        if nb_operands == 0:
-            return "AstNode({0})".format(OPCODES_INFO[self.opcode]["name"])
-        elif nb_operands == 1 and self.left is not None:
-            return "AstNode(m_{0}, {1})".format(
-                OPCODES_INFO[self.opcode]["name"], self.left.get_pattern()
-            )
-        elif nb_operands == 2 and self.left is not None and self.right is not None:
-            return "AstNode(m_{0}, {1}, {2})".format(
-                OPCODES_INFO[self.opcode]["name"],
-                self.left.get_pattern(),
-                self.right.get_pattern(),
-            )
-        else:
-            raise ValueError(f"Invalid number of operands: {nb_operands}")
-
-    def evaluate_with_leaf_info(
-        self, leafs_info: list[AstInfo], leafs_value: list[int]
-    ) -> int:
-        dict_index_to_value: dict[int, int] = {}
-        for leaf_info, leaf_value in zip(leafs_info, leafs_value):
-            if leaf_info.ast.ast_index is not None:
-                dict_index_to_value[leaf_info.ast.ast_index] = leaf_value
-        res = self.evaluate(dict_index_to_value)
-        return res
-
-    def evaluate(self, dict_index_to_value: dict[int, int]) -> int:
-        if self.ast_index in dict_index_to_value:
-            return dict_index_to_value[self.ast_index]
-        if self.dest_size is None:
-            raise ValueError("dest_size is None")
-
-        res_mask = AND_TABLE[self.dest_size]
-
-        if self.left is None:
-            raise ValueError(f"left is None for opcode: {self.opcode}")
-
-        binary_opcodes = {
-            ida_hexrays.m_add,
-            ida_hexrays.m_sub,
-            ida_hexrays.m_mul,
-            ida_hexrays.m_udiv,
-            ida_hexrays.m_sdiv,
-            ida_hexrays.m_umod,
-            ida_hexrays.m_smod,
-            ida_hexrays.m_or,
-            ida_hexrays.m_and,
-            ida_hexrays.m_xor,
-            ida_hexrays.m_shl,
-            ida_hexrays.m_shr,
-            ida_hexrays.m_sar,
-            ida_hexrays.m_cfadd,
-            ida_hexrays.m_ofadd,
-            ida_hexrays.m_seto,
-            ida_hexrays.m_setnz,
-            ida_hexrays.m_setz,
-            ida_hexrays.m_setae,
-            ida_hexrays.m_setb,
-            ida_hexrays.m_seta,
-            ida_hexrays.m_setbe,
-            ida_hexrays.m_setg,
-            ida_hexrays.m_setge,
-            ida_hexrays.m_setl,
-            ida_hexrays.m_setle,
-            ida_hexrays.m_setp,
-        }
-
-        if self.opcode in binary_opcodes and self.right is None:
-            raise ValueError("right is None for binary opcode: {0}".format(self.opcode))
-
-        match self.opcode:
-            case ida_hexrays.m_mov:
-                return (self.left.evaluate(dict_index_to_value)) & res_mask
-            case ida_hexrays.m_neg:
-                return (-self.left.evaluate(dict_index_to_value)) & res_mask
-            case ida_hexrays.m_lnot:
-                return self.left.evaluate(dict_index_to_value) != 0
-            case ida_hexrays.m_bnot:
-                return (self.left.evaluate(dict_index_to_value) ^ res_mask) & res_mask
-            case ida_hexrays.m_xds:
-                left_value_signed = unsigned_to_signed(
-                    self.left.evaluate(dict_index_to_value), self.left.dest_size
-                )
-                return signed_to_unsigned(left_value_signed, self.dest_size) & res_mask
-            case ida_hexrays.m_xdu:
-                return (self.left.evaluate(dict_index_to_value)) & res_mask
-            case ida_hexrays.m_low:
-                return (self.left.evaluate(dict_index_to_value)) & res_mask
-            case ida_hexrays.m_high:
-                # Extract the upper half of the operand. We shift right by the
-                # size (in bits) of the current destination. For example, when
-                # evaluating a 32-bit "high" of a 64-bit operand we shift by
-                # 32 bits, then mask the result to the destination size.
-                if self.left.dest_size is None:
-                    raise ValueError("left.dest_size is None for m_high")
-                shift_bits = self.dest_size * 8 if self.dest_size is not None else 0
-                return (
-                    self.left.evaluate(dict_index_to_value) >> shift_bits
-                ) & res_mask
-            case ida_hexrays.m_add if self.right is not None:
-                return (
-                    self.left.evaluate(dict_index_to_value)
-                    + self.right.evaluate(dict_index_to_value)
-                ) & res_mask
-            case ida_hexrays.m_sub if self.right is not None:
-                return (
-                    self.left.evaluate(dict_index_to_value)
-                    - self.right.evaluate(dict_index_to_value)
-                ) & res_mask
-            case ida_hexrays.m_mul if self.right is not None:
-                return (
-                    self.left.evaluate(dict_index_to_value)
-                    * self.right.evaluate(dict_index_to_value)
-                ) & res_mask
-            case ida_hexrays.m_udiv if self.right is not None:
-                return (
-                    self.left.evaluate(dict_index_to_value)
-                    // self.right.evaluate(dict_index_to_value)
-                ) & res_mask
-            case ida_hexrays.m_sdiv if self.right is not None:
-                return (
-                    self.left.evaluate(dict_index_to_value)
-                    // self.right.evaluate(dict_index_to_value)
-                ) & res_mask
-            case ida_hexrays.m_umod if self.right is not None:
-                return (
-                    self.left.evaluate(dict_index_to_value)
-                    % self.right.evaluate(dict_index_to_value)
-                ) & res_mask
-            case ida_hexrays.m_smod if self.right is not None:
-                return (
-                    self.left.evaluate(dict_index_to_value)
-                    % self.right.evaluate(dict_index_to_value)
-                ) & res_mask
-            case ida_hexrays.m_or if self.right is not None:
-                return (
-                    self.left.evaluate(dict_index_to_value)
-                    | self.right.evaluate(dict_index_to_value)
-                ) & res_mask
-            case ida_hexrays.m_and if self.right is not None:
-                return (
-                    self.left.evaluate(dict_index_to_value)
-                    & self.right.evaluate(dict_index_to_value)
-                ) & res_mask
-            case ida_hexrays.m_xor if self.right is not None:
-                return (
-                    self.left.evaluate(dict_index_to_value)
-                    ^ self.right.evaluate(dict_index_to_value)
-                ) & res_mask
-            case ida_hexrays.m_shl if self.right is not None:
-                return (
-                    self.left.evaluate(dict_index_to_value)
-                    << self.right.evaluate(dict_index_to_value)
-                ) & res_mask
-            case ida_hexrays.m_shr if self.right is not None:
-                return (
-                    self.left.evaluate(dict_index_to_value)
-                    >> self.right.evaluate(dict_index_to_value)
-                ) & res_mask
-            case ida_hexrays.m_sar if self.right is not None:
-                left_value_signed = unsigned_to_signed(
-                    self.left.evaluate(dict_index_to_value), self.left.dest_size
-                )
-                res_signed = left_value_signed >> self.right.evaluate(
-                    dict_index_to_value
-                )
-                return signed_to_unsigned(res_signed, self.dest_size) & res_mask
-            case ida_hexrays.m_cfadd if self.right is not None:
-                tmp = get_add_cf(
-                    self.left.evaluate(dict_index_to_value),
-                    self.right.evaluate(dict_index_to_value),
-                    self.left.dest_size,
-                )
-                return tmp & res_mask
-            case ida_hexrays.m_ofadd if self.right is not None:
-                tmp = get_add_of(
-                    self.left.evaluate(dict_index_to_value),
-                    self.right.evaluate(dict_index_to_value),
-                    self.left.dest_size,
-                )
-                return tmp & res_mask
-            case ida_hexrays.m_sets:
-                left_value_signed = unsigned_to_signed(
-                    self.left.evaluate(dict_index_to_value), self.left.dest_size
-                )
-                res = 1 if left_value_signed < 0 else 0
-                return res & res_mask
-            case ida_hexrays.m_seto if self.right is not None:
-                left_value_signed = unsigned_to_signed(
-                    self.left.evaluate(dict_index_to_value), self.left.dest_size
-                )
-                right_value_signed = unsigned_to_signed(
-                    self.right.evaluate(dict_index_to_value), self.right.dest_size
-                )
-                sub_overflow = get_sub_of(
-                    left_value_signed, right_value_signed, self.left.dest_size
-                )
-                return sub_overflow & res_mask
-            case ida_hexrays.m_setnz if self.right is not None:
-                res = (
-                    1
-                    if self.left.evaluate(dict_index_to_value)
-                    != self.right.evaluate(dict_index_to_value)
-                    else 0
-                )
-                return res & res_mask
-            case ida_hexrays.m_setz if self.right is not None:
-                res = (
-                    1
-                    if self.left.evaluate(dict_index_to_value)
-                    == self.right.evaluate(dict_index_to_value)
-                    else 0
-                )
-                return res & res_mask
-            case ida_hexrays.m_setae if self.right is not None:
-                res = (
-                    1
-                    if self.left.evaluate(dict_index_to_value)
-                    >= self.right.evaluate(dict_index_to_value)
-                    else 0
-                )
-                return res & res_mask
-            case ida_hexrays.m_setb if self.right is not None:
-                res = (
-                    1
-                    if self.left.evaluate(dict_index_to_value)
-                    < self.right.evaluate(dict_index_to_value)
-                    else 0
-                )
-                return res & res_mask
-            case ida_hexrays.m_seta if self.right is not None:
-                res = (
-                    1
-                    if self.left.evaluate(dict_index_to_value)
-                    > self.right.evaluate(dict_index_to_value)
-                    else 0
-                )
-                return res & res_mask
-            case ida_hexrays.m_setbe if self.right is not None:
-                res = (
-                    1
-                    if self.left.evaluate(dict_index_to_value)
-                    <= self.right.evaluate(dict_index_to_value)
-                    else 0
-                )
-                return res & res_mask
-            case ida_hexrays.m_setg if self.right is not None:
-                left_value_signed = unsigned_to_signed(
-                    self.left.evaluate(dict_index_to_value), self.left.dest_size
-                )
-                right_value_signed = unsigned_to_signed(
-                    self.right.evaluate(dict_index_to_value), self.right.dest_size
-                )
-                res = 1 if left_value_signed > right_value_signed else 0
-                return res & res_mask
-            case ida_hexrays.m_setge if self.right is not None:
-                left_value_signed = unsigned_to_signed(
-                    self.left.evaluate(dict_index_to_value), self.left.dest_size
-                )
-                right_value_signed = unsigned_to_signed(
-                    self.right.evaluate(dict_index_to_value), self.right.dest_size
-                )
-                res = 1 if left_value_signed >= right_value_signed else 0
-                return res & res_mask
-            case ida_hexrays.m_setl if self.right is not None:
-                left_value_signed = unsigned_to_signed(
-                    self.left.evaluate(dict_index_to_value), self.left.dest_size
-                )
-                right_value_signed = unsigned_to_signed(
-                    self.right.evaluate(dict_index_to_value), self.right.dest_size
-                )
-                res = 1 if left_value_signed < right_value_signed else 0
-                return res & res_mask
-            case ida_hexrays.m_setle if self.right is not None:
-                left_value_signed = unsigned_to_signed(
-                    self.left.evaluate(dict_index_to_value), self.left.dest_size
-                )
-                right_value_signed = unsigned_to_signed(
-                    self.right.evaluate(dict_index_to_value), self.right.dest_size
-                )
-                res = 1 if left_value_signed <= right_value_signed else 0
-                return res & res_mask
-            case ida_hexrays.m_setp if self.right is not None:
-                res = get_parity_flag(
-                    self.left.evaluate(dict_index_to_value),
-                    self.right.evaluate(dict_index_to_value),
-                    self.left.dest_size,
-                )
-                return res & res_mask
-            case ida_hexrays.m_call:
-                if logger.debug_on:
-                    logger.debug(
-                        "evaluate m_call: ast_index=%s, dest_size=%s, callee=%s, args=%s",
-                        self.ast_index,
-                        self.dest_size,
-                        self.left,
-                        self.right,
-                    )
-                # Unknown runtime value – treat as 0 to let constant evaluation proceed.
-                return 0 & res_mask
-            case _:
-                raise AstEvaluationException(
-                    "Can't evaluate opcode: {0}".format(self.opcode)
-                )
-
-    def get_depth_signature(self, depth):
-        if depth == 1:
-            return ["{0}".format(self.opcode)]
-        tmp = []
-        nb_operands = OPCODES_INFO[self.opcode]["nb_operands"]
-        if (nb_operands >= 1) and self.left is not None:
-            tmp += self.left.get_depth_signature(depth - 1)
-        else:
-            tmp += ["N"] * (2 ** (depth - 2))
-        if (nb_operands >= 2) and self.right is not None:
-            tmp += self.right.get_depth_signature(depth - 1)
-        else:
-            tmp += ["N"] * (2 ** (depth - 2))
-        return tmp
-
-    def __str__(self):
-        try:
-            nb_operands = OPCODES_INFO[self.opcode]["nb_operands"]
-            if "symbol" in OPCODES_INFO[self.opcode].keys():
-                if nb_operands == 0:
-                    return "{0}()".format(OPCODES_INFO[self.opcode]["symbol"])
-                elif nb_operands == 1:
-                    return "{0}({1})".format(
-                        OPCODES_INFO[self.opcode]["symbol"], self.left
-                    )
-                elif nb_operands == 2:
-                    if OPCODES_INFO[self.opcode]["symbol"] not in Z3_SPECIAL_OPERANDS:
-                        return "({1} {0} {2})".format(
-                            OPCODES_INFO[self.opcode]["symbol"], self.left, self.right
-                        )
-                    else:
-                        return "{0}({1}, {2})".format(
-                            OPCODES_INFO[self.opcode]["symbol"], self.left, self.right
-                        )
-            else:
-                if nb_operands == 0:
-                    return "{0}()".format(OPCODES_INFO[self.opcode]["name"])
-                elif nb_operands == 1:
-                    return "{0}({1})".format(
-                        OPCODES_INFO[self.opcode]["name"], self.left
-                    )
-                elif nb_operands == 2:
-                    return "{0}({1}, {2})".format(
-                        OPCODES_INFO[self.opcode]["name"], self.left, self.right
-                    )
-        except RuntimeError as e:
-            logger.info("Error while calling __str__ on AstNode: {0}".format(e))
-        return "Error_AstNode"
-
-    def __repr__(self):
-        return f"AstNode({opcode_to_string(self.opcode)}, left={self.left}, right={self.right})"
-
-    @_compat.override
-    def clone(self):
-        # Use __new__ to bypass __init__ for speed
-        new_node = self.__class__.__new__(self.__class__)
-        super(AstNode, new_node).__init__()  # Initialize the dict part
-
-        # Manually copy attributes and clone children
-        new_node.opcode = self.opcode
-        new_node.left = self.left.clone() if self.left else None
-        new_node.right = self.right.clone() if self.right else None
-        new_node.dst = self.dst.clone() if self.dst else None
-
-        new_node.mop = self.mop
-        new_node.dst_mop = self.dst_mop
-        new_node.dest_size = self.dest_size
-        new_node.ea = self.ea
-        new_node.ast_index = self.ast_index
-
-        # Initialize transient state
-        new_node.is_candidate_ok = False
-        new_node.leafs = []
-        new_node.leafs_by_name = {}
-        new_node.opcodes = []
-        new_node.sub_ast_info_by_index = {}  # Start fresh
-
-        # Cloned objects start mutable
-        new_node._is_frozen = False
-
-        return new_node
-
-    @_compat.override
-    def is_node(self):
-        return True
-
-    @_compat.override
-    def is_leaf(self):
-        # An AstNode is not a leaf, so returns False
-        return False
-
-    @_compat.override
-    def is_constant(self):
-        return False
-
-
-class AstLeaf(AstBase):
-    def __init__(self, name):
-        self.name = name
-        self.ast_index: int | None = None
-
-        self.mop = None
-        self.z3_var = None
-        self.z3_var_name: str | NotGiven = NOT_GIVEN
-
-        self.dest_size = None
-        self.ea = None
-        self._is_frozen = False  # All newly created nodes are mutable by default
-        self.sub_ast_info_by_index = {}
-
-    @property
-    @_compat.override
-    def is_frozen(self) -> bool:
-        return self._is_frozen
-
-    @_compat.override
-    def freeze(self):
-        """Recursively freezes this node and all its children."""
-        if self._is_frozen:
-            return
-        self._is_frozen = True
-
-    @_compat.override
-    def is_node(self):
-        return False
-
-    @_compat.override
-    def is_leaf(self):
-        return True
-
-    @_compat.override
-    def is_constant(self):
-        if self.mop is None:
-            return False
-        return self.mop.t == ida_hexrays.mop_n
-
-    @_compat.override
-    def clone(self):
-        # Use __new__ to bypass __init__ for speed
-        new_leaf = self.__class__.__new__(self.__class__)
-
-        # Manually copy attributes. This is faster than generic deepcopy.
-        new_leaf.name = self.name
-        new_leaf.ast_index = self.ast_index
-        new_leaf.mop = self.mop
-        new_leaf.dest_size = self.dest_size
-        new_leaf.ea = self.ea
-
-        # Initialize transient state
-        new_leaf.z3_var = None
-        new_leaf.z3_var_name = NOT_GIVEN
-        new_leaf.sub_ast_info_by_index = {}  # Start fresh
-
-        # Cloned objects start mutable by definition
-        new_leaf._is_frozen = False
-
-        return new_leaf
-
-    def __getitem__(self, name: str) -> AstLeaf:
-        if name == self.name:
-            return self
-        raise KeyError
-
-    @property
-    def size(self):
-        return self.mop.size if self.mop else 0
-
-    @property
-    def dst_mop(self):
-        return self.mop
-
-    @dst_mop.setter
-    def dst_mop(self, mop):
-        self.mop = mop
-
-    @property
-    def value(self):
-        if self.is_constant() and self.mop is not None:
-            return self.mop.nnn.value
-        else:
-            return None
-
-    def compute_sub_ast(self):
-        self.sub_ast_info_by_index = {}
-        assert self.ast_index is not None
-        self.sub_ast_info_by_index[self.ast_index] = AstInfo(self, 1)
-
-    def get_information(self):
-        # Just here to allow calling get_information on either a AstNode or AstLeaf
-        return [], [], []
-
-    def get_leaf_list(self):
-        return [self]
-
-    def create_mop(self, ea):
-        # 1. Constant operands can keep using the shared cache
-        if self.is_constant() and self.value is not None:
-            # TODO: is this right?
-            size = self.dest_size if self.dest_size is not None else self.size
-            if logger.debug_on:
-                logger.debug(
-                    "AstLeaf.create_mop: Constant operand @ 0x%x: %s, size: %s, dest_size: %s, equal? %s",
-                    ea,
-                    self.value,
-                    size,
-                    self.dest_size,
-                    size == self.dest_size,
-                )
-            val = get_constant_mop(self.value, size)
-            if logger.debug_on:
-                logger.debug(
-                    "AstLeaf.create_mop: Constant operand reused: %s",
-                    val,
-                    extra={"ea": hex(ea)},
-                )
-            return val
-
-        if self.mop is None:
-            logger.error(
-                "%r mop is None in create_mop for 0x%x",
-                self,
-                ea,
-            )
-            raise AstEvaluationException(
-                f"{repr(self)}'s mop is None in create_mop for {hex(ea)}"
-            )
-
-        # 2. Otherwise, we need to create a new mop
-        new_mop = ida_hexrays.mop_t()
-        new_mop.assign(self.mop)
-        return new_mop  # duplicates the C++ object
-
-    def update_leafs_mop(self, other: AstNode, other2: AstNode | None = None):
-        if other is not None and self.name in other.leafs_by_name:
-            self.mop = other.leafs_by_name[self.name].mop
-            return True
-        elif other2 is not None and self.name in other2.leafs_by_name:
-            self.mop = other2.leafs_by_name[self.name].mop
-            return True
-        return False
-
-    def check_pattern_and_copy_mops(self, ast, read_only: bool = False):
-        if not read_only:
-            self.reset_mops()
-        is_matching_shape = self._copy_mops_from_ast(ast, read_only)
-
-        if not is_matching_shape:
-            return False
-        return self._check_implicit_equalities()
-
-    def reset_mops(self):
-        self.z3_var = None
-        self.z3_var_name = NOT_GIVEN
-        self.mop = None
-
-    def _copy_mops_from_ast(self, other, read_only: bool = False):
-        if other.mop is None:
-            if logger.debug_on:
-                logger.debug(
-                    "AstLeaf._copy_mops_from_ast: other %r's mop is None",
-                    other,
-                )
-            return False
-        if logger.debug_on:
-            logger.debug(
-                "AstLeaf._copy_mops_from_ast: other %r's mop %s is not None",
-                other,
-                format_mop_t(other.mop),
-            )
-        if not read_only:
-            self.mop = other.mop
-        return True
-
-    @staticmethod
-    def _check_implicit_equalities():
-        # An AstLeaf does not have any implicit equalities to be checked, so we always returns True
-        return True
-
-    def get_pattern(self):
-        if self.is_constant() and self.mop is not None:
-            return "AstConstant('{0}', {0})".format(self.mop.nnn.value)
-        if self.ast_index is not None:
-            return "AstLeaf('x_{0}')".format(self.ast_index)
-        if self.name is not None:
-            return "AstLeaf('{0}')".format(self.name)
-
-    def evaluate_with_leaf_info(self, leafs_info, leafs_value):
-        dict_index_to_value = {
-            leaf_info.ast.ast_index: leaf_value
-            for leaf_info, leaf_value in zip(leafs_info, leafs_value)
-        }
-        res = self.evaluate(dict_index_to_value)
-        return res
-
-    def evaluate(self, dict_index_to_value):
-        if self.is_constant() and self.mop is not None:
-            return self.mop.nnn.value
-        assert self.ast_index is not None
-        return dict_index_to_value.get(self.ast_index)
-
-    def get_depth_signature(self, depth):
-        if depth == 1:
-            if self.is_constant():
-                return ["C"]
-            return ["L"]
-        else:
-            return ["N"] * (2 ** (depth - 1))
-
-    def __str__(self):
-        try:
-            if self.is_constant() and self.mop is not None:
-                return "{0}".format(hex(self.mop.nnn.value))
-            if self.z3_var_name is not NOT_GIVEN:
-                return self.z3_var_name
-            if self.ast_index is not None:
-                return "x_{0}".format(self.ast_index)
-            if self.mop is not None:
-                return format_mop_t(self.mop)
-            return self.name
-        except RuntimeError as e:
-            logger.info("Error while calling __str__ on AstLeaf: {0}".format(e))
-            return "Error_AstLeaf"
-
-    def __repr__(self):
-        return f"AstLeaf('{str(self)}')"
-
-
-class AstConstant(AstLeaf):
-    def __init__(self, name, expected_value=None, expected_size=None):
-        super().__init__(name)
-        self.expected_value = expected_value
-        self.expected_size = expected_size
-
-    @property
-    def value(self):
-        assert self.mop is not None and self.mop.t == ida_hexrays.mop_n
-        return self.mop.nnn.value
-
-    @_compat.override
-    def is_constant(self) -> bool:
-        # An AstConstant is always constant, so return True
-        return True
-
-    def _copy_mops_from_ast(self, other, read_only: bool = False):
-        if other.mop is not None and other.mop.t != ida_hexrays.mop_n:
-            if logger.debug_on:
-                logger.debug(
-                    "AstConstant._copy_mops_from_ast: other.mop is not a constant: %r",
-                    other.mop,
-                )
-            return False
-
-        if logger.debug_on:
-            logger.debug(
-                "AstConstant._copy_mops_from_ast: other %r's mop %s is a constant",
-                other,
-                format_mop_t(other.mop),
-            )
-        if not read_only:
-            self.mop = other.mop
-        if self.expected_value is None:
-            if not read_only:
-                self.expected_value = other.mop.nnn.value
-                self.expected_size = other.mop.size
-            else:
-                return True
-        return self.expected_value == other.mop.nnn.value
-
-    def evaluate(self, dict_index_to_value=None):
-        if self.mop is not None and self.mop.t == ida_hexrays.mop_n:
-            return self.mop.nnn.value
-        return self.expected_value
-
-    def get_depth_signature(self, depth):
-        if depth == 1:
-            return ["C"]
-        else:
-            return ["N"] * (2 ** (depth - 1))
-
-    @_compat.override
-    def __str__(self):
-        try:
-            if self.mop is not None and self.mop.t == ida_hexrays.mop_n:
-                return "0x{0:x}".format(self.mop.nnn.value)
-            if getattr(self, "expected_value", None) is not None:
-                return "0x{0:x}".format(self.expected_value)
-            return self.name
-        except RuntimeError as e:
-            logger.info("Error while calling __str__ on AstConstant: {0}".format(e))
-            return "Error_AstConstant"
-
-    @_compat.override
-    def __repr__(self):
-        return f"AstConstant({str(self)})"
-
-
-class AstProxy(AstBase):
-
-    def __init__(self, target_ast: AstBase):
-        # The proxy initially holds a reference to the shared, frozen template
-        self._target = target_ast
-
-    @property
-    @_compat.override
-    def is_frozen(self) -> bool:
-        return self._target.is_frozen
-
-    @_compat.override
-    def clone(self) -> AstBase:
-        return AstProxy(self._target.clone())
-
-    @_compat.override
-    def freeze(self) -> None:
-        self._target.freeze()
-
-    @_compat.override
-    def is_node(self) -> bool:
-        return self._target.is_node()
-
-    @_compat.override
-    def is_leaf(self) -> bool:
-        return self._target.is_leaf()
-
-    @_compat.override
-    def is_constant(self) -> bool:
-        return self._target.is_constant()
-
-    @_compat.override
-    def compute_sub_ast(self) -> None:
-        self._target.compute_sub_ast()
-
-    @_compat.override
-    def get_leaf_list(self) -> list[AstLeaf]:
-        return self._target.get_leaf_list()
-
-    @_compat.override
-    def reset_mops(self) -> None:
-        self._target.reset_mops()
-
-    @_compat.override
-    def _copy_mops_from_ast(self, other: AstBase) -> bool:
-        return self._target._copy_mops_from_ast(other)
-
-    @_compat.override
-    def create_mop(self, ea: int) -> ida_hexrays.mop_t:
-        return self._target.create_mop(ea)
-
-    @_compat.override
-    def get_pattern(self) -> str:
-        return self._target.get_pattern()
-
-    @_compat.override
-    def evaluate(self, dict_index_to_value: dict[int, int]) -> int:
-        return self._target.evaluate(dict_index_to_value)
-
-    @_compat.override
-    def get_depth_signature(self, depth: int) -> list[str]:
-        return self._target.get_depth_signature(depth)
-
-    @_compat.override
-    def __str__(self):
-        return f"AstProxy({self._target.__class__.__name__}({str(self._target)}))"
-
-    @_compat.override
-    def __repr__(self):
-        return f"AstProxy({repr(self._target)})"
-
-    # Explicitly forward critical leaf data that callers expect to access
-    # directly.  Without these properties Python finds the *class*-level
-    # attribute defined in AstBase (value = None) and never triggers
-    # __getattr__, so evaluators see a leaf with no mop.
-
-    # ‑-- Mop -------------------------------------------------------------
-    @property
-    def mop(self):  # type: ignore[override]
-        return self._target.mop
-
-    @mop.setter
-    def mop(self, value):  # noqa: ANN001
-        self._ensure_mutable()
-        self._target.mop = value
-
-    def _ensure_mutable(self):
-        """
-        The magic method. If the target is frozen, clone it and
-        replace our internal reference with the new, mutable clone.
-        """
-        if self._target.is_frozen:
-            # This is the first write attempt. Time to clone.
-            self._target = self._target.clone()  # Assumes clone() exists
-
-    def __getattr__(self, name):
-        """
-        Handles all read access to attributes (e.g., proxy.opcode, proxy.left).
-        """
-        # Forward read requests directly to the target (shared or cloned).
-        return getattr(self._target, name)
-
-    def __setattr__(self, name, value):
-        """
-        Handles all write access to attributes (e.g., proxy.ast_index = 5).
-        """
-        if name == "_target":
-            # Special case to allow initialization of the proxy itself.
-            self.__dict__["_target"] = value
-            return
-
-        # 1. Trigger the clone-on-write check.
-        self._ensure_mutable()
-
-        # 2. Perform the write on the (now guaranteed to be mutable) target.
-        setattr(self._target, name, value)
-
-    # You might need to proxy other magic methods if your code uses them
-    # For example, if you use AstNode as a dict:
-    def __getitem__(self, key):
-        getitem = getattr(self._target, "__getitem__", None)
-        if getitem is None:
-            raise AttributeError(
-                f"Object of type {type(self._target)} does not support __getitem__"
-            )
-        return getitem(key)
-
-    def __setitem__(self, key, value):
-        setitem = getattr(self._target, "__setitem__", None)
-        if setitem is None:
-            raise AttributeError(
-                f"Object of type {type(self._target)} does not support __setitem__"
-            )
-        self._ensure_mutable()
-        setitem(key, value)
-
-    # ------------------------------------------------------------------
-    # Transparent attribute forwarding with sane fallback.
-    # ------------------------------------------------------------------
-
-    def __getattribute__(self, name):  # noqa: D401, ANN001
-        """Forward *all* attribute access to the wrapped target when:
-        1) the attribute is not private to the proxy itself, and
-        2) the value obtained from the proxy's own namespace is *None*.
-
-        This retains the cheap class-level default attributes coming from
-        AstBase (all set to None) while still exposing the real runtime
-        values stored in the wrapped AST object.
-        """
-
-        # Fast-path: internal/private attributes stay local.
-        if name.startswith("_"):
-            return super().__getattribute__(name)
-
-        try:
-            val = super().__getattribute__(name)
-        except AttributeError:
-            # Attribute not present on proxy → delegate unconditionally.
-            return getattr(super().__getattribute__("_target"), name)
-
-        # If the proxy's value is a meaningless placeholder (None) but the
-        # underlying object has a better value, return the latter instead.
-        if val is None:
-            target = super().__getattribute__("_target")
-            return getattr(target, name)
-        return val
-
-    # Convenience setters for a few commonly mutated fields
-    @property
-    def dest_size(self):  # type: ignore[override]
-        return self._target.dest_size
-
-    @dest_size.setter
-    def dest_size(self, value):  # noqa: ANN001
-        self._ensure_mutable()
-        self._target.dest_size = value
-
-    @property
-    def ea(self):  # type: ignore[override]
-        return self._target.ea
-
-    @ea.setter
-    def ea(self, value):  # noqa: ANN001
-        self._ensure_mutable()
-        self._target.ea = value
-
-    @property
-    def ast_index(self):  # type: ignore[override]
-        return self._target.ast_index
-
-    @ast_index.setter
-    def ast_index(self, value):  # noqa: ANN001
-        self._ensure_mutable()
-        self._target.ast_index = value
-
-
-class AstBuilderContext:
-    """
-    Manages the state during the recursive construction of an AST.
-    This avoids passing multiple related arguments through the recursion
-    and provides a clean way to store the lookup dictionary.
-    """
-
-    def __init__(self):
-        # The list of unique AST nodes. The index in this list is the ast_index.
-        self.unique_asts: list[AstBase] = []
-
-        # The fast lookup dictionary.
-        # Maps a mop's unique key to its index in the unique_asts list.
-        self.mop_key_to_index: dict[tuple[int, str], int] = {}
-
-
-def get_mop_key(mop: ida_hexrays.mop_t) -> tuple:
-    """
-    Generates a fast, hashable key from a mop_t's essential attributes.
-    This is significantly faster than using mop.dstr().
-    """
-    t = mop.t
-
-    # Hex-Rays assigns a new SSA value number (valnum) every time an operand is
-    # produced, even when it represents the *same* memory/register location.
-    # Including valnum in the cache key therefore forces the AST builder to
-    # create a distinct AstLeaf for each SSA instance (x_0, x_6, …), which
-    # breaks pattern rules that expect a single variable.
-
-    # We drop valnum from the key for all operand kinds except plain numeric
-    # constants, where valnum is useful to avoid collisions when two literals
-    # share the same size.
-
-    key = (t, mop.size) if t != ida_hexrays.mop_n else (t, mop.size, mop.valnum)
-    match t:
-        case ida_hexrays.mop_n:
-            return key + (mop.nnn.value,)
-        case ida_hexrays.mop_r:
-            return key + (mop.r,)
-        case ida_hexrays.mop_d:
-            # Using the micro-instruction EA differentiates identical loads that
-            # happen at different addresses, producing multiple leaves for the
-            # same logical value.  Instead we rely on the operand text (dstr),
-            # which is identical for identical expressions regardless of SSA
-            # copy location.
-            try:
-                return key + (mop.dstr(),)
-            except Exception:
-                # As a last resort fall back to EA.
-                return key + (mop.d.ea if mop.d else idaapi.BADADDR,)
-        case ida_hexrays.mop_S:
-            return key + (mop.s.off,)
-        case ida_hexrays.mop_v:
-            return key + (mop.g,)
-        case ida_hexrays.mop_l:
-            return key + (mop.l.idx, mop.l.off)
-        case ida_hexrays.mop_b:
-            return key + (mop.b,)
-        case ida_hexrays.mop_h:
-            return key + (mop.helper,)
-        case ida_hexrays.mop_str:
-            return key + (mop.cstr,)
-        case _:
-            # For other types, including complex ones like mop_f, mop_a, etc.,
-            # and mop_z, we fall back to the slower but safer dstr().
-            # This is a deliberate trade-off for robustness.
-            try:
-                return key + (mop.dstr(),)
-            except Exception:
-                # As a last resort, if dstr() fails, use a placeholder.
-                # This can happen for uninitialized or unusual mop_t instances.
-                logger.warning(
-                    "get_mop_key: Unsupported mop_t type: %s, returning placeholder",
-                    mop_type_to_string(t),
-                )
-                return key + (f"unsupported_mop_t_{t}",)
-
-
-def mop_to_ast_internal(
-    mop: ida_hexrays.mop_t, context: AstBuilderContext, root: bool = False
-) -> AstBase | None:
-    # Only log at root
-    if root and logger.debug_on:
-        logger.debug(
-            "[mop_to_ast_internal] Processing root mop: %s",
-            str(mop.dstr()) if hasattr(mop, "dstr") else str(mop),
-        )
-
-    # Early filter at root: only process if supported, with one exception:
-    # If the root is an m_call that has no argument list (r is mop_z) we treat it
-    # as transparent and attempt to build an AST from its destination operand.
-    if root:
-        if hasattr(mop, "d") and hasattr(mop.d, "opcode"):
-            root_opcode = mop.d.opcode
-
-            # Transparent helper call wrappers are now normalised by a
-            # peephole pass (TransparentCallUnwrapRule).  No special handling
-            # needed here anymore.
-
-            if root_opcode not in MBA_RELATED_OPCODES and not is_rotate_helper_call(
-                mop.d
-            ):
-                if logger.debug_on:
-                    logger.debug(
-                        "Skipping AST build for unsupported root opcode: %s",
-                        opcode_to_string(root_opcode),
-                    )
-                return None
-
-    # 1. Create the unique, hashable key for the current mop.
-    key = get_mop_key(mop)
-
-    # 2. Thread-local deduplication: if we've already built an AST for *this*
-    #    mop during the current recursive walk, return the existing instance to
-    #    avoid exponential explosion.
-    if key in context.mop_key_to_index:
-        existing_index = context.mop_key_to_index[key]
-        return context.unique_asts[existing_index]
-
-    # Rotate helper calls (__ROL*/__ROR*) are now inlined into plain shift/or
-    # instructions by RotateHelperInlineRule (peephole, MMAT_GLBOPT1).
-    # No special handling required here.
-
-    # Helper calls that evaluate to constants are now canonicalised by
-    # ConstantCallResultFoldRule (peephole GLBOPT1).
-
-    # NEW: Build AST nodes for MBA-related opcodes (binary or unary)
-    if mop.t == ida_hexrays.mop_d and mop.d.opcode in MBA_RELATED_OPCODES:
-        nb_ops = OPCODES_INFO[mop.d.opcode]["nb_operands"]
-
-        # Gather children ASTs based on operand count
-        left_ast = (
-            mop_to_ast_internal(mop.d.l, context) if mop.d.l is not None else None
-        )
-        right_ast = (
-            mop_to_ast_internal(mop.d.r, context)
-            if (nb_ops >= 2 and mop.d.r is not None)
-            else None
-        )
-
-        # Require at least the mandatory operands; if missing, fall back to leaf
-        if left_ast is None:
-            # Can't build meaningful node - fallback later to leaf
-            if logger.debug_on:
-                logger.debug(
-                    "[mop_to_ast_internal] Missing mandatory operand(s) for opcode %s, will treat as leaf",
-                    opcode_to_string(mop.d.opcode),
-                )
-        else:
-            # Only use dst_ast if destination present (ternary ops like m_stx etc.)
-            dst_ast = (
-                mop_to_ast_internal(mop.d.d, context) if mop.d.d is not None else None
-            )
-            tree = AstNode(mop.d.opcode, left_ast, right_ast, dst_ast)
-
-            # Set dest_size robustly
-            if hasattr(mop, "size") and mop.size:
-                tree.dest_size = mop.size
-            elif hasattr(mop.d, "size") and mop.d.size:
-                tree.dest_size = mop.d.size
-            elif mop.d.l is not None and hasattr(mop.d.l, "size"):
-                tree.dest_size = mop.d.l.size
-            else:
-                tree.dest_size = None
-
-            tree.mop = mop
-            tree.ea = sanitize_ea(mop.d.ea)
-
-            if logger.debug_on:
-                logger.debug(
-                    "[mop_to_ast_internal] Created AstNode for opcode %s (ea=0x%X): %s",
-                    opcode_to_string(mop.d.opcode),
-                    mop.d.ea if hasattr(mop.d, "ea") else -1,
-                    tree,
-                )
-            new_index = len(context.unique_asts)
-            tree.ast_index = new_index
-            context.unique_asts.append(tree)
-            context.mop_key_to_index[key] = new_index
-            return tree
-
-    # Special handling for mop_d that wraps an m_ldc as a constant leaf
-    if (
-        mop.t == ida_hexrays.mop_d
-        and mop.d is not None
-        and mop.d.opcode == ida_hexrays.m_ldc
-    ):
-        # Only treat it as constant if the *source* of the ldc is itself a
-        # numeric constant.  Otherwise we ignore the ldc wrapper and fall
-        # back to the generic leaf logic below.
-        ldc_src = mop.d.l
-        if ldc_src is not None and ldc_src.t == ida_hexrays.mop_n:
-            const_val = int(ldc_src.nnn.value)
-            const_size = ldc_src.size
-
-            const_leaf = AstConstant(hex(const_val), const_val, const_size)
-            # Clone numeric mop to detach from Hex-Rays internal storage
-            cloned_mop = ida_hexrays.mop_t()
-            cloned_mop.make_number(const_val, const_size)
-            const_leaf.mop = cloned_mop
-            const_leaf.dest_size = const_size
-
-            new_index = len(context.unique_asts)
-            const_leaf.ast_index = new_index
-            context.unique_asts.append(const_leaf)
-            context.mop_key_to_index[key] = new_index
-            return const_leaf
-
-    # Fallback for any unhandled mop: treat as a leaf.
-    # This is for simple operands (registers, stack vars) or complex
-    # instructions that are not part of our MBA analysis.
-    if (
-        mop.t != ida_hexrays.mop_d
-        or (mop.d.opcode not in MBA_RELATED_OPCODES)
-        or mop.d.l is None
-        or mop.d.r is None
-    ):
-        tree: AstBase | None
-        if mop.t == ida_hexrays.mop_n:
-            const_val = int(mop.nnn.value)
-            const_size = mop.size
-            tree = AstConstant(hex(const_val), const_val, const_size)
-            # Re-use a shared constant mop_t from the global cache to avoid the
-            # overhead of allocating a fresh object for every identical literal.
-            tree.mop = get_constant_mop(const_val, const_size)
-            tree.dest_size = const_size  # detached copy
-        # Typed-immediate wrappers (mop_f) are now normalised by the
-        # TypedImmediateCanonicaliseRule peephole pass.  If we still see one
-        # here it means it holds *no* literal value, therefore fall through to
-        # generic leaf creation.
-        elif mop.t == ida_hexrays.mop_f:
-            tree = None
-        else:
-            tree = None
-
-        # ------------------------------------------------------------------
-        # If we still haven't built a node, create a generic AstLeaf now.  This
-        # guarantees that *tree* is always defined even if new mop_t kinds are
-        # introduced in future IDA versions.
-        # ------------------------------------------------------------------
-        if tree is None:
-            tree = AstLeaf(format_mop_t(mop))
-            if logger.debug_on:
-                logger.debug(
-                    "[mop_to_ast_internal] Tree is NONE! Defaulting to AstLeaf for mop type %s dstr=%s",
-                    mop_type_to_string(mop.t),
-                    str(mop.dstr()) if hasattr(mop, "dstr") else str(mop),
-                )
-            tree.dest_size = mop.size
-
-        # For non-constant leaves we deliberately *do not* keep a reference
-        # to the original mop_t object, because Hex-Rays may free or reuse
-        # it after micro-optimisations, leading to use-after-free crashes.
-        # Only constant leaves benefit from holding the numeric mop to
-        # speed up further evaluations.
-        if tree.is_constant():
-            tree.mop = getattr(tree, "mop", None) or mop
-        else:
-            tree = AstLeaf(format_mop_t(mop))
-            if logger.debug_on:
-                logger.debug(
-                    "[mop_to_ast_internal] Fallback to AstLeaf for mop type %s dstr=%s",
-                    mop_type_to_string(mop.t),
-                    str(mop.dstr()) if hasattr(mop, "dstr") else str(mop),
-                )
-            tree.dest_size = mop.size
-
-        # Preserve previously assigned mop (e.g., inner numeric mop) unless
-        # it is still unset.  This prevents clobbering the constant `mop_n`
-        # we stored above with the wrapper operand, which would break
-        # constant detection later in the pipeline.
-        if getattr(tree, "mop", None) is None:
-            tree.mop = mop
-        dest_size = (
-            mop.size
-            if mop.t != ida_hexrays.mop_d
-            else mop.d.d.size if mop.d.d is not None else mop.size
-        )
-        tree.dest_size = dest_size
-        new_index = len(context.unique_asts)
-        tree.ast_index = new_index
-        context.unique_asts.append(tree)
-        context.mop_key_to_index[key] = new_index
-        return tree
-
-    # If we reach here, we failed to build an AST. Log the full mop tree.
-    logger.error("[mop_to_ast_internal] Could not build AST for mop. Dumping mop tree:")
-    mop_tree(mop)
-    return None
-
-
-def mop_to_ast(mop: ida_hexrays.mop_t) -> AstProxy | None:
-    """
-    Converts a mop_t to an AST node, with caching to avoid re-computation.
-
-    Returns a deep copy of the cached AST to prevent side-effects from
-    mutations by the caller.
-    """
-
-    # 1. Create a stable, hashable key from the mop_t object.
-    cache_key = get_mop_key(mop)
-
-    # 2. Global template cache: return a proxy if we already know the template
-    if cache_key in MOP_TO_AST_CACHE:
-        cached_template = MOP_TO_AST_CACHE[cache_key]
-        if cached_template is None:
-            return None  # Previously determined unconvertible.
-        return AstProxy(cached_template)
-
-    builder_context = AstBuilderContext()
-    # Start the optimized recursive build.
-
-    if not (mop_ast := mop_to_ast_internal(mop, builder_context, root=True)):
-        # Cache the failure to avoid re-computing it.
-        MOP_TO_AST_CACHE[cache_key] = None
-        return None
-
-    # This mutates the mop_ast object, populating its sub_ast_info.
-    # We do this ONCE before caching the "template" object, then we
-    # freeze the object to prevent mutations.
-    mop_ast.compute_sub_ast()
-    mop_ast.freeze()
-
-    # 4. Store the newly computed "template" object in the cache.
-    MOP_TO_AST_CACHE[cache_key] = mop_ast
-
-    # 5. Return a proxy to the caller for safety.
-    return AstProxy(mop_ast)
-
-
-def minsn_to_ast(instruction: ida_hexrays.minsn_t) -> AstProxy | None:
-    try:
-        # Early filter: forbidden opcodes
-        if instruction.opcode in MINSN_TO_AST_FORBIDDEN_OPCODES:
-            if logger.debug_on:
-                logger.debug(
-                    "Skipping AST build for forbidden opcode: %s @ 0x%x %s",
-                    opcode_to_string(instruction.opcode),
-                    instruction.ea,
-                    (
-                        "({0})".format(instruction.dstr())
-                        if instruction.opcode != ida_hexrays.m_jtbl
-                        else ""
-                    ),
-                )
-            return None
-
-        # Early filter: unsupported opcodes (not in MBA_RELATED_OPCODES)
-        # Allow rotate helper calls ("__ROL*" / "__ROR*") even though m_call
-        # is normally filtered out - they can be constant-folded later.
-        if instruction.opcode not in MBA_RELATED_OPCODES and not is_rotate_helper_call(
-            instruction
-        ):
-            if logger.debug_on:
-                logger.debug(
-                    "Skipping AST build for unsupported opcode: %s @ 0x%x %s",
-                    opcode_to_string(instruction.opcode),
-                    instruction.ea,
-                    (
-                        "({0})".format(instruction.dstr())
-                        if instruction.opcode != ida_hexrays.m_jtbl
-                        else ""
-                    ),
-                )
-            return None
-
-        # Constant-returning helper calls are folded to m_ldc by the peephole
-        # pass ConstantCallResultFoldRule.  No need for AST special case.
-
-        # Transparent-call shortcut: no args, computation stored in destination mop_d
-        if (
-            instruction.opcode == ida_hexrays.m_call
-            and (instruction.r is None or instruction.r.t == ida_hexrays.mop_z)
-            and instruction.d is not None
-            and instruction.d.t == ida_hexrays.mop_d
-        ):
-            if logger.debug_on:
-                logger.debug(
-                    "[minsn_to_ast] Unwrapping call with empty args; using destination expression for AST",
-                )
-            dest_ast = mop_to_ast(instruction.d)
-            if dest_ast is not None:
-                return dest_ast
-
-        ins_mop = ida_hexrays.mop_t()
-        ins_mop.create_from_insn(instruction)
-
-        # if instruction.opcode == ida_hexrays.m_mov:
-        #     tmp = AstNode(ida_hexrays.m_mov, mop_to_ast(ins_mop))
-        #     tmp.mop = ins_mop
-        #     tmp.dest_size = instruction.d.size
-        #     tmp.ea = instruction.ea
-        #     tmp.dst_mop = instruction.d
-        #     return tmp
-
-        tmp = mop_to_ast(ins_mop)
-        if tmp is None:
-            if logger.debug_on:
-                logger.debug(
-                    "Skipping AST build for unsupported or nop instruction: %s @ 0x%x %s",
-                    opcode_to_string(instruction.opcode),
-                    instruction.ea,
-                    (
-                        "({0})".format(instruction.dstr())
-                        if instruction.opcode != ida_hexrays.m_jtbl
-                        else ""
-                    ),
-                )
-        else:
-            tmp.dst_mop = instruction.d
-        return tmp
-    except RuntimeError as e:
-        logger.error(
-            "Error while transforming instruction %s: %s",
-            format_minsn_t(instruction),
-            e,
-        )
+from d810.cythxr.cymode import CythonMode
+
+if CythonMode().is_enabled():
+    from d810.expr._ast import (
+        AstBase,
+        AstConstant,
+        AstLeaf,
+        AstNode,
+        AstProxy,
+        minsn_to_ast,
+        mop_to_ast,
+    )
+else:
+    from d810.expr._slow_ast import (
+        AstBase,
+        AstConstant,
+        AstLeaf,
+        AstNode,
+        AstProxy,
+        minsn_to_ast,
+        mop_to_ast,
+    )
diff --git a/src/d810/expr/emulator.py b/src/d810/expr/emulator.py
index c3cbc68..9ccbe74 100644
--- a/src/d810/expr/emulator.py
+++ b/src/d810/expr/emulator.py
@@ -1,12 +1,16 @@
 from __future__ import annotations
 
 import dataclasses
+import functools
 import typing
 
 import ida_hexrays
 import idaapi
 
+import d810.expr.utils as utils
 from d810.conf.loggers import getLogger
+from d810.cythxr._chexrays_api import get_stack_or_reg_name
+from d810.cythxr._chexrays_api import hash_mop as cy_hash_mop
 from d810.errors import (
     EmulationException,
     EmulationIndirectJumpException,
@@ -18,7 +22,6 @@ from d810.expr.utils import (
     get_add_of,
     get_parity_flag,
     get_sub_of,
-    ror,
     signed_to_unsigned,
     unsigned_to_signed,
 )
@@ -40,11 +43,160 @@ from d810.hexrays.hexrays_helpers import (
 emulator_log = getLogger(__name__)
 
 
+# Extracted class
+class SyntheticCallReturnCache:
+    """
+    Provides stable, synthetic return values for calls based on (EA, dest_size).
+
+    Also tracks provenance: chained synthetic values can be traced back to their origin.
+    """
+
+    DEFAULT_TAG = 0xD810000000000000
+    DEFAULT_MASK = 0xFFFFFFFFFFFF
+
+    def __init__(self, tag: int = DEFAULT_TAG, mask: int = DEFAULT_MASK):
+        self.tag = tag
+        self.mask = mask
+        self._cache: dict[tuple[int, ...], int] = {}
+        # Track provenance: maps synthetic value -> origin synthetic value
+        self._provenance: dict[int, int] = {}
+
+    def _ensure_nonzero(self, value: int, dest_size: int) -> int:
+        """Ensure value is non-zero after masking to dest_size."""
+        mask = AND_TABLE.get(dest_size, AND_TABLE[8])
+        val = value & mask
+        if val == 0:
+            val = (value | 1) & mask
+            if val == 0:
+                val = 1  # final fallback
+        return val
+
+    def _get_or_create(
+        self,
+        key: tuple[int, ...],
+        dest_size: int,
+        base_value: int,
+        origin: int,
+    ) -> int:
+        """Common caching logic: check cache, compute if needed, track provenance."""
+        cached = self._cache.get(key)
+        if cached is not None:
+            return cached
+
+        val = self._ensure_nonzero(base_value, dest_size)
+        self._cache[key] = val
+        self._provenance[val] = origin
+        return val
+
+    @functools.singledispatchmethod
+    def get(self, ins: ida_hexrays.minsn_t) -> int:
+        """Return a stable, non-zero synthetic value for a call result.
+
+        Keyed by (EA, dest_size). Ensures non-zero after masking to dest size.
+        """
+        dest_size = getattr(ins.d, "size", 0) or 8
+        ea = getattr(ins, "ea", 0) or id(ins)
+        key = (ea, dest_size)
+
+        # Construct a high-tagged synthetic pointer-like value
+        base = self.tag ^ (ea & self.mask)
+        # Root synthetic values are their own origin (pass base as origin placeholder)
+        val = self._get_or_create(key, dest_size, base, origin=0)
+        # Update origin after creation for root values
+        if val not in self._provenance or self._provenance[val] == 0:
+            self._provenance[val] = val
+        return val
+
+    @get.register(ida_hexrays.mop_t)
+    def _(self, mop: ida_hexrays.mop_t) -> int:
+        """Return a stable, non-zero synthetic value for an unresolved mop.
+
+        Uses structural info when available to keep values stable across runs.
+        """
+        try:
+            if cy_hash_mop is not None:
+                h = int(cy_hash_mop(mop, 0)) & self.mask
+            else:
+                h = hash(format_mop_t(mop)) & self.mask
+        except Exception:
+            h = id(mop) & self.mask
+        dest_size = getattr(mop, "size", 0) or 8
+        key = (h, dest_size)
+        tag = self.tag
+        base = tag ^ h
+        val = self._get_or_create(key, dest_size, base, origin=0)
+        # Update origin after creation for root values
+        if val not in self._provenance or self._provenance[val] == 0:
+            self._provenance[val] = val
+        return val
+
+    def chain(self, ins: ida_hexrays.minsn_t, from_address: int) -> int:
+        """Return a cached, stable synthetic value derived from from_address.
+
+        This allows symbolic propagation through pointer chains while maintaining
+        provenance tracking back to the original synthetic value.
+
+        Args:
+            ins: The instruction (used for dest_size and optional EA)
+            from_address: The synthetic address being dereferenced
+
+        Returns:
+            A new synthetic value that is tracked and cacheable
+        """
+        dest_size = ins.d.size
+        key = (from_address, dest_size, getattr(ins, "ea", 0) or id(ins))
+
+        # Compute a new synthetic value based on the address + dest size
+        base = from_address ^ (dest_size << 48)
+        if (base & AND_TABLE[dest_size]) == 0:
+            base = (from_address & 0xFFFFFFFF) | self.tag
+
+        # Track provenance: find the origin of from_address and propagate it
+        origin = self._provenance.get(from_address, from_address)
+
+        val = self._get_or_create(key, dest_size, base, origin)
+
+        if emulator_log.debug_on:
+            emulator_log.debug(
+                "ldx %x (synthetic sentinel -> chained %x, origin %x)",
+                from_address,
+                val,
+                origin,
+            )
+        return val
+
+    def get_origin(self, synthetic_value: int) -> int | None:
+        """Return the original synthetic value that this value was derived from.
+
+        Args:
+            synthetic_value: A synthetic value (either root or chained)
+
+        Returns:
+            The original synthetic value, or None if not tracked
+        """
+        return self._provenance.get(synthetic_value)
+
+    def is_synthetic_address(self, addr: int) -> bool:
+        """
+        Returns True if 'addr' matches the synthetic call/TEB/PEB sentinel pattern.
+        this is used to avoid spurious MEMORY[0] issues, rather than erroring out.
+        """
+        # TODO: make the mask configurable or derived from the passed in tag
+        if (addr & 0xFFF0000000000000) == self.tag:
+            emulator_log.debug("ldx %x (synthetic sentinel, skip deref)", addr)
+            return True
+        return False
+
+
 class MicroCodeInterpreter(object):
-    def __init__(self, global_environment=None):
+    def __init__(self, global_environment=None, symbolic_mode=False):
         self.global_environment = (
             MicroCodeEnvironment() if global_environment is None else global_environment
         )
+        # Stable synthetic return values for calls (per EA, per size)
+        self.synthetic_call = SyntheticCallReturnCache()
+        # Enable symbolic fallback for unresolved variables
+        self.symbolic_mode: bool = symbolic_mode
 
     def _eval_instruction_and_update_environment(
         self,
@@ -392,87 +544,239 @@ class MicroCodeInterpreter(object):
         helper_name = ins.l.helper
         args_list = ins.d
 
-        emulator_log.debug("Call helper for {0}".format(helper_name))
-        # and we support only __ROR4__ (we should add other Hex-Rays created helper calls)
-        if helper_name == "__ROR4__":
+        if emulator_log.debug_on:
+            emulator_log.debug("Call helper for %s", helper_name)
+        # and we support only __RORX__/__ROLX__
+        if helper_name.startswith("__ROR") or helper_name.startswith("__ROL"):
+            shift_count = int(helper_name.split("__")[1])
             data_1 = self.eval(args_list.f.args[0], environment)
             data_2 = self.eval(args_list.f.args[1], environment)
-            return ror(data_1, data_2, 8 * args_list.f.args[0].size) & res_mask
-        elif helper_name == "__readfsqword":
-            return 0
+            if data_1 is None or data_2 is None:
+                if emulator_log.debug_on:
+                    emulator_log.debug(
+                        "Call helper for %s: data_1 (%s) or data_2 (%s) is None",
+                        helper_name,
+                        data_1,
+                        data_2,
+                    )
+                return None
+            helper_func = getattr(utils, f"{helper_name}{shift_count}__")
+            return helper_func(data_1, data_2) & res_mask
+        elif helper_name in ("__readfsqword", "__readgsqword"):
+            # These helpers read from FS/GS: they are known to be non-null in practice.
+            # Return a stable non-zero synthetic value to avoid null folding.
+            return self.synthetic_call.get(ins) & res_mask
         return None
 
     def _eval_load(
         self, ins: ida_hexrays.minsn_t, environment: MicroCodeEnvironment
     ) -> int | None:
         res_mask = AND_TABLE[ins.d.size]
-        if ins.opcode == ida_hexrays.m_ldx and environment.cur_blk is not None:
-            load_address = self.eval(ins.r, environment)
-            formatted_seg_register = format_mop_t(ins.l)
-            if formatted_seg_register == "ss.2":
-                stack_mop = ida_hexrays.mop_t()
-                stack_mop.erase()
-                stack_mop._make_stkvar(environment.cur_blk.mba, load_address)
+        if ins.opcode != ida_hexrays.m_ldx or environment.cur_blk is None:
+            return None
+
+        load_address = self.eval(ins.r, environment)
+        formatted_seg_register = format_mop_t(ins.l)
+        # formatted segment register: <mop_t type=mop_r size=2 dstr=ds.2>
+        if formatted_seg_register == "ss.2":
+            stack_mop = ida_hexrays.mop_t()
+            stack_mop.erase()
+            stack_mop._make_stkvar(environment.cur_blk.mba, load_address)
+            if emulator_log.debug_on:
                 emulator_log.debug(
                     "Searching for stack mop {0}".format(format_mop_t(stack_mop))
                 )
-                if (stack_mop_value := environment.lookup(stack_mop)) is None:
-                    raise EmulationException(
-                        "Variable '{0}' is not defined for mop_r or mop_S".format(
-                            format_mop_t(stack_mop)
+            if (
+                stack_mop_value := environment.lookup(
+                    stack_mop, raise_exception=not self.symbolic_mode
+                )
+            ) is None:
+                if self.symbolic_mode:
+                    stack_mop_value = self.synthetic_call.get(stack_mop)
+                    environment.define(stack_mop, stack_mop_value)
+                    if emulator_log.info_on:
+                        emulator_log.info(
+                            " synthetic stack mop {0} @ address {1:x} defined as: {2:x}".format(
+                                get_stack_or_reg_name(stack_mop),
+                                load_address,
+                                stack_mop_value,
+                            )
                         )
+                else:
+                    # Avoid dstr/format_mop_t in exception text (hot path)
+                    _name = get_stack_or_reg_name(stack_mop)
+                    raise EmulationException(
+                        "Variable {0} is not defined for mop_r or mop_S".format(_name)
                     )
+            if emulator_log.debug_on:
                 emulator_log.debug(
                     "  stack mop {0} value : {1}".format(
                         format_mop_t(stack_mop), stack_mop_value
                     )
                 )
-                return stack_mop_value & res_mask
-            else:
-                mem_seg = idaapi.getseg(load_address)
-                seg_perm = mem_seg.perm
-                if (seg_perm & idaapi.SEGPERM_WRITE) != 0:
-                    raise WritableMemoryReadException(
-                        "ldx {0:x} (writable -> return None)".format(load_address)
-                    )
-                else:
-                    memory_value = idaapi.get_qword(load_address)
-                    emulator_log.debug(
-                        "ldx {0:x} (non writable -> return {1:x})".format(
-                            load_address, memory_value & res_mask
+            return stack_mop_value & res_mask
+        else:
+            if emulator_log.debug_on:
+                # formatted segment register: <mop_t type=mop_r size=2 dstr=ds.2>
+                emulator_log.debug(
+                    "formatted segment register: {0}".format(formatted_seg_register)
+                )
+            mem_seg = idaapi.getseg(load_address)
+            if mem_seg is None:
+                # If this address looks like a synthetic call/TEB/PEB sentinel, treat as unknown
+                # to avoid spurious MEMORY[0] issues, rather than erroring out.
+                if self.synthetic_call.is_synthetic_address(load_address):
+                    # Return a cached, stable synthetic value based on the address + dest size
+                    # This allows symbolic propagation through pointer chains
+                    return self.synthetic_call.chain(ins, load_address)
+                # Treat null deref as unknown to avoid spurious MEMORY[0]
+                if load_address == 0:
+                    emulator_log.warning(
+                        "ldx 0 @ {0:x} (null deref, returning None)".format(
+                            load_address
                         )
                     )
-                    return memory_value & res_mask
+                    return None
+                raise EmulationException(
+                    "ldx {0:x} (no segment -> return None)".format(load_address)
+                )
+            seg_perm = mem_seg.perm
+            if (seg_perm & idaapi.SEGPERM_WRITE) != 0:
+                raise WritableMemoryReadException(
+                    "ldx {0:x} (writable -> return None)".format(load_address)
+                )
+            else:
+                memory_value = idaapi.get_qword(load_address)
+                if emulator_log.debug_on:
+                    emulator_log.debug(
+                        "ldx %x (non writable -> return %x)",
+                        load_address,
+                        memory_value & res_mask,
+                    )
+                return memory_value & res_mask
 
     def _eval_store(
         self, ins: ida_hexrays.minsn_t, environment: MicroCodeEnvironment
     ) -> int | None:
-        # TODO: implement
-        emulator_log.warning(
-            "Evaluation of %s not implemented: bypassing", format_minsn_t(ins)
-        )
-        return None
+        """Evaluate store to memory (stx) instruction.
+
+        Format: stx source, segment, address
+        - ins.l = value to store (source)
+        - ins.d = segment register (typically ds.2 or ss.2)
+        - ins.r = address to store to
+
+        For stack stores (ss.2), we convert the address to a stack variable and store it.
+        For other segments, we currently don't track memory writes (would need memory model).
+        """
+        res_mask = AND_TABLE[ins.l.size]
+        if ins.opcode != ida_hexrays.m_stx or environment.cur_blk is None:
+            return None
+
+        try:
+            # Evaluate the value to store
+            store_value = self.eval(ins.l, environment)
+
+            # Evaluate the address
+            store_address = self.eval(ins.r, environment)
+        except EmulationException as e:
+            # If we can't evaluate operands and symbolic mode is off, bypass
+            if not self.symbolic_mode:
+                emulator_log.warning("Can't evaluate stx operands: %s, bypassing", e)
+                return None
+            raise
+
+        # Get segment register (formatted like "ss.2" or "ds.2")
+        formatted_seg_register = format_mop_t(ins.d)
+
+        if formatted_seg_register == "ss.2":
+            # Stack store - create a stack mop and store the value in the environment
+            stack_mop = ida_hexrays.mop_t()
+            stack_mop.erase()
+            stack_mop._make_stkvar(environment.cur_blk.mba, store_address)
+
+            if emulator_log.debug_on:
+                emulator_log.debug(
+                    "Storing stack mop {0} @ address {1:x} with value: {2:x}".format(
+                        format_mop_t(stack_mop), store_address, store_value & res_mask
+                    )
+                )
+            environment.define(stack_mop, store_value & res_mask)
+        else:
+            # Non-stack memory write - we don't track writes to global memory
+            # but we shouldn't fail either (this is common in real code)
+            if emulator_log.debug_on:
+                emulator_log.debug(
+                    "Ignoring store to non-stack memory (segment: {0}) @ address {1:x}".format(
+                        formatted_seg_register, store_address
+                    )
+                )
+        return store_value & res_mask
 
     def _eval_call(
         self, ins: ida_hexrays.minsn_t, environment: MicroCodeEnvironment
     ) -> int | None:
-        # TODO: implement
+        # call   ld   l is mop_v or mop_b or mop_h
+        if ins.l.t in [ida_hexrays.mop_v, ida_hexrays.mop_b]:
+            # TODO: implement
+            emulator_log.warning(
+                "Evaluation of call with unsupported mop type %s (%s): bypassing",
+                mop_type_to_string(ins.l.t),
+                format_minsn_t(ins),
+            )
+            return None
+        # we only support ida_hexrays.mop_h for calls atm
+        res_mask = AND_TABLE[ins.d.size]
+        insn_helper: ida_hexrays.mop_t = ins.l
+        # extract helper name and width from helper string (e.g., __ROL4__)
+        helper_name = (insn_helper.helper or "").lstrip("!")
+        # Windows-specific helpers sometimes show up as named helpers (e.g., !NtCurrentPeb <fast:>)
+        hname = (helper_name or "").lstrip("!")
+        if hname.startswith("NtCurrentPeb"):
+            return self.synthetic_call.get(ins) & res_mask
+
         emulator_log.warning(
-            "Evaluation of %s not implemented: bypassing", format_minsn_t(ins)
+            "Evaluation of helper %s (%s) not implemented: bypassing",
+            helper_name,
+            format_minsn_t(ins),
         )
-        return None
+        return
 
     def eval(self, mop: ida_hexrays.mop_t, environment: MicroCodeEnvironment) -> int:
+        # Check for invalid mop sizes (e.g., function references have size=-1)
+        if mop.size < 0:
+            raise EmulationException(
+                "Cannot evaluate mop with invalid size ({0}): {1}".format(
+                    mop.size, mop_type_to_string(mop.t)
+                )
+            )
+
         if mop.t == ida_hexrays.mop_n:
             return mop.nnn.value
         elif mop.t in [ida_hexrays.mop_r, ida_hexrays.mop_S]:
-            if (value := environment.lookup(mop)) is not None:
+            if (
+                value := environment.lookup(mop, raise_exception=not self.symbolic_mode)
+            ) is not None:
                 return value
-            raise EmulationException(
-                "Variable '{0}' is not defined for mop_r or mop_S".format(
-                    format_mop_t(mop)
+            if self.symbolic_mode:
+                value = self.synthetic_call.get(mop)
+                environment.define(mop, value)
+                if emulator_log.info_on:
+                    emulator_log.info(
+                        " synthetic mop_r/mop_S {0} defined as: {1:x}".format(
+                            get_stack_or_reg_name(mop),
+                            value,
+                        )
+                    )
+                return value
+            else:
+                # Avoid dstr/format_mop_t in exception text (hot path)
+                _name = get_stack_or_reg_name(mop)
+                # Dump environment for debugging
+                if emulator_log.debug_on:
+                    environment.dump(f"Environment when looking up {_name}")
+                raise EmulationException(
+                    "Variable {0} is not defined for mop_r or mop_S".format(_name)
                 )
-            )
         elif mop.t == ida_hexrays.mop_d:
             res = self._eval_instruction(mop.d, environment)
             if res is None:
@@ -484,46 +788,54 @@ class MicroCodeInterpreter(object):
             return res
         elif mop.t == ida_hexrays.mop_a:
             if mop.a.t == ida_hexrays.mop_v:
-                emulator_log.debug(
-                    "Reading a mop_a '{0}' -> {1:x}".format(format_mop_t(mop), mop.a.g)
-                )
+                if emulator_log.debug_on:
+                    emulator_log.debug(
+                        "Reading a mop_a '%s' -> %x", format_mop_t(mop), mop.a.g
+                    )
                 return mop.a.g
             elif mop.a.t == ida_hexrays.mop_S:
-                emulator_log.debug(
-                    "Reading a mop_a '{0}' -> {1:x}".format(
-                        format_mop_t(mop), mop.a.s.off
+                if emulator_log.debug_on:
+                    emulator_log.debug(
+                        "Reading a mop_a '%s' -> %x", format_mop_t(mop), mop.a.s.off
                     )
-                )
                 return mop.a.s.off
+            # Keep message compact and avoid dstr
             raise UnresolvedMopException(
-                "Calling get_cst with unsupported mop type {0} - {1}: '{2}'".format(
-                    mop.t, mop.a.t, format_mop_t(mop)
+                "Calling get_cst with unsupported mop type {0} - {1}".format(
+                    mop_type_to_string(mop.t), mop_type_to_string(mop.a.t)
                 )
             )
         elif mop.t == ida_hexrays.mop_v:
             mem_seg = idaapi.getseg(mop.g)
+            if mem_seg is None:
+                raise EmulationException(
+                    "Reading a mop_v at 0x{0:X} (no segment)".format(mop.g)
+                )
             seg_perm = mem_seg.perm
             if (seg_perm & idaapi.SEGPERM_WRITE) != 0:
-                emulator_log.debug(
-                    "Reading a (writable) mop_v {0}".format(format_mop_t(mop))
-                )
+                if emulator_log.debug_on:
+                    emulator_log.debug(
+                        "Reading a (writable) mop_v %s", format_mop_t(mop)
+                    )
                 if (value := environment.lookup(mop)) is not None:
                     return value
                 raise EmulationException(
-                    "Variable '{0}' is not defined for mop_v".format(mop.dstr())
+                    "Variable for mop_v at 0x{0:X} (size={1}) is not defined".format(
+                        mop.g, mop.size
+                    )
                 )
             else:
                 memory_value = idaapi.get_qword(mop.g)
-                emulator_log.debug(
-                    "Reading a mop_v {0:x} (non writable -> return {1:x})".format(
-                        mop.g, memory_value
+                if emulator_log.debug_on:
+                    emulator_log.debug(
+                        "Reading a mop_v %x (non writable -> return %x)",
+                        mop.g,
+                        memory_value,
                     )
-                )
-                return mop.g
+                return memory_value & AND_TABLE[mop.size]
+        # Avoid dstr in exception text
         raise EmulationException(
-            "Unsupported mop type '{0}': '{1}'".format(
-                mop_type_to_string(mop.t), format_mop_t(mop)
-            )
+            "Unsupported mop type '{0}'".format(mop_type_to_string(mop.t))
         )
 
     def eval_instruction(
@@ -533,15 +845,15 @@ class MicroCodeInterpreter(object):
         environment: MicroCodeEnvironment | None = None,
         raise_exception: bool = False,
     ) -> bool:
+        if environment is None:
+            environment = self.global_environment
+        if ins is None:
+            return False
+        if emulator_log.debug_on:
+            emulator_log.debug(
+                "Evaluating microcode instruction : '%s'", format_minsn_t(ins)
+            )
         try:
-            if environment is None:
-                environment = self.global_environment
-            if ins is None:
-                return False
-            if emulator_log.debug_on:
-                emulator_log.debug(
-                    "Evaluating microcode instruction : '%s'", format_minsn_t(ins)
-                )
             self._eval_instruction_and_update_environment(blk, ins, environment)
             return True
         except EmulationException as e:
@@ -570,12 +882,17 @@ class MicroCodeInterpreter(object):
             res = self.eval(mop, environment)
             return res
         except EmulationException as e:
+            # Prefer canonical name for registers/stack vars; fall back to hash
+            name = get_stack_or_reg_name(mop)
             emulator_log.warning(
-                "Can't get constant mop value: '%s' for mop: '%s': %s",
-                mop.dstr(),
-                format_mop_t(mop),
+                "Can't get constant mop value: %s for mop '%s': %s",
+                name,
+                mop_type_to_string(mop.t),
                 e,
             )
+            # Dump environment for debugging
+            if emulator_log.debug_on and environment is not None:
+                environment.dump("Environment at lookup failure")
             if raise_exception:
                 raise e
             else:
@@ -699,11 +1016,12 @@ class MicroCodeEnvironment:
                     self.cur_blk.mba.get_mblock(self.cur_blk.serial + 1),
                 )
                 self.next_ins = typing.cast(ida_hexrays.minsn_t, self.next_blk.head)
-        emulator_log.debug(
-            "Setting next block {0} and next ins {1}".format(
-                self.next_blk.serial, format_minsn_t(self.next_ins)
+        if emulator_log.debug_on:
+            emulator_log.debug(
+                "Setting next block %d and next ins %s",
+                self.next_blk.serial,
+                format_minsn_t(self.next_ins),
             )
-        )
 
     def set_next_flow(
         self, next_blk: ida_hexrays.mblock_t, next_ins: ida_hexrays.minsn_t
@@ -742,8 +1060,11 @@ class MicroCodeEnvironment:
             self.define(searched_mop, new_mop_value)
             return new_mop_value
         if raise_exception:
+            _name = get_stack_or_reg_name(searched_mop)
             raise EmulationException(
-                "Variable '{0}' is not defined".format(format_mop_t(searched_mop))
+                "Variable {0} of type {1} is not defined".format(
+                    _name, mop_type_to_string(searched_mop.t)
+                )
             )
 
     def lookup(
@@ -770,3 +1091,14 @@ class MicroCodeEnvironment:
                 mop_type_to_string(mop.t), format_mop_t(mop)
             )
         )
+
+    def dump(self, header: str = "Environment dump"):
+        """Dump the current environment state for debugging."""
+        emulator_log.debug("=== %s ===", header)
+        emulator_log.debug("  mop_r records (%d):", len(self.mop_r_record))
+        for mop, value in self.mop_r_record.items():
+            emulator_log.debug("    %s = 0x%x", format_mop_t(mop), value)
+        emulator_log.debug("  mop_S records (%d):", len(self.mop_S_record))
+        for mop, value in self.mop_S_record.items():
+            emulator_log.debug("    %s = 0x%x", format_mop_t(mop), value)
+        emulator_log.debug("=== End %s ===", header)
diff --git a/src/d810/expr/utils.py b/src/d810/expr/utils.py
index 001a137..721fad0 100644
--- a/src/d810/expr/utils.py
+++ b/src/d810/expr/utils.py
@@ -2,18 +2,21 @@ import ctypes
 
 from d810.cache import CacheImpl
 from d810.hexrays.hexrays_helpers import MSB_TABLE
+from d810.registry import survives_reload
 
 CTYPE_SIGNED_TABLE = {
     1: ctypes.c_int8,
     2: ctypes.c_int16,
     4: ctypes.c_int32,
     8: ctypes.c_int64,
+    16: ctypes.c_ubyte * 16,
 }
 CTYPE_UNSIGNED_TABLE = {
     1: ctypes.c_uint8,
     2: ctypes.c_uint16,
     4: ctypes.c_uint32,
     8: ctypes.c_uint64,
+    16: ctypes.c_ubyte * 16,
 }
 
 
@@ -34,11 +37,38 @@ def get_all_subclasses(python_class):
 
 
 def unsigned_to_signed(unsigned_value, nb_bytes):
-    return CTYPE_SIGNED_TABLE[nb_bytes](unsigned_value).value
+    ctype_class = CTYPE_SIGNED_TABLE[nb_bytes]
+    if nb_bytes == 16:
+        # For 128-bit values, convert to bytes and back as signed
+        byte_array = ctype_class()
+        for i in range(16):
+            byte_array[i] = (unsigned_value >> (i * 8)) & 0xFF
+        # Convert back to int, treating as signed
+        result = 0
+        for i in range(16):
+            result |= byte_array[i] << (i * 8)
+        # Apply sign extension if MSB is set
+        if result & (1 << 127):
+            result |= ~((1 << 128) - 1)
+        return result
+    else:
+        return ctype_class(unsigned_value).value
 
 
 def signed_to_unsigned(signed_value, nb_bytes):
-    return CTYPE_UNSIGNED_TABLE[nb_bytes](signed_value).value
+    ctype_class = CTYPE_UNSIGNED_TABLE[nb_bytes]
+    if nb_bytes == 16:
+        # For 128-bit values, convert to bytes and back as unsigned
+        byte_array = ctype_class()
+        for i in range(16):
+            byte_array[i] = (signed_value >> (i * 8)) & 0xFF
+        # Convert back to int as unsigned
+        result = 0
+        for i in range(16):
+            result |= byte_array[i] << (i * 8)
+        return result & ((1 << 128) - 1)
+    else:
+        return ctype_class(signed_value).value
 
 
 def get_msb(value, nb_bytes):
@@ -66,7 +96,10 @@ def get_sub_of(op1, op2, nb_bytes):
 
 
 def get_parity_flag(op1, op2, nb_bytes):
-    tmp = CTYPE_UNSIGNED_TABLE[nb_bytes](op1 - op2).value
+    if nb_bytes == 16:
+        tmp = signed_to_unsigned(op1 - op2, nb_bytes)
+    else:
+        tmp = CTYPE_UNSIGNED_TABLE[nb_bytes](op1 - op2).value
     return (bin(tmp).count("1") + 1) % 2
 
 
@@ -129,14 +162,22 @@ def __ROR8__(value: int, count: int) -> int:
     return __ror__(value, count, 64)
 
 
-MOP_CONSTANT_CACHE = CacheImpl(
-    max_size=20480,
-    survive_reload=True,
-    reload_key="_SHARED_MOP_CONSTANT_CACHE",
-)
+@survives_reload(reload_key="_SHARED_MOP_CACHES")
+class _SharedMopCaches:
+    """
+    Holds the global mop caches and survives module reloads so every
+    importer (Python or Cython) sees the same instances.
+    """
 
-MOP_TO_AST_CACHE = CacheImpl(
-    max_size=20480,
-    survive_reload=True,
-    reload_key="_SHARED_MOP_TO_AST_CACHE",
-)
+    def __init__(self) -> None:
+        # Keep sizes reasonable; tweak as needed elsewhere.
+        self.MOP_CONSTANT_CACHE = CacheImpl(max_size=1000)
+        self.MOP_TO_AST_CACHE = CacheImpl(max_size=20480)
+
+
+_shared_caches = _SharedMopCaches()
+
+# Public module-level aliases used throughout the codebase (and Cython)
+MOP_CONSTANT_CACHE = _shared_caches.MOP_CONSTANT_CACHE
+
+MOP_TO_AST_CACHE = _shared_caches.MOP_TO_AST_CACHE
diff --git a/src/d810/expr/z3_utils.py b/src/d810/expr/z3_utils.py
index 38e3767..e7c1b0b 100644
--- a/src/d810/expr/z3_utils.py
+++ b/src/d810/expr/z3_utils.py
@@ -1,5 +1,6 @@
 import functools
 import typing
+from typing import Dict, Tuple
 
 import ida_hexrays
 
@@ -11,7 +12,7 @@ from d810.hexrays.hexrays_formatters import (
     format_mop_t,
     opcode_to_string,
 )
-from d810.hexrays.hexrays_helpers import get_mop_index
+from d810.hexrays.hexrays_helpers import get_mop_index, structural_mop_hash
 
 logger = getLogger(__name__)
 z3_file_logger = getLogger("D810.z3_test")
@@ -49,7 +50,18 @@ def requires_z3_installed(func: typing.Callable[..., typing.Any]):
 @requires_z3_installed
 @functools.lru_cache(maxsize=1)
 def get_solver() -> z3.Solver:
-    return z3.Solver()
+    s = z3.Solver()
+    # Bound solver work to prevent pathological slowdowns in hot paths.
+    # 50ms per query is generally enough for our simple equalities and keeps
+    # total time bounded in large functions.
+    try:
+        p = z3.ParamsRef()
+        p.set("timeout", 50)  # milliseconds
+        s.set(params=p)
+    except Exception:
+        # Older z3 versions or API quirks – ignore and keep default settings.
+        pass
+    return s
 
 
 @requires_z3_installed
@@ -205,11 +217,10 @@ def ast_to_z3_expression(ast: AstNode | AstLeaf | None, use_bitvecval=False):
                 )
             return extracted
         case _:
-            raise D810Z3Exception(
-                "Z3 evaluation: Unknown opcode {0} for {1}".format(
-                    opcode_to_string(ast.opcode), ast
-                )
-            )
+            # Gracefully fail on unknown opcode; avoid type issues in logging
+            op = getattr(ast, "opcode", None)
+            op_str = opcode_to_string(int(op)) if isinstance(op, int) else str(op)
+            raise D810Z3Exception(f"Z3 evaluation: Unknown opcode {op_str} for {ast}")
 
 
 @requires_z3_installed
@@ -234,6 +245,10 @@ def mop_list_to_z3_expression_list(mop_list: list[ida_hexrays.mop_t]):
     return [ast_to_z3_expression(ast) for ast in ast_list]
 
 
+# Module-level memoization for Z3 checks
+_Z3_EQ_CACHE: Dict[Tuple[Tuple[int, int, int|str], Tuple[int, int, int|str]], bool] = {}
+
+
 @requires_z3_installed
 def z3_check_mop_equality(
     mop1: ida_hexrays.mop_t | None,
@@ -259,16 +274,31 @@ def z3_check_mop_equality(
     # If quick checks didn't decide, fall back to Z3 even when types differ.
     if logger.debug_on:
         logger.debug(
-            "z3_check_mop_equality: mop1.t: %s, mop2.t: %s",
+            "z3_check_mop_equality: mop1: %s, mop2: %s",
             format_mop_t(mop1),
             format_mop_t(mop2),
         )
         logger.debug(
-            "z3_check_mop_equality: mop1.dstr(): %s, mop2.dstr(): %s",
+            "z3_check_mop_equality:\n\tmop1.dstr(): %s\n\tmop2.dstr(): %s\n\thashes: %016X vs %016X",
             mop1.dstr(),
             mop2.dstr(),
+            structural_mop_hash(mop1, 0),
+            structural_mop_hash(mop2, 0),
         )
-    # If pre-filters don't apply, fall back to Z3.
+    # If pre-filters don't apply, fall back to Z3 with a memoized check keyed by
+    # a cheap representation of the operands.
+    try:
+        k1 = (int(mop1.t), int(mop1.size), structural_mop_hash(mop1, 0))
+        k2 = (int(mop2.t), int(mop2.size), structural_mop_hash(mop2, 0))
+    except Exception:
+        k1 = (int(mop1.t), int(mop1.size), mop1.dstr())
+        k2 = (int(mop2.t), int(mop2.size), mop2.dstr())
+    if k2 < k1:
+        k1, k2 = k2, k1
+    cache_key = (k1, k2)
+    cached = _Z3_EQ_CACHE.get(cache_key)
+    if cached is not None:
+        return cached
     exprs = mop_list_to_z3_expression_list([mop1, mop2])
     if len(exprs) != 2:
         return False
@@ -277,17 +307,14 @@ def z3_check_mop_equality(
     _solver.push()
     _solver.add(z3.Not(z3_mop1 == z3_mop2))
     is_equal = _solver.check() == z3.unsat
-    if logger.debug_on:
-        logger.debug(
-            "z3_mop1: %s, z3_mop2: %s, z3_check_mop_equality: is_equal: %s",
-            z3_mop1,
-            z3_mop2,
-            is_equal,
-        )
     _solver.pop()
+    _Z3_EQ_CACHE[cache_key] = is_equal
     return is_equal
 
 
+_Z3_NEQ_CACHE: Dict[Tuple[Tuple[int, int, int|str], Tuple[int, int, int|str]], bool] = {}
+
+
 @requires_z3_installed
 def z3_check_mop_inequality(
     mop1: ida_hexrays.mop_t | None,
@@ -312,16 +339,33 @@ def z3_check_mop_inequality(
     # Otherwise fall back to Z3 (also handles differing types).
     if logger.debug_on:
         logger.debug(
-            "z3_check_mop_inequality: mop1.t: %s, mop2.t: %s",
+            "z3_check_mop_inequality: mop1: %s, mop2: %s",
             format_mop_t(mop1),
             format_mop_t(mop2),
         )
         logger.debug(
-            "z3_check_mop_inequality: mop1.dstr(): %s, mop2.dstr(): %s",
+            "z3_check_mop_inequality:\n\tmop1.dstr(): %s\n\tmop2.dstr(): %s\n\thashes: %016X vs %016X",
             mop1.dstr(),
             mop2.dstr(),
+            structural_mop_hash(mop1, 0),
+            structural_mop_hash(mop2, 0),
         )
-    # If pre-filters don't apply, fall back to Z3.
+    # If pre-filters don't apply, fall back to Z3 with a memoized check keyed by
+    # a cheap representation of the operands.
+    try:
+        k1 = (int(mop1.t), int(mop1.size), structural_mop_hash(mop1, 0))
+        k2 = (int(mop2.t), int(mop2.size), structural_mop_hash(mop2, 0))
+    except Exception:
+        k1 = (int(mop1.t), int(mop1.size), mop1.dstr())
+        k2 = (int(mop2.t), int(mop2.size), mop2.dstr())
+    if k2 < k1:
+        k1, k2 = k2, k1
+    if k2 < k1:
+        k1, k2 = k2, k1
+    cache_key = (k1, k2)
+    cached = _Z3_NEQ_CACHE.get(cache_key)
+    if cached is not None:
+        return cached
     exprs = mop_list_to_z3_expression_list([mop1, mop2])
     if len(exprs) != 2:
         return True
@@ -330,14 +374,8 @@ def z3_check_mop_inequality(
     _solver.push()
     _solver.add(z3_mop1 == z3_mop2)
     is_unequal = _solver.check() == z3.unsat
-    if logger.debug_on:
-        logger.debug(
-            "z3_check_mop_inequality: z3_mop1 ( %s ) != z3_mop2 ( %s ) ? is_unequal: %s",
-            z3_mop1,
-            z3_mop2,
-            is_unequal,
-        )
     _solver.pop()
+    _Z3_NEQ_CACHE[cache_key] = is_unequal
     return is_unequal
 
 
diff --git a/src/d810/hexrays/cfg_utils.py b/src/d810/hexrays/cfg_utils.py
index 4f9bcc9..e53e9d1 100644
--- a/src/d810/hexrays/cfg_utils.py
+++ b/src/d810/hexrays/cfg_utils.py
@@ -1,4 +1,6 @@
+import contextlib
 import functools
+import logging
 
 import ida_hexrays
 import idaapi
@@ -9,22 +11,23 @@ from d810.errors import ControlFlowException
 from d810.hexrays.hexrays_formatters import block_printer
 from d810.hexrays.hexrays_helpers import CONDITIONAL_JUMP_OPCODES
 
-helper_logger = getLogger("D810.helper")
+helper_logger = getLogger(__name__)
 
 
-def log_block_info(blk: mblock_t, logger_func=helper_logger.info):
+def log_block_info(blk: mblock_t, logger_func=helper_logger.info, ctx: str = ""):
     if blk is None:
         logger_func("Block is None")
         return
+    if ctx:
+        logger_func("%s", ctx)
     vp = block_printer()
     blk._print(vp)
     logger_func(
-        "Block {0} with successors {1} and predecessors {2}:\n{3}".format(
-            blk.serial,
-            [x for x in blk.succset],
-            [x for x in blk.predset],
-            vp.get_block_mc(),
-        )
+        "Block %s with successors %s and predecessors %s:\n%s",
+        blk.serial,
+        list(blk.succset),
+        list(blk.predset),
+        vp.get_block_mc(),
     )
 
 
@@ -40,6 +43,10 @@ def _get_mba_frame_size(mba: ida_hexrays.mba_t | None) -> int | None:
     return None
 
 
+# Optional second-level cache: one name per SSA *valnum* (fast path)
+_VALNUM_NAME_CACHE: dict[int, str] = {}
+
+
 @functools.lru_cache(maxsize=16384)
 def _cached_stack_var_name(
     mop_identity: int,  #  not used in the function but we need this bad boy for caching
@@ -49,12 +56,7 @@ def _cached_stack_var_name(
     valnum: int,
     frame_size: int | None,
 ) -> str:
-    """Compute & cache printable variable names (identity-based).
-
-    All arguments are immutable scalars, so the function is safe for
-    functools.lru_cache.  The *mop_identity* (id(mop)) ensures each concrete
-    mop_t instance gets its own entry even if scalar fields collide.
-    """
+    """Compute & cache printable variable names (identity-based)."""
     if t == ida_hexrays.mop_S:
         if frame_size is not None and frame_size >= reg_or_off:
             disp = frame_size - reg_or_off
@@ -67,14 +69,18 @@ def _cached_stack_var_name(
 
 
 def get_stack_var_name(mop: ida_hexrays.mop_t) -> str | None:
-    """Return a stable human-readable name for a stack/register operand.
+    """Return a stable human-readable name for *mop*.
 
     Fast path: lookup by ``mop.valnum`` in `_VALNUM_NAME_CACHE`.  Falls back to
     identity-based LRU cache on a miss.
     """
+    cached = _VALNUM_NAME_CACHE.get(mop.valnum)
+    if cached is not None:
+        return cached
+
     if mop.t == ida_hexrays.mop_S:
         frame_size = _get_mba_frame_size(getattr(mop.s, "mba", None))
-        return _cached_stack_var_name(
+        name = _cached_stack_var_name(
             id(mop), mop.t, mop.s.off, mop.size, mop.valnum, frame_size
         )
     elif mop.t == ida_hexrays.mop_r:
@@ -83,6 +89,9 @@ def get_stack_var_name(mop: ida_hexrays.mop_t) -> str | None:
         return None
     return name
 
+    _VALNUM_NAME_CACHE[mop.valnum] = name
+    return name
+
 
 def extract_base_and_offset(mop: ida_hexrays.mop_t) -> tuple[mop_t | None, int]:
     if (
@@ -108,14 +117,24 @@ def safe_verify(
         mba.verify(True)
     except RuntimeError as e:
         logger_func("verify failed after %s: %s", ctx, e, exc_info=True)
-        # attempt to locate a problematic block: dump the last one
-        try:
-            last_blk = (
-                mba.get_mblock(mba.qty - 2) if mba.qty >= 2 else mba.get_mblock(0)
-            )
-            log_block_info(last_blk, logger_func)
-        except Exception:  # pragma: no cover
-            pass
+        # attempt to locate problematic blocks: dump the last two blocks if possible
+        with contextlib.suppress(Exception):
+            divider = "-" * 14
+            if (num_blocks := mba.qty) != 0:
+                if num_blocks >= 2:
+                    log_block_info(
+                        mba.get_mblock(num_blocks - 2),
+                        logger_func,
+                        f"{divider}[blk -2]{divider}",
+                    )
+                    log_block_info(
+                        mba.get_mblock(num_blocks - 1),
+                        logger_func,
+                        f"{divider}[blk -1]{divider}",
+                    )
+                log_block_info(
+                    mba.get_mblock(0), logger_func, f"{divider}[blk 0]{divider}"
+                )
         raise
 
 
@@ -332,7 +351,7 @@ def update_blk_successor(
     if blk.nsucc() == 1:
         change_1way_block_successor(blk, new_successor_serial)
     elif blk.nsucc() == 2:
-        if old_successor_serial == blk.serial + 1:
+        if old_successor_serial == blk.nextb.serial:
             helper_logger.info(
                 "Can't update direct block successor: {0} - {1} - {2}".format(
                     blk.serial, old_successor_serial, new_successor_serial
@@ -452,7 +471,7 @@ def update_block_successors(blk: mblock_t, blk_succ_serial_list: list[int]):
 
 def insert_nop_blk(blk: mblock_t) -> mblock_t:
     mba = blk.mba
-    nop_block = mba.copy_block(blk, blk.serial + 1)
+    nop_block = mba.copy_block(blk, blk.nextb.serial)
     cur_ins = nop_block.head
     if cur_ins == None:
         cur_inst = minsn_t(blk.start)
diff --git a/src/d810/hexrays/hexrays_formatters.py b/src/d810/hexrays/hexrays_formatters.py
index 17d510b..f64a733 100644
--- a/src/d810/hexrays/hexrays_formatters.py
+++ b/src/d810/hexrays/hexrays_formatters.py
@@ -14,7 +14,7 @@ from d810.hexrays.hexrays_helpers import (
     STRING_TO_MATURITY_DICT,
 )
 
-logger = getLogger("D810.helper")
+logger = getLogger(__name__)
 
 _trans_table = str.maketrans(
     "", "", "".join(chr(i) for i in range(256) if not (0x20 <= i <= 0x7E))
diff --git a/src/d810/hexrays/hexrays_helpers.py b/src/d810/hexrays/hexrays_helpers.py
index f73ab1f..ea86d5e 100644
--- a/src/d810/hexrays/hexrays_helpers.py
+++ b/src/d810/hexrays/hexrays_helpers.py
@@ -36,6 +36,7 @@ from ida_hexrays import (
 )
 
 from d810.conf.loggers import getLogger
+from d810.cythxr._chexrays_api import hash_mop as cy_hash_mop
 
 logger = getLogger(__name__)
 
@@ -166,7 +167,7 @@ OPCODES_INFO = {
 }
 
 
-MATURITY_TO_STRING_DICT = {
+MATURITY_TO_STRING_DICT: dict[int, str] = {
     MMAT_ZERO: "MMAT_ZERO",
     MMAT_GENERATED: "MMAT_GENERATED",
     MMAT_PREOPTIMIZED: "MMAT_PREOPTIMIZED",
@@ -177,9 +178,11 @@ MATURITY_TO_STRING_DICT = {
     MMAT_GLBOPT3: "MMAT_GLBOPT3",
     MMAT_LVARS: "MMAT_LVARS",
 }
-STRING_TO_MATURITY_DICT = {v: k for k, v in MATURITY_TO_STRING_DICT.items()}
+STRING_TO_MATURITY_DICT: dict[str, int] = {
+    v: k for k, v in MATURITY_TO_STRING_DICT.items()
+}
 
-MOP_TYPE_TO_STRING_DICT = {
+MOP_TYPE_TO_STRING_DICT: dict[int, str] = {
     mop_z: "mop_z",
     mop_r: "mop_r",
     mop_n: "mop_n",
@@ -198,12 +201,30 @@ MOP_TYPE_TO_STRING_DICT = {
     mop_sc: "mop_sc",
 }
 
-Z3_SPECIAL_OPERANDS = ["UDiv", "URem", "LShR", "UGT", "UGE", "ULT", "ULE"]
+Z3_SPECIAL_OPERANDS: list[str] = ["UDiv", "URem", "LShR", "UGT", "UGE", "ULT", "ULE"]
 
-BOOLEAN_OPCODES = [m_lnot, m_bnot, m_or, m_and, m_xor]
-ARITHMETICAL_OPCODES = [m_neg, m_add, m_sub, m_mul, m_udiv, m_sdiv, m_umod, m_smod]
-BIT_OPERATIONS_OPCODES = [m_shl, m_shr, m_sar, m_mov, m_xds, m_xdu, m_low, m_high]
-CHECK_OPCODES = [
+BOOLEAN_OPCODES: list[int] = [m_lnot, m_bnot, m_or, m_and, m_xor]
+ARITHMETICAL_OPCODES: list[int] = [
+    m_neg,
+    m_add,
+    m_sub,
+    m_mul,
+    m_udiv,
+    m_sdiv,
+    m_umod,
+    m_smod,
+]
+BIT_OPERATIONS_OPCODES: list[int] = [
+    m_shl,
+    m_shr,
+    m_sar,
+    m_mov,
+    m_xds,
+    m_xdu,
+    m_low,
+    m_high,
+]
+CHECK_OPCODES: list[int] = [
     m_sets,
     m_seto,
     m_setp,
@@ -219,11 +240,11 @@ CHECK_OPCODES = [
     m_setle,
 ]
 
-MBA_RELATED_OPCODES = (
+MBA_RELATED_OPCODES: list[int] = (
     BOOLEAN_OPCODES + ARITHMETICAL_OPCODES + BIT_OPERATIONS_OPCODES + CHECK_OPCODES
 )
 
-CONDITIONAL_JUMP_OPCODES = [
+CONDITIONAL_JUMP_OPCODES: list[int] = [
     m_jcnd,
     m_jnz,
     m_jz,
@@ -237,10 +258,10 @@ CONDITIONAL_JUMP_OPCODES = [
     m_jle,
     m_jtbl,
 ]
-UNCONDITIONAL_JUMP_OPCODES = [m_goto, m_ijmp]
-CONTROL_FLOW_OPCODES = CONDITIONAL_JUMP_OPCODES + UNCONDITIONAL_JUMP_OPCODES
+UNCONDITIONAL_JUMP_OPCODES: list[int] = [m_goto, m_ijmp]
+CONTROL_FLOW_OPCODES: list[int] = CONDITIONAL_JUMP_OPCODES + UNCONDITIONAL_JUMP_OPCODES
 
-MINSN_TO_AST_FORBIDDEN_OPCODES = CONTROL_FLOW_OPCODES + [
+MINSN_TO_AST_FORBIDDEN_OPCODES: list[int] = CONTROL_FLOW_OPCODES + [
     m_ret,
     m_nop,
     m_stx,
@@ -250,7 +271,7 @@ MINSN_TO_AST_FORBIDDEN_OPCODES = CONTROL_FLOW_OPCODES + [
     m_ext,
 ]
 
-SUB_TABLE = {
+SUB_TABLE: dict[int, int] = {
     1: 0x100,
     2: 0x10000,
     4: 0x100000000,
@@ -259,14 +280,14 @@ SUB_TABLE = {
 }
 # The AND_TABLE is an all-ones mask (equivalent to -1 in two's complement).
 # XORing with an all-ones mask is the same as a bitwise NOT (~).
-AND_TABLE = {
+AND_TABLE: dict[int, int] = {
     1: 0xFF,
     2: 0xFFFF,
     4: 0xFFFFFFFF,
     8: 0xFFFFFFFFFFFFFFFF,
     16: 0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF,
 }
-MSB_TABLE = {
+MSB_TABLE: dict[int, int] = {
     1: 0x80,
     2: 0x8000,
     4: 0x80000000,
@@ -276,6 +297,107 @@ MSB_TABLE = {
 
 
 # Hex-Rays mop equality checking
+_EQUAL_BNOT_CACHE: dict[tuple[int, int], bool] = {}
+_EQUAL_BNOT_MAX = 8192
+_EQUAL_IGN_CACHE: dict[tuple[int, int], bool] = {}
+_EQUAL_IGN_MAX = 8192
+
+
+def _mop_cache_key(op: mop_t) -> str:
+    """Stable, cheap key for caching equality checks without using dstr()."""
+    t = op.t
+    sz = op.size
+    # Constants: include value
+    if t == mop_n:
+        try:
+            return f"n:{sz}:{op.nnn.value}"
+        except Exception:
+            return f"n:{sz}:?"
+    # Global address
+    if t == mop_v:
+        try:
+            return f"v:{sz}:{op.g}"
+        except Exception:
+            return f"v:{sz}:?"
+    # Symbolic reference; prefer start_ea when available (exclude stkvars)
+    if t == mop_S:
+        start_ea = getattr(op.s, "start_ea", None)
+        if start_ea is not None:
+            return f"S:{sz}:{start_ea}"
+        off = getattr(op.s, "off", -1)
+        return f"Sstk:{sz}:{off}"
+    # Nested instruction: use opcode + identity of the inner instruction
+    if t == mop_d and op.d is not None:
+        try:
+            return f"d:{sz}:{op.d.opcode}:{id(op.d)}"
+        except Exception:
+            return f"d:{sz}:{id(op)}"
+    # Register
+    if t == mop_r:
+        return f"r:{sz}:{getattr(op, 'r', '?')}"
+    # Memory b form: capture base type; avoid non-existent fields access
+    if t == mop_b:
+        bt = getattr(op.b, "t", -1)
+        return f"b:{sz}:{bt}:{id(op)}"
+    # Pair
+    if t == mop_p:
+        return f"p:{sz}:{id(op)}"
+    # Fallback to type+size+identity
+    return f"{t}:{sz}:{id(op)}"
+
+
+def mop_quick_key_ignore_size(op: mop_t) -> str:
+    """Cheap signature for grouping operands under ignore-size equality.
+
+    This intentionally ignores the operand size and uses structural fields that
+    are compared in equal_mops_ignore_size. It is not a perfect hash: keys may
+    collide across non-equal operands, so callers must still verify equality
+    within a bucket using equal_mops_ignore_size.
+    """
+    t = op.t
+    if t == mop_n:
+        try:
+            return f"n:{op.nnn.value}"
+        except Exception:
+            return "n:?"
+    if t == mop_v:
+        try:
+            return f"v:{op.g}"
+        except Exception:
+            return "v:?"
+    if t == mop_S:
+        start_ea = getattr(op.s, "start_ea", None)
+        if start_ea is not None:
+            return f"S:{start_ea}"
+        off = getattr(op.s, "off", -1)
+        return f"Soff:{off}"
+    if t == mop_r:
+        return f"r:{getattr(op, 'r', '?')}"
+    if t == mop_b:
+        bt = getattr(op.b, "t", -1)
+        return f"b:{bt}"
+    if t == mop_d and op.d is not None:
+        # Group by opcode only; detailed check is done later
+        return f"d:{op.d.opcode}"
+    if t == mop_p:
+        return "p"
+    return f"t:{t}"
+
+
+def structural_mop_hash(op: mop_t, func_entry_ea: int = 0) -> int:
+    """Use Cython fast hasher if available; fallback to Python quick key.
+
+    This returns a 64-bit int when Cython is present; otherwise a Python hash
+    of the quick key which is still cheap and avoids dstr().
+    """
+    if cy_hash_mop is not None:
+        try:
+            return int(cy_hash_mop(op, func_entry_ea))
+        except Exception:
+            pass
+    return hash(mop_quick_key_ignore_size(op))
+
+
 def equal_bnot_cst(lo: mop_t, ro: mop_t, mop_size=None) -> bool:
     if (lo.t != mop_n) or (ro.t != mop_n):
         return False
@@ -287,28 +409,43 @@ def equal_bnot_cst(lo: mop_t, ro: mop_t, mop_size=None) -> bool:
 
 
 def equal_bnot_mop(lo: mop_t, ro: mop_t, test_two_sides=True) -> bool:
+    # Try cache first (symmetry-aware)
+    try:
+        h1 = int(structural_mop_hash(lo, 0))
+        h2 = int(structural_mop_hash(ro, 0))
+        key = (h1, h2) if h1 <= h2 else (h2, h1)
+    except Exception:
+        key = (id(lo), id(ro)) if id(lo) <= id(ro) else (id(ro), id(lo))
+    cached = _EQUAL_BNOT_CACHE.get(key)
+    if cached is not None:
+        return cached
+
+    result = False
     if lo.t == mop_n:
-        return equal_bnot_cst(lo, ro)
+        result = equal_bnot_cst(lo, ro)
+    else:
+        # Direct ~x pattern
+        if (lo.t == mop_d) and lo.d.opcode == m_bnot:
+            if equal_mops_ignore_size(lo.d.l, ro):
+                result = True
+        # Hex-Rays: ~(-x) == x - 1
+        if not result and (lo.t == mop_d) and lo.d.opcode == m_neg:
+            if (ro.t == mop_d) and ro.d.opcode == m_sub:
+                if ro.d.r.t == mop_n and ro.d.r.nnn.value == 1:
+                    if equal_mops_ignore_size(ro.d.l, lo.d.l):
+                        result = True
+        # Unsigned extend wrapper
+        if not result and (lo.t == mop_d) and lo.d.opcode == m_xds:
+            if equal_bnot_mop(lo.d.l, ro):
+                result = True
+        # Symmetry
+        if not result and test_two_sides:
+            result = equal_bnot_mop(ro, lo, test_two_sides=False)
 
-    # We first check for a bnot operand
-    if (lo.t == mop_d) and lo.d.opcode == m_bnot:
-        if equal_mops_ignore_size(lo.d.l, ro):
-            return True
-
-    # Otherwise Hexrays may have optimized using ~(-x) = x - 1
-    if (lo.t == mop_d) and lo.d.opcode == m_neg:
-        if (ro.t == mop_d) and ro.d.opcode == m_sub:
-            if ro.d.r.t == mop_n and ro.d.r.nnn.value == 1:
-                if equal_mops_ignore_size(ro.d.l, lo.d.l):
-                    return True
-
-    if (lo.t == mop_d) and lo.d.opcode == m_xds:
-        if equal_bnot_mop(lo.d.l, ro):
-            return True
-
-    if test_two_sides:
-        return equal_bnot_mop(ro, lo, test_two_sides=False)
-    return False
+    if len(_EQUAL_BNOT_CACHE) > _EQUAL_BNOT_MAX:
+        _EQUAL_BNOT_CACHE.clear()
+    _EQUAL_BNOT_CACHE[key] = result
+    return result
 
 
 def equal_ignore_msb_cst(lo: mop_t, ro: mop_t) -> bool:
@@ -333,54 +470,80 @@ def equal_mops_bypass_xdu(lo: mop_t, ro: mop_t) -> bool:
 def equal_mops_ignore_size(lo: mop_t, ro: mop_t) -> bool:
     if (lo is None) or (ro is None):
         return False
+    # Exact same SWIG object → equal
+    if lo is ro:
+        return True
+    # Cheap type check first
     if lo.t != ro.t:
         return False
+    # Symmetry-aware bounded cache using structural hash (fast path)
+    try:
+        h1 = int(structural_mop_hash(lo, 0))
+        h2 = int(structural_mop_hash(ro, 0))
+        key = (h1, h2) if h1 <= h2 else (h2, h1)
+        cached = _EQUAL_IGN_CACHE.get(key)
+        if cached is not None:
+            return cached
+    except Exception:
+        key = None  # fallback
     if lo.t == mop_z:
-        return True
+        result = True
     elif lo.t == mop_fn:
-        return lo.fpc == ro.fpc
+        result = lo.fpc == ro.fpc
     elif lo.t == mop_n:
-        return lo.nnn.value == ro.nnn.value
+        result = lo.nnn.value == ro.nnn.value
     elif lo.t == mop_S:
         if lo.s == ro.s:
-            return True
-        if lo.s.off == ro.s.off:
-            # Is it right?
-            return True
-        return False
+            result = True
+        elif lo.s.off == ro.s.off:
+            result = True
+        else:
+            result = False
     elif lo.t == mop_v:
-        return lo.g == ro.g
+        result = lo.g == ro.g
     elif lo.t == mop_d:
-        return lo.d.equal_insns(ro.d, EQ_IGNSIZE)
+        result = lo.d.equal_insns(ro.d, EQ_IGNSIZE)
         # return lo.d.equal_insns(ro.d, EQ_IGNSIZE | EQ_IGNCODE)
     elif lo.t == mop_b:
-        return lo.b == ro.b
+        result = lo.b == ro.b
     elif lo.t == mop_r:
-        return lo.r == ro.r
+        result = lo.r == ro.r
     elif lo.t == mop_f:
-        return False
+        result = False
     elif lo.t == mop_l:
-        return lo.l == ro.l
+        result = lo.l == ro.l
     elif lo.t == mop_a:
         if lo.a.insize != ro.a.insize:
-            return False
-        if lo.a.outsize != ro.a.outsize:
-            return False
-        return equal_mops_ignore_size(lo.a, ro.a)
+            result = False
+        elif lo.a.outsize != ro.a.outsize:
+            result = False
+        else:
+            result = equal_mops_ignore_size(lo.a, ro.a)
+        if key is not None:
+            if len(_EQUAL_IGN_CACHE) > _EQUAL_IGN_MAX:
+                _EQUAL_IGN_CACHE.clear()
+            _EQUAL_IGN_CACHE[key] = result
+        return result
     elif lo.t == mop_h:
-        return ro.helper == lo.helper
+        result = ro.helper == lo.helper
     elif lo.t == mop_str:
-        return ro.cstr == lo.cstr
+        result = ro.cstr == lo.cstr
     elif lo.t == mop_c:
-        return ro.c == lo.c
+        result = ro.c == lo.c
     elif lo.t == mop_p:
-        return equal_mops_ignore_size(
+        result = equal_mops_ignore_size(
             lo.pair.lop, ro.pair.lop
         ) and equal_mops_ignore_size(lo.pair.hop, ro.pair.hop)
     elif lo.t == mop_sc:
-        return False
+        result = False
     else:
-        return False
+        result = False
+
+    if key is not None:
+        if len(_EQUAL_IGN_CACHE) > _EQUAL_IGN_MAX:
+            _EQUAL_IGN_CACHE.clear()
+        _EQUAL_IGN_CACHE[key] = result
+    return result
 
 
 def is_check_mop(lo: mop_t) -> bool:
diff --git a/src/d810/hexrays/hexrays_hooks.py b/src/d810/hexrays/hexrays_hooks.py
index 56fd001..48c9986 100644
--- a/src/d810/hexrays/hexrays_hooks.py
+++ b/src/d810/hexrays/hexrays_hooks.py
@@ -4,7 +4,6 @@ import enum
 import pathlib
 import typing
 
-import ida_hexrays
 from ida_hexrays import *
 
 from d810.conf.loggers import getLogger
@@ -25,7 +24,6 @@ from d810.optimizers.microcode.instructions.handler import (
 
 main_logger = getLogger("D810")
 optimizer_logger = getLogger("D810.optimizer")
-helper_logger = getLogger("D810.helper")
 
 DEFAULT_OPTIMIZATION_PATTERN_MATURITIES = [
     MMAT_PREOPTIMIZED,
@@ -63,13 +61,15 @@ if typing.TYPE_CHECKING:
         PeepholeOptimizer,
     )
     from d810.optimizers.microcode.instructions.z3.handler import Z3Optimizer
+    from d810.stats import OptimizationStatistics
 
 
 class InstructionOptimizerManager(optinsn_t):
-    def __init__(self, log_dir: pathlib.Path):
+    def __init__(self, stats: OptimizationStatistics, log_dir: pathlib.Path):
         optimizer_logger.debug("Initializing {0}...".format(self.__class__.__name__))
         super().__init__()
         self.log_dir = log_dir
+        self.stats = stats
         self.instruction_visitor = InstructionVisitorManager(self)
         self._last_optimizer_tried = None
         self.current_maturity = None
@@ -78,7 +78,7 @@ class InstructionOptimizerManager(optinsn_t):
         self.dump_intermediate_microcode = False
 
         self.instruction_optimizers = []
-        self.optimizer_usage_info = {}
+        # usage tracking moved to centralized statistics object
         ChainOptimizer: type[ChainOptimizer] = InstructionOptimizer.get(
             "ChainOptimizer"
         )
@@ -97,30 +97,47 @@ class InstructionOptimizerManager(optinsn_t):
         Z3Optimizer: type[Z3Optimizer] = InstructionOptimizer.get("Z3Optimizer")
         self.add_optimizer(
             PatternOptimizer(
-                DEFAULT_OPTIMIZATION_PATTERN_MATURITIES, log_dir=self.log_dir
+                DEFAULT_OPTIMIZATION_PATTERN_MATURITIES,
+                stats=self.stats,
+                log_dir=self.log_dir,
             )
         )
         self.add_optimizer(
-            ChainOptimizer(DEFAULT_OPTIMIZATION_CHAIN_MATURITIES, log_dir=self.log_dir)
+            ChainOptimizer(
+                DEFAULT_OPTIMIZATION_CHAIN_MATURITIES,
+                stats=self.stats,
+                log_dir=self.log_dir,
+            )
         )
         self.add_optimizer(
-            Z3Optimizer(DEFAULT_OPTIMIZATION_Z3_MATURITIES, log_dir=self.log_dir)
+            Z3Optimizer(
+                DEFAULT_OPTIMIZATION_Z3_MATURITIES,
+                stats=self.stats,
+                log_dir=self.log_dir,
+            )
         )
         self.add_optimizer(
-            EarlyOptimizer(DEFAULT_OPTIMIZATION_EARLY_MATURITIES, log_dir=self.log_dir)
+            EarlyOptimizer(
+                DEFAULT_OPTIMIZATION_EARLY_MATURITIES,
+                stats=self.stats,
+                log_dir=self.log_dir,
+            )
         )
         self.add_optimizer(
             PeepholeOptimizer(
-                DEFAULT_OPTIMIZATION_PEEPHOLE_MATURITIES, log_dir=self.log_dir
+                DEFAULT_OPTIMIZATION_PEEPHOLE_MATURITIES,
+                stats=self.stats,
+                log_dir=self.log_dir,
             )
         )
         self.analyzer = InstructionAnalyzer(
-            DEFAULT_ANALYZER_MATURITIES, log_dir=self.log_dir
+            DEFAULT_ANALYZER_MATURITIES,
+            stats=self.stats,
+            log_dir=self.log_dir,
         )
 
     def add_optimizer(self, optimizer: InstructionOptimizer):
         self.instruction_optimizers.append(optimizer)
-        self.optimizer_usage_info[optimizer.name] = 0
 
     def add_rule(self, rule: InstructionOptimizationRule):
         # optimizer_log.info("Trying to add rule {0}".format(rule))
@@ -160,22 +177,7 @@ class InstructionOptimizerManager(optinsn_t):
             )
         return False
 
-    def reset_rule_usage_statistic(self):
-        self.optimizer_usage_info = {}
-        for ins_optimizer in self.instruction_optimizers:
-            self.optimizer_usage_info[ins_optimizer.name] = 0
-            ins_optimizer.reset_rule_usage_statistic()
-
-    def show_rule_usage_statistic(self):
-        for optimizer_name, optimizer_nb_match in self.optimizer_usage_info.items():
-            if optimizer_nb_match > 0:
-                main_logger.info(
-                    "Instruction optimizer '%s' has been used %d times",
-                    optimizer_name,
-                    optimizer_nb_match,
-                )
-        for ins_optimizer in self.instruction_optimizers:
-            ins_optimizer.show_rule_usage_statistic()
+    # statistics are managed centrally via the stats object
 
     def log_info_on_input(self, blk: mblock_t, ins: minsn_t):
         mba: mbl_array_t = blk.mba
@@ -234,7 +236,9 @@ class InstructionOptimizerManager(optinsn_t):
                         )
                 else:
                     ins.swap(new_ins)
-                    self.optimizer_usage_info[ins_optimizer.name] += 1
+                    if self.stats is not None:
+                        self.stats.record_optimizer_match(ins_optimizer.name)
+
                     if self.generate_z3_code:
                         try:
                             log_z3_instructions(new_ins, ins)
@@ -257,14 +261,15 @@ class InstructionVisitorManager(minsn_visitor_t):
 
 
 class BlockOptimizerManager(optblock_t):
-    def __init__(self, log_dir: pathlib.Path):
+    def __init__(self, stats: OptimizationStatistics, log_dir: pathlib.Path):
         optimizer_logger.debug("Initializing {0}...".format(self.__class__.__name__))
         super().__init__()
         self.log_dir = log_dir
+        self.stats = stats
         self.cfg_rules = set()
 
         self.current_maturity = None
-        self.cfg_rules_usage_info = {}
+        # usage tracking moved to centralized statistics object
 
     def func(self, blk: mblock_t):
         self.log_info_on_input(blk)
@@ -283,21 +288,7 @@ class BlockOptimizerManager(optblock_t):
 
             self.current_maturity = mba.maturity
 
-    def reset_rule_usage_statistic(self):
-        self.cfg_rules_usage_info = {}
-        for rule in self.cfg_rules:
-            self.cfg_rules_usage_info[rule.name] = []
-
-    def show_rule_usage_statistic(self):
-        for rule_name, rule_nb_patch_list in self.cfg_rules_usage_info.items():
-            nb_use = len(rule_nb_patch_list)
-            if nb_use > 0:
-                main_logger.info(
-                    "BlkRule '%s' has been used %d times for a total of %d patches",
-                    rule_name,
-                    nb_use,
-                    sum(rule_nb_patch_list),
-                )
+    # statistics are managed centrally via the stats object
 
     def optimize(self, blk: mblock_t):
         for cfg_rule in self.cfg_rules:
@@ -312,14 +303,15 @@ class BlockOptimizerManager(optblock_t):
                     optimizer_logger.info(
                         "Rule {0} matched: {1} patches".format(cfg_rule.name, nb_patch)
                     )
-                    self.cfg_rules_usage_info[cfg_rule.name].append(nb_patch)
+                    if self.stats is not None:
+                        self.stats.record_cfg_rule_patches(cfg_rule.name, nb_patch)
+
                     return nb_patch
         return 0
 
     def add_rule(self, cfg_rule: FlowOptimizationRule):
         optimizer_logger.info("Adding cfg rule {0}".format(cfg_rule))
         self.cfg_rules.add(cfg_rule)
-        self.cfg_rules_usage_info[cfg_rule.name] = []
 
     def configure(self, **kwargs):
         pass
diff --git a/src/d810/hexrays/tracker.py b/src/d810/hexrays/tracker.py
index 98d4a09..bcdac9a 100644
--- a/src/d810/hexrays/tracker.py
+++ b/src/d810/hexrays/tracker.py
@@ -1,6 +1,6 @@
 from __future__ import annotations
 
-from typing import Dict
+import logging
 
 from ida_hexrays import *
 
@@ -37,7 +37,7 @@ from d810.hexrays.hexrays_helpers import (
 # the searched mops have only one possible values. For instance, this is a preliminary step used in code unflattening.
 
 
-logger = getLogger("D810.tracker")
+logger = getLogger(__name__, logging.WARNING)
 
 
 class InstructionDefUseCollector(mop_visitor_t):
@@ -48,6 +48,10 @@ class InstructionDefUseCollector(mop_visitor_t):
         self.target_mops = []
 
     def visit_mop(self, op: mop_t, op_type: int, is_target: bool):
+        # Skip mops with invalid sizes (e.g., function references with size=-1)
+        if op.size < 0:
+            return 0
+
         if is_target:
             append_mop_if_not_in_list(op, self.target_mops)
         else:
@@ -109,7 +113,8 @@ class MopHistory(object):
         self.history = []
         self.unresolved_mop_list = []
 
-        self._mc_interpreter = MicroCodeInterpreter()
+        # Don't use symbolic mode for tracking - we need to know when variables are unresolved
+        self._mc_interpreter = MicroCodeInterpreter(symbolic_mode=False)
         self._mc_initial_environment = MicroCodeEnvironment()
         self._mc_current_environment = self._mc_initial_environment.get_copy()
         self._is_dirty = True
@@ -176,6 +181,7 @@ class MopHistory(object):
 
     def _execute_microcode(self) -> bool:
         if not self._is_dirty:
+            logger.debug("_execute_microcode: already clean, using cached results")
             return True
         formatted_mop_searched_list = (
             "['" + "', '".join([format_mop_t(x) for x in self.searched_mop_list]) + "']"
@@ -185,6 +191,12 @@ class MopHistory(object):
                 formatted_mop_searched_list, self.block_serial_path
             )
         )
+        logger.debug(
+            "History has {0} blocks with total {1} instructions".format(
+                len(self.history),
+                sum(len(blk_info.ins_list) for blk_info in self.history),
+            )
+        )
         self._mc_current_environment = self._mc_initial_environment.get_copy()
         for blk_info in self.history:
             for blk_ins in blk_info.ins_list:
@@ -199,11 +211,27 @@ class MopHistory(object):
                     self._is_dirty = False
                     return False
         self._is_dirty = False
+        # Debug: dump environment after execution
+        if logger.debug_on:
+            self._mc_current_environment.dump(
+                "Tracker environment after _execute_microcode"
+            )
         return True
 
     def get_mop_constant_value(self, searched_mop: mop_t) -> Union[None, int]:
+        if logger.debug_on:
+            logger.debug(
+                "get_mop_constant_value called for {0}, _is_dirty={1}, history_len={2}".format(
+                    format_mop_t(searched_mop), self._is_dirty, len(self.history)
+                )
+            )
         if not self._execute_microcode():
+            logger.debug("get_mop_constant_value: _execute_microcode returned False")
             return None
+        if logger.debug_on:
+            self._mc_current_environment.dump(
+                f"Tracker environment before eval_mop for {format_mop_t(searched_mop)}"
+            )
         return self._mc_interpreter.eval_mop(searched_mop, self._mc_current_environment)
 
     def print_info(self, detailed_info=False):
@@ -253,6 +281,17 @@ class MopHistory(object):
 
 
 def get_standard_and_memory_mop_lists(mop_in: mop_t) -> tuple[list[mop_t], list[mop_t]]:
+    # Filter out mops with invalid sizes (e.g., function references with size=-1)
+    # These cannot be tracked or evaluated as variables
+    if mop_in.size < 0:
+        if logger.debug_on:
+            logger.debug(
+                "Skipping mop with invalid size (%d): %s",
+                mop_in.size,
+                format_mop_t(mop_in),
+            )
+        return [], []
+
     if mop_in.t in [mop_r, mop_S]:
         return [mop_in], []
     elif mop_in.t == mop_v:
diff --git a/src/d810/manager.py b/src/d810/manager.py
index c4c244a..82b6d4a 100644
--- a/src/d810/manager.py
+++ b/src/d810/manager.py
@@ -1,13 +1,17 @@
 from __future__ import annotations
 
 import contextlib
+import cProfile
 import dataclasses
 import inspect
 import pathlib
+import pstats
+import time
 import typing
 
 from d810.conf import D810Configuration, ProjectConfiguration
 from d810.conf.loggers import clear_logs, configure_loggers, getLogger
+from d810.cythxr.cymode import CythonMode
 from d810.expr.utils import MOP_CONSTANT_CACHE, MOP_TO_AST_CACHE
 from d810.hexrays.hexrays_hooks import (
     BlockOptimizerManager,
@@ -20,9 +24,11 @@ from d810.optimizers.microcode.instructions.handler import InstructionOptimizati
 from d810.project_manager import ProjectManager
 from d810.registry import EventEmitter
 from d810.singleton import SingletonMeta
+from d810.stats import OptimizationStatistics
 from d810.ui.ida_ui import D810GUI
 
 try:
+
     import pyinstrument  # type: ignore
 except ImportError:
     pyinstrument = None
@@ -32,6 +38,32 @@ D810_LOG_DIR_NAME = "d810_logs"
 logger = getLogger("D810")
 
 
+class CProfileWrapper:
+    """
+    A simple wrapper around cProfile.Profile that exposes an `.is_running` property.
+    """
+
+    def __init__(self):
+        self._profiler = cProfile.Profile()
+        self._is_running = False
+
+    @property
+    def is_running(self):
+        return self._is_running
+
+    def enable(self, *args, **kwargs):
+        self._profiler.enable(*args, **kwargs)
+        self._is_running = True
+
+    def disable(self):
+        self._profiler.disable()
+        self._is_running = False
+
+    @property
+    def profiler(self):
+        return self._profiler
+
+
 @dataclasses.dataclass
 class D810Manager:
     log_dir: pathlib.Path
@@ -44,10 +76,18 @@ class D810Manager:
     profiler: typing.Any = dataclasses.field(
         default_factory=lambda: pyinstrument.Profiler() if pyinstrument else None
     )
+    cprofiler: CProfileWrapper | None = dataclasses.field(
+        default_factory=lambda: CProfileWrapper() if cProfile else None
+    )
+    stats: OptimizationStatistics = dataclasses.field(
+        default_factory=OptimizationStatistics
+    )
     instruction_optimizer: InstructionOptimizerManager = dataclasses.field(init=False)
     block_optimizer: BlockOptimizerManager = dataclasses.field(init=False)
     hx_decompiler_hook: HexraysDecompilationHook = dataclasses.field(init=False)
     _started: bool = dataclasses.field(default=False, init=False)
+    _profiling_enabled: bool = dataclasses.field(default=False, init=False)
+    _start_ts: float = dataclasses.field(default=0.0, init=False)
 
     @property
     def started(self):
@@ -57,10 +97,21 @@ class D810Manager:
         self.config = kwargs
 
     def start_profiling(self):
+        if not self._profiling_enabled:
+            return
+
+        if self.cprofiler and not self.cprofiler.is_running:
+            self.cprofiler.enable()
         if self.profiler and not self.profiler.is_running:
             self.profiler.start()
 
     def stop_profiling(self) -> pathlib.Path | None:
+        if self.cprofiler and self.cprofiler.is_running:
+            self.cprofiler.disable()
+            output_path = self.log_dir / "d810_cprofile.prof"
+            self.cprofiler.profiler.dump_stats(str(output_path))
+            pstats.Stats(str(output_path)).strip_dirs().sort_stats("time").print_stats()
+            return output_path
         if self.profiler and self.profiler.is_running:
             self.profiler.stop()
             self.profiler.print()
@@ -70,15 +121,29 @@ class D810Manager:
                 f.write(self.profiler.output_html())
             return output_path
 
+    def disable_profiling(self):
+        self._profiling_enabled = False
+        self.stop_profiling()
+
+    def enable_profiling(self):
+        self._profiling_enabled = True
+        self.start_profiling()
+
     def start(self):
         if self._started:
             self.stop()
         logger.debug("Starting manager...")
 
         # Instantiate core manager classes from registry
-        self.instruction_optimizer = InstructionOptimizerManager(self.log_dir)
+        self.instruction_optimizer = InstructionOptimizerManager(
+            self.stats,
+            log_dir=self.log_dir,
+        )
         self.instruction_optimizer.configure(**self.instruction_optimizer_config)
-        self.block_optimizer = BlockOptimizerManager(self.log_dir)
+        self.block_optimizer = BlockOptimizerManager(
+            self.stats,
+            log_dir=self.log_dir,
+        )
         self.block_optimizer.configure(**self.block_optimizer_config)
 
         for rule in self.instruction_optimizer_rules:
@@ -93,40 +158,6 @@ class D810Manager:
         self._install_hooks()
         self._started = True
 
-    def _install_hooks(self):
-        # must become before listeners are installed
-        for _subscriber in (
-            self.start_profiling,
-            self.instruction_optimizer.reset_rule_usage_statistic,
-            self.block_optimizer.reset_rule_usage_statistic,
-            MOP_CONSTANT_CACHE.clear,
-            MOP_TO_AST_CACHE.clear,
-        ):
-            self.event_emitter.on(DecompilationEvent.STARTED, _subscriber)
-
-        for _subscriber in (
-            self.stop_profiling,
-            self.instruction_optimizer.show_rule_usage_statistic,
-            self.block_optimizer.show_rule_usage_statistic,
-            lambda: logger.info(
-                "MOP_CONSTANT_CACHE stats: %s", MOP_CONSTANT_CACHE.stats
-            ),
-            lambda: logger.info("MOP_TO_AST_CACHE stats: %s", MOP_TO_AST_CACHE.stats),
-        ):
-            self.event_emitter.on(DecompilationEvent.FINISHED, _subscriber)
-
-        self.instruction_optimizer.install()
-        self.block_optimizer.install()
-        self.hx_decompiler_hook.hook()
-
-    def configure_instruction_optimizer(self, rules, **kwargs):
-        self.instruction_optimizer_rules = [rule for rule in rules]
-        self.instruction_optimizer_config = kwargs
-
-    def configure_block_optimizer(self, rules, **kwargs):
-        self.block_optimizer_rules = [rule for rule in rules]
-        self.block_optimizer_config = kwargs
-
     def stop(self):
         if not self._started:
             return
@@ -136,9 +167,57 @@ class D810Manager:
         self.block_optimizer.remove()
         self.hx_decompiler_hook.unhook()
         self.event_emitter.clear()
-        if self.profiler:
+        if self.profiler or self.cprofiler:
             self.stop_profiling()
 
+    def _start_timer(self):
+        self._start_ts = time.perf_counter()
+
+    def _stop_timer(self, report: bool = True):
+        if report:
+            m, s = divmod(time.perf_counter() - self._start_ts, 60)
+            logger.info(
+                "Decompilation finished in %dm %ds",
+                int(m),
+                int(s),
+            )
+        self._start_ts = 0.0
+
+    def _install_hooks(self):
+        # must become before listeners are installed
+        for _subscriber in (
+            self.start_profiling,
+            MOP_CONSTANT_CACHE.clear,
+            MOP_TO_AST_CACHE.clear,
+            self.stats.reset,
+            self._start_timer,
+        ):
+            self.event_emitter.on(DecompilationEvent.STARTED, _subscriber)
+
+        for _subscriber in (
+            self.stop_profiling,
+            self._report_caches,
+            self.stats.report,
+            self._stop_timer,
+        ):
+            self.event_emitter.on(DecompilationEvent.FINISHED, _subscriber)
+
+        self.instruction_optimizer.install()
+        self.block_optimizer.install()
+        self.hx_decompiler_hook.hook()
+
+    def _report_caches(self):
+        logger.info("MOP_CONSTANT_CACHE stats: %s", MOP_CONSTANT_CACHE.stats())
+        logger.info("MOP_TO_AST_CACHE stats: %s", MOP_TO_AST_CACHE.stats())
+
+    def configure_instruction_optimizer(self, rules, **kwargs):
+        self.instruction_optimizer_rules = [rule for rule in rules]
+        self.instruction_optimizer_config = kwargs
+
+    def configure_block_optimizer(self, rules, **kwargs):
+        self.block_optimizer_rules = [rule for rule in rules]
+        self.block_optimizer_config = kwargs
+
 
 class D810State(metaclass=SingletonMeta):
     """
@@ -183,6 +262,7 @@ class D810State(metaclass=SingletonMeta):
         # to a sensible default when the option is missing, instead of reading
         # the raw option that may be None and break pathlib.Path construction.
         self.manager = D810Manager(self.log_dir)
+        self._cython_mode = CythonMode(self.d810_config.get("cython_mode", True))
         self._initialized = True
 
     def add_project(self, config: ProjectConfiguration):
@@ -322,3 +402,17 @@ class D810State(metaclass=SingletonMeta):
         if project_index != _old_project_index:
             logger.info("switching back to project %s", _old_project_index)
             self.load_project(_old_project_index)
+
+    def enable_cython_speedups(self):
+        self._cython_mode.enable()
+
+    def disable_cython_speedups(self):
+        self._cython_mode.disable()
+
+    def are_cython_speedups_enabled(self):
+        return self._cython_mode.is_enabled()
+
+    # Expose statistics to callers (e.g., tests)
+    @property
+    def stats(self) -> OptimizationStatistics:
+        return self.manager.stats
diff --git a/src/d810/optimizers/microcode/flow/constant_prop/_fast_dataflow.pyx b/src/d810/optimizers/microcode/flow/constant_prop/_fast_dataflow.pyx
new file mode 100644
index 0000000..168b291
--- /dev/null
+++ b/src/d810/optimizers/microcode/flow/constant_prop/_fast_dataflow.pyx
@@ -0,0 +1,753 @@
+# cython: language_level=3, embedsignature=True
+# distutils: language=c++
+# distutils: define_macros=__EA64__=1
+"""
+Simplified, container-free Cython implementation for constant-propagation data-flow.
+This purposefully drops all C++ STL usage.  We only keep raw Hex-Rays SDK structs
+and convert everything to plain Python data-structures at the API boundary.
+
+Only three tiny helpers remain in C: they inspect a handful of `mop_t` / `minsn_t`
+fields that are heavily used in the data-flow loop.  Everything else happens
+in Python space which drastically reduces the surface-area for Cython-level
+compiler errors while still giving us the hot-path speed-up we want.
+"""
+from cython.operator cimport preincrement as inc
+from cython.operator cimport dereference as deref
+from libcpp.map cimport map as cpp_map
+from libcpp.utility cimport pair
+import logging as _logging
+
+# Module-level logger aligned with Python rule logger
+py_logger = _logging.getLogger("d810.optimizers.microcode.flow.constant_prop.stackvars_constprop")
+cdef bint debug_on = <bint>False
+
+# ---------------------------------------------------------------------------
+#  Raw C declarations from sibling pxd
+# ---------------------------------------------------------------------------
+from d810.cythxr._chexrays cimport (
+    SwigPyObject,
+    uint,
+    uint64,
+    intvec_t,
+    qstring, 
+    qvector,
+    mop_t, 
+    minsn_t, 
+    mba_t, 
+    get_mreg_name, 
+    mblock_t,
+    sval_t,
+    mop_t_ptr,
+    mop_off_pair_t,
+    MOPT,
+    mcode_t,
+    stkvar_ref_t,
+    const_val_t,
+    CppConstMap ,
+    mnumber_t,
+    mcallinfo_t,
+    LOCOPT_FLAGS,
+)
+from d810.cythxr._chexrays_api cimport stack_var_name as _stack_var_name
+
+# ---------------------------------------------------------------------------
+#  Tiny helpers – still in C for speed
+# ---------------------------------------------------------------------------
+
+cdef inline uint64 _mask_for_bytes(int num_bytes):
+    """Return an all-ones mask for the given byte width (1..8)."""
+    if num_bytes >= 8:
+        return <uint64>0xFFFFFFFFFFFFFFFF
+    return (<uint64>1 << (num_bytes * 8)) - <uint64>1
+
+cdef sval_t _get_mba_frame_size(mba_t* mba):
+    """Return cached frame size for an MBA. Uses object identity as cache key.
+
+    Returns `int` or `None`.
+    """
+    if not mba:
+        return 0
+    if mba.frsize:
+        return mba.frsize
+    if mba.stacksize:
+        return mba.stacksize       
+    py_logger.warning("mba.frsize and mba.stacksize are 0, using minstkref and fullsize")
+    if mba.minstkref:
+        return mba.minstkref
+    if mba.fullsize:
+        return mba.fullsize
+    return 0
+
+# cdef inline qstring _stack_var_name(mop_t* mop):
+#     cdef qstring rname
+#     cdef stkvar_ref_t* s_ptr
+#     cdef sval_t ida_off
+#     if mop.t == MOPT.STACK:
+#         s_ptr = <stkvar_ref_t*> mop.s
+#         if s_ptr.mba != NULL and s_ptr.mba.use_frame():
+#             ida_off = s_ptr.mba.stkoff_vd2ida(s_ptr.off)
+#             rname.sprnt("var_%X.%d", <unsigned>(ida_off), mop.size)
+#             # else:
+#             #     rname.sprnt("arg_%X.%d", <unsigned>(ida_off), mop.size)
+#         else:
+#             rname.sprnt("stk_%llX.%d", <unsigned long long> s_ptr.off, mop.size)
+#         rname.cat_sprnt("{%d}", mop.valnum)
+#         return rname
+
+#     elif mop.t == MOPT.REGISTER:
+#         get_mreg_name(&rname, mop.r, mop.size, <void*>NULL)
+#         rname.cat_sprnt("{%d}", mop.valnum)
+#         return rname
+
+#     cdef qstring empty
+#     return empty
+
+# cdef inline qstring _stack_var_name(mop_t* mop):
+#     """Return qstring with the canonical name of a stack/register var."""
+#     cdef:
+#         qstring empty
+#         qstring rname
+#         stkvar_ref_t* s_ptr
+#         int frame_size
+#         int disp
+
+#     if mop.t == MOPT.STACK:
+#         s_ptr = <stkvar_ref_t*>(&mop.s)
+#         frame_size = _get_mba_frame_size(s_ptr.mba)
+#         if frame_size >= s_ptr.off:
+#             disp = frame_size - s_ptr.off
+#             rname.sprnt("%%var_%X.%d", disp, mop.size)
+#         else:
+#             rname.sprnt("stk_%llX.%d", s_ptr.off, mop.size)
+#         # Match Python helper: append value number in braces
+#         rname.cat_sprnt("{%d}", mop.valnum)
+#         return rname
+
+#     elif mop.t == MOPT.REGISTER:
+#         get_mreg_name(&rname, mop.r, mop.size, <void*>NULL)
+#         # The Python helper formats registers as "reg{valnum}" (no .size)
+#         rname.cat_sprnt("{%d}", mop.valnum)
+#         return rname
+#     return empty
+
+
+cdef inline mop_off_pair_t _extract_base_and_offset(mop_t* mop):
+    """Return pointer to base mop if pattern (base + const) else NULL; stores offset."""
+    cdef:
+        mop_off_pair_t result
+
+    if mop.t == MOPT.DEST_RESULT and mop.d != NULL and mop.d.opcode == mcode_t.m_add:
+        if mop.d.l.t in (MOPT.STACK, MOPT.REGISTER):
+            result.first = <mop_t_ptr>&mop.d.l
+            result.second = mop.d.r.nnn.value if mop.d.r.t == MOPT.NUMBER else 0
+            return result
+        if mop.d.r.t in (MOPT.STACK, MOPT.REGISTER):
+            result.first = <mop_t_ptr>&mop.d.r
+            result.second = mop.d.l.nnn.value if mop.d.l.t == MOPT.NUMBER else 0
+            return result
+    result.first = <mop_t_ptr>NULL
+    result.second = 0
+    return result
+
+
+cdef inline qstring _get_written_var_name(minsn_t* ins):
+    """Return the variable name written by *ins or "" if none."""
+    cdef:
+        mop_t* d = &ins.d
+        mop_t* base
+        qstring base_name
+        mop_off_pair_t result
+        qstring empty
+
+    if d.t in (MOPT.STACK, MOPT.REGISTER):
+        return _stack_var_name(d)
+
+
+    if ins.opcode == mcode_t.m_stx:
+        result = _extract_base_and_offset(d)
+        if result.first != NULL:
+            base_name = _stack_var_name(result.first)
+            if not base_name.empty():
+                if result.second:
+                    base_name.cat_sprnt("+%llX", result.second)
+                return base_name
+    return empty
+
+
+
+cdef inline bint _is_constant_stack_assignment(minsn_t* ins):
+    """True if instruction assigns a constant to a stack/register var."""
+    cdef:
+        mop_t* base
+        mop_off_pair_t result
+
+    if ins.l.t != MOPT.NUMBER:
+        return <bint>False
+    if ins.opcode == mcode_t.m_mov and ins.d.t in (MOPT.STACK, MOPT.REGISTER):
+        return <bint>True
+    if ins.opcode == mcode_t.m_stx:
+        if ins.d.t == MOPT.STACK:
+            return <bint>True
+        result = _extract_base_and_offset(&ins.d)
+        return result.first != NULL
+    return <bint>False
+
+# ---------------------------------------------------------------------------
+#  Map & iteration helpers (hide iterator casting, make meet/transfer readable)
+# ---------------------------------------------------------------------------
+
+# NOTE:
+# We previously exposed `_key_of`/`_val_of` helpers that cast the iterator
+# storage to `pair[const qstring, const_val_t]` in order to access `.first/
+# .second`. Cython chokes on `const` template args for non-trivial classes
+# like `qstring`, producing errors such as "Cannot assign type 'long' to
+# 'const T'". To avoid this, we now access keys/values directly using
+# `deref(it).first` / `deref(it).second` at call sites.
+
+cdef inline bint _map_equals(const CppConstMap& a, const CppConstMap& b):
+    if a.size() != b.size():
+        return False
+    cdef CppConstMap.const_iterator it = a.begin()
+    cdef CppConstMap.const_iterator f
+    cdef const_val_t av, bv
+    while it != a.end():
+        f = b.find(deref(it).first)       # was: cast to pair[...] then .first
+        if f == b.end():
+            return False
+        av = deref(it).second             # was: cast to pair[...] then .second
+        bv = deref(f).second
+        if av.first != bv.first or av.second != bv.second:
+            return False
+        inc(it)
+    return True
+
+cdef inline void _map_assign(CppConstMap& dst, const CppConstMap& src):
+    dst = src  # C++ copy
+
+# Intersect OUT of predecessors into 'inm'
+cdef void _meet_preds(
+    size_t b,
+    qvector[intvec_t]& preds,
+    qvector[CppConstMap]& OUT_cpp,
+    CppConstMap& inm
+):
+    inm.clear()
+    if preds[b].empty():
+        return
+
+    # Seed with first predecessor's OUT (copy if present)
+    cdef unsigned int p0 = preds[b][0]
+    if p0 < OUT_cpp.size():
+        inm = OUT_cpp[p0]
+    else:
+        inm.clear()
+
+    # Intersect with remaining predecessors
+    cdef size_t i
+    cdef unsigned int p
+    cdef CppConstMap.iterator it
+    cdef CppConstMap.iterator f
+    cdef const_val_t v, w
+
+    for i in range(1, preds[b].size()):
+        p = preds[b][i]
+        it = inm.begin()
+        while it != inm.end():
+            if p < OUT_cpp.size():
+                f = OUT_cpp[p].find(deref(it).first)
+                if f != OUT_cpp[p].end():
+                    v = deref(it).second
+                    w = deref(f).second
+                    if v.first == w.first and v.second == w.second:
+                        inc(it)
+                        continue
+            it = inm.erase(it)
+
+cdef inline void _transfer_insn(mblock_t* blk, minsn_t* ins, CppConstMap& env):
+    # conservative side-effects handling
+    if ins.is_unknown_call():  # or opcode test if needed
+        env.clear()
+        return
+
+    cdef bint is_const = _is_constant_stack_assignment(ins)
+    cdef qstring name = _get_written_var_name(ins)
+    if not is_const:
+        if not name.empty():
+            env.erase(name)
+        return
+
+    cdef qstring var_name
+    cdef mop_off_pair_t pair
+
+    if ins.opcode == mcode_t.m_mov or ins.d.t in (MOPT.STACK, MOPT.REGISTER):
+        var_name = _stack_var_name(&ins.d)
+    else:
+        pair = _extract_base_and_offset(&ins.d)
+        if pair.first != NULL:
+            var_name = _stack_var_name(pair.first)
+            if not var_name.empty() and pair.second:
+                var_name.cat_sprnt("+%llX", pair.second)
+
+    if not var_name.empty():
+        env[var_name] = const_val_t(ins.l.nnn.value, ins.l.size)
+
+
+cdef inline void _clear_on_side_effect(minsn_t* ins, CppConstMap& env):
+    if ins.has_side_effects(False) and ins.opcode != mcode_t.m_stx:
+        env.clear()
+        
+# Block transfer: OUTb = F_b(INb)
+cdef void _transfer_block(mblock_t* blk, const CppConstMap& INb, CppConstMap& OUTb):
+    OUTb = INb  # start from IN
+    # Cython cannot declare C++ references in local scope; operate on OUTb directly.
+    cdef minsn_t* ins = blk.head
+    cdef bint is_const
+    cdef qstring name, var_name
+    cdef mop_off_pair_t res_pair
+    
+
+    while ins:
+        _clear_on_side_effect(ins, OUTb)
+
+        name = _get_written_var_name(ins)
+        is_const = _is_constant_stack_assignment(ins)
+
+        if not is_const:
+            if not name.empty():
+                OUTb.erase(name)
+        else:
+            if ins.opcode == mcode_t.m_mov:
+                var_name = _stack_var_name(&ins.d)
+            elif ins.d.t in (MOPT.STACK, MOPT.REGISTER):
+                var_name = _stack_var_name(&ins.d)
+            else:
+                res_pair = _extract_base_and_offset(&ins.d)
+                if res_pair.first != NULL:
+                    var_name = _stack_var_name(res_pair.first)
+                    if not var_name.empty() and res_pair.second:
+                        var_name.cat_sprnt("+%llX", res_pair.second)
+
+            if not var_name.empty():
+                OUTb[var_name] = const_val_t(ins.l.nnn.value, ins.l.size)
+
+        ins = ins.next
+
+# C-level rewrite helper for the full pass
+cdef bint _rewrite_instruction_c(minsn_t* ins, CppConstMap& consts):
+    """C-level rewrite helper. Operates on C++ map directly."""
+    cdef bint changed = <bint>False
+    cdef uint64 lval, rval, res
+    cdef bint can_fold
+    cdef mop_t nm, zr
+    cdef int dsize
+
+    if _cy_process_operand(&ins.l, consts): changed = <bint>True
+    if _cy_process_operand(&ins.r, consts): changed = <bint>True
+    if ins.opcode == mcode_t.m_stx and _cy_process_operand(&ins.d, consts):
+        changed = <bint>True
+
+    if changed:
+        ins.optimize_solo(0)
+
+    # NOTE: The constant folding logic from the Python wrapper is omitted here
+    # for simplicity, but could be added if needed. The primary gain is from
+    # operand processing.
+
+    return changed
+
+# ---------------------------------------------------------------------------
+#  Python-callable wrappers (cpdef)
+# ---------------------------------------------------------------------------
+
+cpdef str cy_get_written_var_name(object ins_py):
+    """Python wrapper for _get_written_var_name."""
+    cdef SwigPyObject* swig_obj = <SwigPyObject*>ins_py.this
+    cdef minsn_t* ins = <minsn_t*>swig_obj.ptr
+    cdef qstring qn = _get_written_var_name(ins)
+    cdef bytes b
+    b = qn.c_str()
+    return b.decode('utf-8')
+
+
+cpdef cy_is_constant_stack_assignment(object ins_py):
+    """Python wrapper for _is_constant_stack_assignment."""
+    cdef SwigPyObject* swig_obj = <SwigPyObject*>ins_py.this
+    cdef minsn_t* ins = <minsn_t*>swig_obj.ptr
+    return bool(_is_constant_stack_assignment(ins))
+
+
+cpdef cy_extract_assignment(object ins_py):
+    """Extracts (var_name, (value, size)) from a constant assignment."""
+    cdef SwigPyObject* swig_obj = <SwigPyObject*>ins_py.this
+    cdef minsn_t* ins = <minsn_t*>swig_obj.ptr
+    cdef:
+        mop_off_pair_t result
+        qstring var_name
+
+    if not _is_constant_stack_assignment(ins):
+        return None
+
+    cdef uint64 value = ins.l.nnn.value
+    cdef int size = ins.l.size
+    if ins.opcode == mcode_t.m_mov:
+        var_name = _stack_var_name(&ins.d)
+    elif ins.d.t in {MOPT.STACK, MOPT.REGISTER}:
+        var_name = _stack_var_name(&ins.d)
+    else:
+        result = _extract_base_and_offset(&ins.d)
+        if result.first != NULL:
+            var_name = _stack_var_name(result.first)
+            if not var_name.empty():
+                if result.second:
+                    var_name.cat_sprnt("+%llX", result.second)
+    
+    if var_name.empty():
+        return None
+    cdef bytes bname
+    bname = var_name.c_str()
+    return (bname.decode('utf-8'), (value, size))
+
+
+cdef bint _cy_process_operand(mop_t* op, CppConstMap& consts):
+    """C-level recursive function to replace variables with constants."""  
+    cdef:
+        bint changed = <bint>False
+        qstring name
+        mop_off_pair_t result
+        CppConstMap.iterator it
+        uint64 val
+        int size
+        mop_t temp_mop
+        mcallinfo_t* f_ptr
+        size_t i
+        mop_t* addr
+        qstring base_name
+        qstring full_name
+        bint const_info_found
+
+    if op.t == MOPT.STACK or op.t == MOPT.REGISTER:
+        name = _stack_var_name(op)
+        if not name.empty():
+            it = consts.find(name)
+            if it != consts.end():
+                val = deref(it).second.first
+                temp_mop.make_number(val & _mask_for_bytes(op.size), op.size)
+                op.assign(temp_mop)
+                return <bint>True
+    elif op.t == MOPT.DEST_RESULT and op.d != NULL:
+        # Special-case: fold loads from memory when the address is a known constant
+        if op.d.opcode == mcode_t.m_ldx:
+            addr = &op.d.r
+            const_info_found = <bint>False
+
+            if addr.t == MOPT.STACK or addr.t == MOPT.REGISTER:
+                name = _stack_var_name(addr)
+                it = consts.find(name)
+                if it != consts.end(): 
+                    const_info_found = <bint>True
+            else:
+                result = _extract_base_and_offset(addr)
+                if result.first != NULL:
+                    base_name = _stack_var_name(result.first)
+                    if not base_name.empty():
+                        if result.second:
+                            full_name = base_name
+                            full_name.cat_sprnt("+%llX", result.second)
+                        else:
+                            full_name = base_name
+                        it = consts.find(full_name)
+                        if it != consts.end(): 
+                            const_info_found = <bint>True
+
+            if const_info_found:
+                val = deref(it).second.first
+                temp_mop.make_number(val & _mask_for_bytes(op.size), op.size)
+                op.assign(temp_mop)
+                return <bint>True
+                
+        # Generic recursion on sub-operands
+        if _cy_process_operand(&op.d.l, consts): 
+            changed = <bint>True
+        if _cy_process_operand(&op.d.r, consts): 
+            changed = <bint>True
+        if changed: 
+            op.d.optimize_solo(0)
+        return changed
+    elif op.t == MOPT.ARGUMENT_LIST and op.f != NULL:
+        f_ptr = <mcallinfo_t*>op.f
+        for i in range(f_ptr.args.size()):
+             if _cy_process_operand(&f_ptr.args.at(i), consts):
+                 changed = <bint>True
+        return changed
+    return <bint>False
+
+
+cpdef int cy_rewrite_instruction(object ins_py, dict consts_py):
+    """Public wrapper to process a single instruction's operands."""
+    cdef SwigPyObject* swig_obj = <SwigPyObject*>ins_py.this
+    cdef minsn_t* ins = <minsn_t*>swig_obj.ptr
+    cdef bint changed = <bint>False
+    cdef uint64 lval
+    cdef uint64 rval
+    cdef uint64 res
+    cdef bint can_fold
+    cdef mop_t nm
+    cdef mop_t zr
+    cdef int dsize
+    cdef bytes b_string   
+
+    cdef CppConstMap consts
+    cdef qstring key
+    for py_key, py_val in consts_py.items():
+        b_string = py_key.encode('utf-8')
+        key = qstring(<char*>b_string)
+        consts[key] = const_val_t(py_val[0], py_val[1])
+        
+    if _cy_process_operand(&ins.l, consts):
+        changed = <bint>True
+    if _cy_process_operand(&ins.r, consts): 
+        changed = <bint>True
+    if ins.opcode == mcode_t.m_stx and _cy_process_operand(&ins.d, consts):
+        changed = <bint>True
+
+    # Let Hex-Rays perform local simplifications before folding whole instruction
+    if changed:
+        ins.optimize_solo(0)
+
+    # If both operands are immediates for a pure binary op, fold to MOV
+    if ins.d.t in (MOPT.STACK, MOPT.REGISTER) and ins.l.t == MOPT.NUMBER and ins.r.t == MOPT.NUMBER:
+        lval = ins.l.nnn.value
+        rval = ins.r.nnn.value
+        res = 0
+        can_fold = <bint>False
+        if ins.opcode == mcode_t.m_or:
+            res = lval | rval; can_fold = <bint>True
+        elif ins.opcode == mcode_t.m_and:
+            res = lval & rval; can_fold = <bint>True
+        elif ins.opcode == mcode_t.m_xor:
+            res = lval ^ rval; can_fold = <bint>True
+        elif ins.opcode == mcode_t.m_add:
+            res = lval + rval; can_fold = <bint>True
+        elif ins.opcode == mcode_t.m_sub:
+            res = lval - rval; can_fold = <bint>True
+        elif ins.opcode == mcode_t.m_mul:
+            res = lval * rval; can_fold = <bint>True
+        elif ins.opcode == mcode_t.m_shl:
+            res = lval << (rval & 0x3F); can_fold = <bint>True
+        elif ins.opcode == mcode_t.m_shr or ins.opcode == mcode_t.m_sar:
+            res = lval >> (rval & 0x3F); can_fold = <bint>True
+        if can_fold:
+            dsize = ins.d.size if ins.d.size != 0 else ins.l.size
+            nm.make_number(res & _mask_for_bytes(dsize), dsize)
+            ins.opcode = mcode_t.m_mov
+            ins.l.assign(nm)
+            # Clear r by assigning a neutral mop_t() declared above
+            zr.make_number(0, ins.r.size if ins.r.size != 0 else dsize)
+            ins.r.assign(zr)
+            changed = <bint>True
+
+    if not changed:
+        return 0
+    # In case we just folded to MOV or further changes occurred, re-run local opt
+    ins.optimize_solo(0)
+    return 1
+
+
+# ---------------------------------------------------------------------------
+#  Main Dataflow Implementation
+# ---------------------------------------------------------------------------
+
+cpdef run_dataflow_cython(object mba_py):
+    """Public entry - performs constant-propagation data-flow, returns (IN, OUT)."""
+    cdef:
+        SwigPyObject* swig_obj = <SwigPyObject*>mba_py.this
+        mba_t* mba = <mba_t*>swig_obj.ptr
+        uint nb
+        qvector[CppConstMap] IN_cpp
+        qvector[CppConstMap] OUT_cpp
+        qvector[intvec_t] preds
+        intvec_t worklist
+        mblock_t* blk
+        unsigned int b
+        intvec_t.iterator it_idx
+        CppConstMap inm
+        CppConstMap outb
+
+    if not mba:
+        raise ValueError("invalid mba_py - cannot get C++ pointer")
+
+    nb = mba.qty
+    if debug_on:
+        py_logger.debug("Running dataflow analysis on mba %s (blocks=%d)", mba_py, nb)
+
+    # Pre-size analysis vectors to avoid size checks
+    IN_cpp.resize_with_default(nb)
+    OUT_cpp.resize_with_default(nb)
+    preds.resize_with_default(nb)
+
+    # Build predecessor lists once
+    for b in range(nb):
+        blk = mba.get_mblock(b)
+        if not blk:
+            continue
+        it_idx = blk.predset.begin()
+        while it_idx != blk.predset.end():
+            preds[b].push_back(it_idx[0])
+            inc(it_idx)
+
+    # Seed worklist with all blocks
+    for b in range(nb):
+        worklist.push_back(b)
+
+    # Main worklist loop
+    while not worklist.empty():
+        b = worklist.back()
+        worklist.pop_back()
+
+        blk = mba.get_mblock(b)
+        if not blk:
+            continue
+
+        # MEET
+        inm.clear()
+        _meet_preds(b, preds, OUT_cpp, inm)
+
+        if not _map_equals(inm, IN_cpp[b]):
+            _map_assign(IN_cpp[b], inm)
+
+        # TRANSFER
+        outb.clear()
+        _transfer_block(blk, IN_cpp[b], outb)
+
+        if not _map_equals(outb, OUT_cpp[b]):
+            _map_assign(OUT_cpp[b], outb)
+            # Push all successors (simple; duplicates are filtered by later equality checks)
+            it_idx = blk.succset.begin()
+            while it_idx != blk.succset.end():
+                worklist.push_back(it_idx[0])
+                inc(it_idx)
+
+    if debug_on:
+        py_logger.debug("Converting C-level results back to Python dicts")
+
+    # Convert C-level results back to Python dicts
+    cdef list IN_py = [{} for _ in range(nb)]
+    cdef list OUT_py = [{} for _ in range(nb)]
+    cdef CppConstMap.iterator it
+    cdef qstring k
+    cdef const_val_t v
+
+    for b in range(nb):
+        it = IN_cpp[b].begin()
+        while it != IN_cpp[b].end():
+            k = deref(it).first
+            v = deref(it).second
+            IN_py[b][k.c_str().decode('utf-8')] = (v.first, v.second)
+            inc(it)
+
+        it = OUT_cpp[b].begin()
+        while it != OUT_cpp[b].end():
+            k = deref(it).first
+            v = deref(it).second
+            OUT_py[b][k.c_str().decode('utf-8')] = (v.first, v.second)
+            inc(it)
+
+    return IN_py, OUT_py
+
+cpdef int cy_run_full_pass(object mba_py):
+    """
+    Performs the full dataflow and rewrite pass entirely in C-level code,
+    minimizing Python-to-C transitions to a single call.
+    """
+    cdef:
+        SwigPyObject* swig_obj = <SwigPyObject*>mba_py.this
+        mba_t* mba = <mba_t*>swig_obj.ptr
+        uint nb
+        qvector[CppConstMap] IN_cpp
+        qvector[CppConstMap] OUT_cpp
+        qvector[intvec_t] preds
+        intvec_t worklist
+        mblock_t* curr_blk
+        minsn_t* ins
+        unsigned int b
+        int total_changes = 0
+        bint block_was_changed
+        bint made_change_this_pass
+        intvec_t.iterator it_idx
+        CppConstMap inm, outb
+        CppConstMap consts
+
+    if not mba:
+        return 0
+    nb = mba.qty
+
+    # --- Phase A: Dataflow Analysis ---
+    # (This is the same logic as run_dataflow_cython)
+    IN_cpp.resize_with_default(nb)
+    OUT_cpp.resize_with_default(nb)
+    preds.resize_with_default(nb)
+    for b in range(nb):
+        curr_blk = mba.get_mblock(b)
+        if not curr_blk: continue
+        it_idx = curr_blk.predset.begin()
+        while it_idx != curr_blk.predset.end():
+            preds[b].push_back(deref(it_idx))
+            inc(it_idx)
+        worklist.push_back(b)
+
+    while not worklist.empty():
+        b = worklist.back()
+        worklist.pop_back()
+        curr_blk = mba.get_mblock(b)
+        if not curr_blk:
+            continue
+
+        _meet_preds(b, preds, OUT_cpp, inm)
+        if not _map_equals(inm, IN_cpp[b]):
+            _map_assign(IN_cpp[b], inm)
+        _transfer_block(curr_blk, IN_cpp[b], outb)
+        if not _map_equals(outb, OUT_cpp[b]):
+            _map_assign(OUT_cpp[b], outb)
+            it_idx = curr_blk.succset.begin()
+            while it_idx != curr_blk.succset.end():
+                worklist.push_back(deref(it_idx))
+                inc(it_idx)
+
+    # --- Phase B: Rewrite Loop ---
+    curr_blk = mba.get_mblock(0)
+    while curr_blk != NULL:
+        block_was_changed = <bint>False
+        while True:
+            consts = IN_cpp[curr_blk.serial]
+            ins = curr_blk.head
+            made_change_this_pass = <bint>False
+            # while ins != NULL:
+            #     if _rewrite_instruction_c(ins, consts):
+            #         total_changes += 1
+            #         made_change_this_pass = <bint>True
+            #         block_was_changed = <bint>True
+            #         break
+            #     _transfer_block(curr_blk, consts, consts) # Re-transfer on the modified map
+            #     ins = ins.next
+            while ins != NULL:
+                if _rewrite_instruction_c(ins, consts):
+                    total_changes += 1
+                    made_change_this_pass = <bint>True
+                    block_was_changed = <bint>True
+                    ins.optimize_solo(0)
+                    # keep scanning; env already reflects rewrites
+
+                _transfer_insn(curr_blk, ins, consts)  # advance env by 1 insn
+                ins = ins.next            
+            if not made_change_this_pass: 
+                break            
+
+        if block_was_changed: 
+            curr_blk.mark_lists_dirty()
+        curr_blk = curr_blk.nextb
+
+    if total_changes > 0:
+        mba.mark_chains_dirty()
+        mba.optimize_local(LOCOPT_FLAGS.LOCOPT_ALL)
+
+    return total_changes
\ No newline at end of file
diff --git a/src/d810/optimizers/microcode/flow/constant_prop/_stackvars_constprop.pyx b/src/d810/optimizers/microcode/flow/constant_prop/_stackvars_constprop.pyx
new file mode 100644
index 0000000..34b0584
--- /dev/null
+++ b/src/d810/optimizers/microcode/flow/constant_prop/_stackvars_constprop.pyx
@@ -0,0 +1,144 @@
+ # distutils: language = c++
+import cython
+from libcpp cimport bool
+from libc.stdint cimport uintptr_t
+# ===========================================================================
+# C++ Definitions from hexrays.hpp
+# ===========================================================================
+
+# Import enums and constants first
+cdef extern from "hexrays.hpp":
+    ctypedef enum mopt_t:
+        mop_z, mop_r, mop_n, mop_str, mop_d, mop_S, mop_v, mop_b,
+        mop_f, mop_l, mop_a, mop_h, mop_c, mop_fn, mop_p, mop_sc
+
+    ctypedef enum mcode_t:
+        m_nop, m_stx, m_ldx, m_mov # Add others as needed
+
+# Forward declare classes to resolve dependencies
+cdef extern from "hexrays.hpp":
+    cdef cppclass mop_t:
+        mopt_t t
+        int size
+        void make_number(unsigned long long, int, long long, int) nogil
+        void assign(const mop_t&) nogil
+
+    cdef cppclass stkvar_ref_t:
+        long long off
+
+    cdef cppclass minsn_t:
+        mcode_t opcode
+        mop_t l, r, d
+        void optimize_solo() nogil
+
+# ===========================================================================
+# Python Imports
+# ===========================================================================
+import ida_hexrays
+
+# We still need Python helpers for name resolution & masking
+from d810.hexrays.cfg_utils import get_stack_var_name, extract_base_and_offset
+from d810.hexrays.hexrays_helpers import AND_TABLE
+# Compile-time alias for SWIG proxy; runtime type is ida_hexrays.mop_t
+ctypedef object MopProxy
+# ===========================================================================
+# Cython Implementation
+# ===========================================================================
+
+
+# … existing imports …
+
+cdef inline mop_t* _borrow_mop_ptr(object py_mop):
+    """
+    Return the underlying C++ mop_t* from a SWIG proxy.
+
+    We cast through uintptr_t to silence Cython's safety check.
+    """
+    # Expect a SWIG ida_hexrays.mop_t proxy that carries the raw C++ pointer in its
+    # attribute "this" (exposed as a Python int).
+    cdef uintptr_t raw
+    try:
+        raw = <uintptr_t> py_mop.this  # type: ignore[attr-defined]
+    except AttributeError:
+        raise TypeError("expected ida_hexrays.mop_t proxy")
+    return <mop_t*> raw
+    
+# This is the public function called from Python.
+# It takes Python objects and casts them to C++ pointers.
+def process_operand_cy(op_py_obj, consts):
+    """Public entry: *op_py_obj* is the ida_hexrays.mop_t proxy."""
+    return _process_operand_impl(op_py_obj, consts)
+
+
+cdef bool _process_operand_impl(object root_py_mop, object consts):
+    """Iterative operand traversal working with Python proxy objects.
+
+    We maintain Python lists (`worklist`, `post_order`) containing mop_t
+    proxies.  The real C++ pointer is borrowed only when we need to access
+    struct fields, avoiding conversions back-and-forth.
+    """
+    cdef list worklist = [root_py_mop]
+    cdef list post_order = []
+    cdef MopProxy py_mop
+    cdef mop_t* op
+
+    cdef bool changed_anything = False
+    cdef bool current_op_changed
+    cdef str name
+    cdef unsigned long long val
+    cdef int op_size
+    cdef mop_t tmp  # reusable temporary instance
+
+    # 1. Build post-order list ------------------------------------------------
+    while worklist:
+        py_mop = worklist.pop()
+        post_order.append(py_mop)
+        op = _borrow_mop_ptr(py_mop)
+        if op.t == mop_d and py_mop.d is not None:
+            # children are themselves proxies accessible via attributes
+            worklist.append(py_mop.d.l)
+            worklist.append(py_mop.d.r)
+
+    # 2. Walk in reverse post-order ------------------------------------------
+    for py_mop in reversed(post_order):
+        op = _borrow_mop_ptr(py_mop)
+        current_op_changed = False
+
+        if op.t == mop_S or op.t == mop_r:
+            name = get_stack_var_name(py_mop)
+            if name is not None and name in consts:
+                val, _ = consts[name]
+                op_size = op.size
+                tmp.make_number(val & AND_TABLE[op_size], op_size, 0, 0)
+                op.assign(tmp)
+                current_op_changed = True
+
+        elif op.t == mop_d and py_mop.d is not None:
+            if py_mop.d.opcode == m_ldx:
+                addr_py = py_mop.d.r  # proxy for address operand
+                addr = _borrow_mop_ptr(addr_py)
+                const_info = None
+                if addr.t == mop_S:
+                    name = get_stack_var_name(addr_py)
+                    if name is not None and name in consts:
+                        const_info = consts[name]
+                else:
+                    base_py, off = extract_base_and_offset(addr_py)
+                    if base_py is not None:
+                        base_name = get_stack_var_name(base_py)
+                        name = f"{base_name}+{off:X}" if off else base_name
+                        if name in consts:
+                            const_info = consts[name]
+                if const_info is not None:
+                    val, _ = const_info
+                    op_size = op.size
+                    tmp.make_number(val & AND_TABLE[op_size], op_size, 0, 0)
+                    op.assign(tmp)
+                    current_op_changed = True
+            if changed_anything:
+                py_mop.d.optimize_solo()
+
+        if current_op_changed:
+            changed_anything = True
+
+    return changed_anything
diff --git a/src/d810/optimizers/microcode/flow/constant_prop/stackvars_constprop.py b/src/d810/optimizers/microcode/flow/constant_prop/stackvars_constprop.py
index eadd1f6..6216448 100644
--- a/src/d810/optimizers/microcode/flow/constant_prop/stackvars_constprop.py
+++ b/src/d810/optimizers/microcode/flow/constant_prop/stackvars_constprop.py
@@ -20,12 +20,7 @@ from d810.hexrays.cfg_utils import (
     get_stack_var_name,
     safe_verify,
 )
-from d810.hexrays.hexrays_formatters import (
-    format_minsn_t,
-    format_mop_t,
-    opcode_to_string,
-    sanitize_ea,
-)
+from d810.hexrays.hexrays_formatters import maturity_to_string
 from d810.hexrays.hexrays_helpers import AND_TABLE
 from d810.optimizers.microcode.flow.handler import FlowOptimizationRule
 
@@ -33,15 +28,14 @@ logger = getLogger(__name__)
 
 ConstMap = dict[str, tuple[int, int]]  # var  -> (value, size)
 
+import weakref
+
 
 class StackVariableConstantPropagationRule(FlowOptimizationRule):
     """Forward constant propagation for stack variables (whole function)."""
 
     DESCRIPTION = "Fold stack variables that are assigned constant values across the whole function"
 
-    # Opcodes whose *operands* we are willing to fold to constants.  The list is
-    # **not** used for KILL/GEN decisions – those depend on the actual destination
-    # operand, not on the opcode.
     ALLOW_PROPAGATION_OPCODES: set[int] = {
         ida_hexrays.m_stx,
         ida_hexrays.m_mov,
@@ -88,109 +82,206 @@ class StackVariableConstantPropagationRule(FlowOptimizationRule):
 
     def __init__(self):
         super().__init__()
-        self._done_funcs: set[tuple[int, int]] = set()
-        # run when SSA names are fixed but before aggressive global opts
         self.maturities = [ida_hexrays.MMAT_CALLS]
+        self.cython_enabled = True
+        self._seen = weakref.WeakKeyDictionary()  # mba -> last_maturity_run
+
+    @_compat.override
+    def configure(self, kwargs):
+        super().configure(kwargs)
+        self.cython_enabled = kwargs.get("cython_enabled", True)
 
     @_compat.override
     def optimize(self, blk: ida_hexrays.mblock_t):
-        logger.debug("Optimizing block %d", blk.serial)
         if self.current_maturity not in self.maturities:
-            logger.debug(
-                "Skipping block %d, maturity %d", blk.serial, self.current_maturity
-            )
+            if logger.debug_on:
+                logger.debug(
+                    "maturity is %s (%d), expecting one of: %s",
+                    maturity_to_string(self.current_maturity),
+                    self.current_maturity,
+                    ", ".join(map(maturity_to_string, self.maturities)),
+                )
             return 0
         mba = blk.mba
         if mba is None:
+            if logger.debug_on:
+                logger.debug("Block %d has no mba", blk.serial)
             return 0
+
+        # Run once per function per maturity; only from block 0
+        last = self._seen.get(mba)
+        if last == self.current_maturity:
+            if logger.debug_on:
+                logger.debug(
+                    "Skipping previous run of block %d, maturity %s (%d)",
+                    blk.serial,
+                    maturity_to_string(self.current_maturity),
+                    self.current_maturity,
+                )
+            return 0
+        if blk.serial != 1:
+            if logger.debug_on:
+                logger.debug(
+                    "Skipping, this block serial is: %d, expecting 1, maturity %s (%d)",
+                    blk.serial,
+                    maturity_to_string(self.current_maturity),
+                    self.current_maturity,
+                )
+            return 0
+        if logger.debug_on:
+            logger.debug(
+                "Running %s analysis on block %d, maturity %s (%d)",
+                self.__class__.__name__,
+                blk.serial,
+                maturity_to_string(self.current_maturity),
+                self.current_maturity,
+            )
         nb_changes = self._run_on_function(mba)
+        self._seen[mba] = self.current_maturity  # remember we've run
         return nb_changes
+        # if logger.debug_on:
+        #     logger.debug("Running dataflow analysis on block %d", blk.serial)
+        # nb_changes = self._run_on_function(mba)
+        # if logger.debug_on:
+        #     logger.debug("Block %d has %d changes", blk.serial, nb_changes)
+        # return nb_changes
 
     def _run_on_function(self, mba: ida_hexrays.mba_t) -> int:
         """
         Performs dataflow analysis and then rewrites the function.
-
-        This function uses a fixed-point iteration strategy for rewriting. This is
-        the standard, safe way to handle optimizers that can delete or replace
-        the instruction being worked on (e.g., via `optimize_solo`), which
-        invalidates simple instruction list iterators.
+        This is a single call to a highly optimized Cython function.
         """
-        # Phase A: Dataflow analysis to find where constants are known at the
-        # start of each block.
-        IN, _ = self._run_dataflow(mba)
+        if not self.cython_enabled:
+            # Fallback to the slower, pure-Python implementation if Cython is disabled
+            return self._slow_run_on_function(mba)
+
+        try:
+            from . import _fast_dataflow
+
+            total_changes = _fast_dataflow.cy_run_full_pass(mba)
+        except ImportError:
+            logger.warning(
+                "Cython module `_fast_dataflow` not found. Falling back to slow Python implementation."
+            )
+            self.cython_enabled = False
+            return self._slow_run_on_function(mba)
+
+        if total_changes > 0:
+            safe_verify(mba, "rewriting", logger_func=logger.error)
+
+        return total_changes
+
+    def _run_dataflow(self, mba: ida_hexrays.mba_t):
+        logger.debug("Running dataflow analysis")
+        if self.cython_enabled:
+            try:
+                from . import _fast_dataflow
+
+                return _fast_dataflow.run_dataflow_cython(mba)
+            except ImportError:
+                logger.warning(
+                    "Cython module `_fast_dataflow` not found. Falling back to slow Python implementation."
+                )
+                self.cython_enabled = False
+        return self._slow_dataflow(mba)
+
+    def _slow_run_on_function(self, mba: ida_hexrays.mba_t) -> int:
+        """The pure Python implementation of the analysis and rewrite pass."""
+        IN, _ = self._slow_dataflow(mba)
+        if not IN:
+            return 0
 
         total_changes = 0
-        # Phase B: Iterate over each block and apply optimizations until the
-        # block is stable (reaches a fixed point).
         curr_blk: ida_hexrays.mblock_t = mba.get_mblock(0)
         while curr_blk:
             block_was_changed = False
-            # Fixed-point loop for the current block.
             while True:
-                # The local constant map MUST be re-initialized inside the
-                # fixed-point loop. A destructive rewrite invalidates the
-                # previous local dataflow analysis, so we must start fresh.
                 consts: ConstMap = IN[curr_blk.serial].copy()
                 if logger.debug_on and consts:
-                    logger.debug(
-                        "[stack-var-cprop] constant map before blk %d: %s",
-                        curr_blk.serial,
-                        consts,
-                    )
+                    logger.debug("[cprop] IN for blk %d: %s", curr_blk.serial, consts)
+
                 made_change_this_pass = False
                 ins = curr_blk.head
                 while ins:
-                    # Attempt to rewrite the current instruction.
-                    if self._rewrite_instruction(ins, consts) > 0:
+                    # FIX: Pass the 'mba' object down to the helper functions.
+                    if self._slow_rewrite_instruction(mba, ins, consts) > 0:
                         total_changes += 1
                         made_change_this_pass = True
                         block_was_changed = True
-                        # A destructive change was made. The instruction list
-                        # is now potentially invalid. We must break this inner
-                        # loop and restart the scan from the block's head.
                         break
 
-                    # If no rewrite happened, update the local constant map
-                    # with the effects of the current instruction.
-                    self._transfer_single(ins, consts)
+                    # FIX: Pass the 'mba' object down to the helper functions.
+                    self._slow_transfer_single(mba, ins, consts)
                     ins = ins.next
 
                 if not made_change_this_pass:
-                    # We completed a full pass over the block with no changes.
-                    # The block is stable, so we can exit the fixed-point loop.
                     break
 
-            # If any instruction in the block was changed, its use/def lists
-            # are now invalid. We must mark them as dirty so the decompiler
-            # knows to recompute them. This is the fix for INTERR 50873.
             if block_was_changed:
                 curr_blk.mark_lists_dirty()
-
             curr_blk = curr_blk.nextb
 
         if total_changes > 0:
             mba.mark_chains_dirty()
             mba.optimize_local(0)
-            safe_verify(mba, "rewriting", logger_func=logger.error)
-
         return total_changes
 
-    # ------------------------------------------------------------------
-    # Phase A – classic forward data-flow (GEN/KILL)
-    # ------------------------------------------------------------------
+    def _rewrite_instruction(
+        self, mba: ida_hexrays.mba_t, ins: ida_hexrays.minsn_t, env: ConstMap
+    ) -> int:
+        if self.cython_enabled:
+            return self._fast_rewrite_instruction(mba, ins, env)
+        else:
+            return self._slow_rewrite_instruction(mba, ins, env)
 
-    def _run_dataflow(self, mba: ida_hexrays.mba_t):
+    def _transfer_single(
+        self, mba: ida_hexrays.mba_t, ins: ida_hexrays.minsn_t, env: ConstMap
+    ):
+        if self.cython_enabled:
+            self._fast_transfer_single(mba, ins, env)
+        else:
+            self._slow_transfer_single(mba, ins, env)
+
+    def _fast_rewrite_instruction(
+        self, mba: ida_hexrays.mba_t, ins: ida_hexrays.minsn_t, env: ConstMap
+    ) -> int:
+        from . import _fast_dataflow
+
+        if ins.opcode not in self.ALLOW_PROPAGATION_OPCODES:
+            return 0
+
+        return _fast_dataflow.cy_rewrite_instruction(ins, env)
+
+    def _fast_transfer_single(
+        self, mba: ida_hexrays.mba_t, ins: ida_hexrays.minsn_t, env: ConstMap
+    ):
+        from . import _fast_dataflow
+
+        # if ins.has_side_effects() and ins.opcode != ida_hexrays.m_stx:
+        #     env.clear()
+        #     return
+        if ins.is_unknown_call():  # or opcode-based check if needed
+            env.clear()
+            return
+        written_var = _fast_dataflow.cy_get_written_var_name(ins)
+        is_const_assign = _fast_dataflow.cy_is_constant_stack_assignment(ins)
+        if written_var and not is_const_assign and written_var in env:
+            del env[written_var]
+        if is_const_assign:
+            res = _fast_dataflow.cy_extract_assignment(ins)
+            if res:
+                var, val_size = res
+                if var:
+                    env[var] = val_size
+
+    def _slow_dataflow(self, mba: ida_hexrays.mba_t):
         nb = mba.qty
         IN: dict[int, ConstMap] = {i: {} for i in range(nb)}
         OUT: dict[int, ConstMap] = {i: {} for i in range(nb)}
-
         worklist: list[int] = list(range(nb))
-
-        preds: dict[int, list[int]] = {}
-        for i in range(nb):
-            blk: ida_hexrays.mblock_t = mba.get_mblock(i)
-            preds[i] = list(blk.predset)
-
+        preds: dict[int, list[int]] = {
+            i: list(mba.get_mblock(i).predset) for i in range(nb)
+        }
         while worklist:
             idx = worklist.pop()
             inm = self._meet([OUT[p] for p in preds[idx]]) if preds[idx] else {}
@@ -204,178 +295,120 @@ class StackVariableConstantPropagationRule(FlowOptimizationRule):
                         worklist.append(succ)
         return IN, OUT
 
-    # meet = intersection of keys where all values agree
     @staticmethod
     def _meet(pred_outs: list[ConstMap]) -> ConstMap:
-        """
-        Compute the meet (intersection) of constant maps coming from the
-        predecessors.
-
-        The previous implementation built *N* temporary ``set`` objects and an
-        additional ``set`` for the intersection result on **every** call – on
-        large functions with thousands of blocks this showed up as a hotspot in
-        the profiler (≈250 ms of a 635 ms pass in the example trace).
-
-        This optimised version avoids most of that overhead:
-        1. Early-out when there are 0 or 1 predecessors.
-        2. Iterate only over the keys of the *first* predecessor and compare the
-           value in the remaining maps.  This replaces costly ``set``
-           allocations with plain dictionary look-ups and short-circuiting.
-
-        For functions with few predecessors per block (the common case) the
-        runtime of ``_meet`` drops by roughly an order of magnitude.
-        """
         if not pred_outs:
             return {}
         if len(pred_outs) == 1:
-            # Fast-path: single predecessor – just copy its map.
             return dict(pred_outs[0])
-
-        first = pred_outs[0]
-        res: ConstMap = {}
+        first, res = pred_outs[0], {}
         for k, v in first.items():
-            for other in pred_outs[1:]:
-                if other.get(k) != v:
-                    break
-            else:  # no ``break`` means all predecessors agree on (k, v)
+            if all(other.get(k) == v for other in pred_outs[1:]):
                 res[k] = v
         return res
 
-    # transfer over whole block
     def _transfer_block(self, blk: ida_hexrays.mblock_t, in_map: ConstMap) -> ConstMap:
         env = dict(in_map)
         ins = blk.head
+        # FIX: Pass the mba from the block down to the slow transfer function.
+        mba = blk.mba
         while ins:
-            self._transfer_single(ins, env)
+            self._slow_transfer_single(mba, ins, env)
             ins = ins.next
         return env
 
-    # transfer for a single instruction (GEN/KILL)
-    def _transfer_single(self, ins: ida_hexrays.minsn_t, env: ConstMap):
-        # 1. Side-effects handling - for *imprecise* side-effecting instructions
-        # (e.g. calls) we must drop every tracked constant.
-        #
-        # A plain store (stx) is a *precise* write that we interpret below,
-        # so we exclude it from the blanket kill.
+    def _slow_transfer_single(
+        self, mba: ida_hexrays.mba_t, ins: ida_hexrays.minsn_t, env: ConstMap
+    ):
         if ins.has_side_effects() and ins.opcode != ida_hexrays.m_stx:
-            if env and logger.debug_on:
-                logger.debug(
-                    "[stack-var-cprop] KILL-ALL at %X due to %s",
-                    sanitize_ea(ins.ea),
-                    opcode_to_string(ins.opcode),
-                )
             env.clear()
-            # Nothing more to learn from this instruction.
             return
-
-        # 2. Determine written variable & apply precise KILL / GEN.
         written_var = self._get_written_var_name(ins)
         is_const_assign = self._is_constant_stack_assignment(ins)
-
-        # KILL when variable overwritten by non-constant value
         if written_var and not is_const_assign and written_var in env:
-            if logger.debug_on:
-                logger.debug(
-                    "[stack-var-cprop] KILL %s at %X via %s",
-                    written_var,
-                    sanitize_ea(ins.ea),
-                    opcode_to_string(ins.opcode),
-                )
             del env[written_var]
-
-        # 3. GEN - introduce new constant
         if is_const_assign:
             res = self._extract_assignment(ins)
-            if res:
-                var, val_size = res
-                if var:
-                    env[var] = val_size
-                    if logger.debug_on:
-                        logger.debug(
-                            "[stack-var-cprop] GEN %s = 0x%X at %X",
-                            var,
-                            val_size[0],
-                            sanitize_ea(ins.ea),
-                        )
+            if res and res[0]:
+                env[res[0]] = res[1]
 
-    # ------------------------------------------------------------------
-    # Phase B – rewrite helpers
-    # ------------------------------------------------------------------
-
-    def _rewrite_instruction(self, ins: ida_hexrays.minsn_t, env: ConstMap) -> int:
+    def _slow_rewrite_instruction(
+        self, mba: ida_hexrays.mba_t, ins: ida_hexrays.minsn_t, env: ConstMap
+    ) -> int:
         if ins.opcode not in self.ALLOW_PROPAGATION_OPCODES:
             return 0
-
-        # We must process one operand, and if it changes, optimize and exit
-        # immediately. Calling `optimize_solo()` can invalidate the `ins`
-        # object, so we cannot continue to access its other operands like
-        # `ins.r`.
-
         changed = False
-        # left operand
-        if ins.l and self._process_operand(ins.l, env):
+        if ins.l and self._slow_process_operand(ins.l, env):
             changed = True
-        # right operand for binary ops
-        if ins.r and self._process_operand(ins.r, env):
+        if ins.r and self._slow_process_operand(ins.r, env):
             changed = True
-        # stx destination address is also an input
         if (
             ins.opcode == ida_hexrays.m_stx
             and ins.d
-            and self._process_operand(ins.d, env)
+            and self._slow_process_operand(ins.d, env)
         ):
             changed = True
-
         if not changed:
             return 0
-
-        # Ensure the instruction is internally consistent after we rewrote its operands.
-        # An operand was changed to a constant. Let Hex-Rays recompute internal
-        # metadata for this instruction.
-        if logger.debug_on:
-            old_repr = format_minsn_t(ins)
         ins.optimize_solo()
-        if logger.debug_on:
-            logger.debug(
-                "[stack-var-cprop] optimized insn at %X: %s -> %s",
-                sanitize_ea(ins.ea),
-                old_repr,
-                format_minsn_t(ins),
-            )
         return 1
 
-    # ------------------------------------------------------------------
-    # Helper utilities
-    # ------------------------------------------------------------------
+    def _slow_process_operand(self, op: ida_hexrays.mop_t, consts: ConstMap) -> bool:
+        changed = False
+        if op.t in {ida_hexrays.mop_S, ida_hexrays.mop_r}:
+            name = get_stack_var_name(op)
+            if name and name in consts:
+                val, _ = consts[name]
+                op.make_number(val & ((1 << (op.size * 8)) - 1), op.size)
+                return True
+        elif op.t == ida_hexrays.mop_f and op.f is not None:
+            for a in op.f.args:
+                if a and self._slow_process_operand(a, consts):
+                    changed = True
+        elif op.t == ida_hexrays.mop_d and op.d is not None:
+            if op.d.opcode == ida_hexrays.m_ldx:
+                addr = op.d.r
+                const_info, name = None, None
+                if addr and addr.t == ida_hexrays.mop_S:
+                    name = get_stack_var_name(addr)
+                    if name and name in consts:
+                        const_info = consts[name]
+                else:
+                    base, off = extract_base_and_offset(addr)
+                    if base:
+                        base_name = get_stack_var_name(base)
+                        name = f"{base_name}+{off:X}" if off else base_name
+                        if name in consts:
+                            const_info = consts[name]
+                if const_info:
+                    val, _ = const_info
+                    tmp = ida_hexrays.mop_t()
+                    tmp.make_number(val & AND_TABLE[op.size], op.size)
+                    op.assign(tmp)
+                    return True
+            for attr in ("l", "r"):
+                sub = getattr(op.d, attr, None)
+                if sub and self._slow_process_operand(sub, consts):
+                    changed = True
+            if changed:
+                op.d.optimize_solo()
+        return changed
 
-    # conservative wiping helper
-    def _kill_all_stack_vars(self, env: ConstMap, reason: str, ea: int):
-        prefixes = ("%var_", "stk_", "rsp", "rbp", "esp", "ebp")
-        to_del = [k for k in env if k.startswith(prefixes)]
-        if to_del and logger.debug_on:
-            logger.debug(
-                "[stack-var-cprop] KILL-ALL at %X (%s): %s", ea, reason, to_del
-            )
-        for k in to_del:
-            del env[k]
-
-    # identify destination variable of an instruction (None if unknown)
     def _get_written_var_name(self, ins: ida_hexrays.minsn_t):
-        if ins.d is None:
+        d = ins.d
+        if d is None:
             return None
-        if ins.d.t in {ida_hexrays.mop_S, ida_hexrays.mop_r}:
-            return get_stack_var_name(ins.d)
-        if ins.opcode == ida_hexrays.m_stx:
-            if ins.d.t == ida_hexrays.mop_S:
-                return get_stack_var_name(ins.d)
-            base, off = extract_base_and_offset(ins.d)
-            if base is not None:
-                base_name = get_stack_var_name(base)
-                if base_name:
-                    return f"{base_name}+{off:X}" if off else base_name
+        if d.t in {ida_hexrays.mop_S, ida_hexrays.mop_r}:
+            return get_stack_var_name(d)
+        if ins.opcode != ida_hexrays.m_stx:
+            return None
+        if d.t == ida_hexrays.mop_S:
+            return get_stack_var_name(d)
+        base, off = extract_base_and_offset(d)
+        if base and (base_name := get_stack_var_name(base)):
+            return f"{base_name}+{off:X}" if off else base_name
         return None
 
-    # is instruction a constant store into stack?
     def _is_constant_stack_assignment(self, ins: ida_hexrays.minsn_t):
         if ins.l is None or ins.l.t != ida_hexrays.mop_n:
             return False
@@ -392,160 +425,17 @@ class StackVariableConstantPropagationRule(FlowOptimizationRule):
             return base is not None
         return False
 
-    # extract (var,(value,size)) for constant assignment
     def _extract_assignment(self, ins: ida_hexrays.minsn_t):
         if not self._is_constant_stack_assignment(ins):
             return None
-        value = ins.l.nnn.value  # type: ignore[attr-defined]
-        size = ins.l.size  # type: ignore[attr-defined]
+        value, size = ins.l.nnn.value, ins.l.size
+        var = None
         if ins.opcode == ida_hexrays.m_mov:
             var = get_stack_var_name(ins.d)
-            return var, (value, size)
-        # stx / mov forms
-        if ins.d.t == ida_hexrays.mop_S:
+        elif ins.d.t in {ida_hexrays.mop_S, ida_hexrays.mop_r}:
             var = get_stack_var_name(ins.d)
-            return var, (value, size)
-        if ins.d.t == ida_hexrays.mop_r:
-            var = get_stack_var_name(ins.d)
-            return var, (value, size)
-        base, off = extract_base_and_offset(ins.d)
-        if base is not None:
-            var_base = get_stack_var_name(base)
-            if var_base:
-                comp = f"{var_base}+{off:X}" if off else var_base
-                return comp, (value, size)
-        return None
-
-    def _process_operand(self, op: ida_hexrays.mop_t, consts: ConstMap):
-        changed = False
-        if op.t == ida_hexrays.mop_S:
-            name = get_stack_var_name(op)
-            if name and name in consts:
-                val, _ = consts[name]
-                op.make_number(val & ((1 << (op.size * 8)) - 1), op.size)
-                return True
-        elif op.t == ida_hexrays.mop_r:
-            name = get_stack_var_name(op)
-            if name and name in consts:
-                val, _ = consts[name]
-                op.make_number(val & ((1 << (op.size * 8)) - 1), op.size)
-                return True
-        elif op.t == ida_hexrays.mop_f and op.f is not None:
-            for a in op.f.args:
-                if a and self._process_operand(a, consts):
-                    changed = True
-        elif op.t == ida_hexrays.mop_d and op.d is not None:
-            if logger.debug_on:
-                logger.debug(
-                    "[stack-var-cprop] mop_d: considering nested insn at ea=0x%X, opcode=%s, l=%s, r=%s, d=%s",
-                    sanitize_ea(op.d.ea),
-                    opcode_to_string(op.d.opcode),
-                    format_mop_t(op.d.l) if op.d.l else "",
-                    format_mop_t(op.d.r) if op.d.r else "",
-                    format_mop_t(op.d.d) if op.d.d else "",
-                )
-            # If a nested instruction is a load from a known constant location,
-            # replace the entire load operation (`mop_d`) with the constant value
-            # itself (`mop_n`).
-            if op.d.opcode == ida_hexrays.m_ldx:
-                # For `ldx`, the address is in the right operand 'r'.
-                addr = op.d.r
-                if logger.debug_on:
-                    logger.debug(
-                        "[stack-var-cprop] mop_d: found load (ldx/ldc), addr=%s",
-                        format_mop_t(addr),
-                    )
-                const_info = None
-                name = None
-                # Case 1: Direct stack variable access
-                if addr and addr.t == ida_hexrays.mop_S:
-                    name = get_stack_var_name(addr)
-                    if logger.debug_on:
-                        logger.debug(
-                            "[stack-var-cprop] mop_d: direct stack var access, name=%s, in consts=%s",
-                            name,
-                            name in consts if name else None,
-                        )
-                    if name and name in consts:
-                        const_info = consts[name]
-                        if logger.debug_on:
-                            logger.debug(
-                                "[stack-var-cprop] mop_d: folding direct stack var '%s' to constant 0x%X (size=%d)",
-                                name,
-                                const_info[0],
-                                const_info[1],
-                            )
-                else:
-                    # Case 2: Base + offset access
-                    base, off = extract_base_and_offset(addr)
-                    if base is not None:
-                        if logger.debug_on:
-                            logger.debug(
-                                "[stack-var-cprop] mop_d: base+off extraction: base=%s, off=%s",
-                                format_mop_t(base),
-                                off,
-                            )
-                        base_name = get_stack_var_name(base)
-                        name = f"{base_name}+{off:X}" if off else base_name
-                        if logger.debug_on:
-                            logger.debug(
-                                "[stack-var-cprop] mop_d: base+off access, comp=%s, in consts=%s",
-                                name,
-                                name in consts,
-                            )
-                        if name in consts:
-                            const_info = consts[name]
-                            if logger.debug_on:
-                                logger.debug(
-                                    "[stack-var-cprop] mop_d: folding base+off '%s' to constant 0x%X (size=%d)",
-                                    name,
-                                    const_info[0],
-                                    const_info[1],
-                                )
-                if const_info:
-                    val, _ = const_info
-                    # The size of the operand is the size of the mop_d itself.
-                    op_size = op.size
-
-                    tmp = ida_hexrays.mop_t()
-                    tmp.make_number(val & AND_TABLE[op_size], op_size)
-                    op.assign(tmp)
-
-                    if logger.debug_on:
-                        logger.debug(
-                            "[stack-var-cprop] mop_d: folded load '%s' -> #%X (size=%d): %s",
-                            name,
-                            val & AND_TABLE[op_size],
-                            op_size,
-                            format_mop_t(op),
-                        )
-                    return True
-
-            # If the load couldn't be resolved, recurse into its children.
-            changed = False
-            for attr in ("l", "r"):
-                sub = getattr(op.d, attr, None)
-                if logger.debug_on:
-                    logger.debug(
-                        "[stack-var-cprop] mop_d: recursing into child attr '%s': %s",
-                        attr,
-                        format_mop_t(sub) if sub else "",
-                    )
-                if sub is not None and self._process_operand(sub, consts):
-                    if logger.debug_on:
-                        logger.debug(
-                            "[stack-var-cprop] mop_d: child attr '%s' changed during recursion",
-                            attr,
-                        )
-                    changed = True
-            # ensure nested instruction is internally consistent after edits
-            if changed and op.t == ida_hexrays.mop_d and op.d is not None:
-                op.d.optimize_solo()
-            if logger.debug_on:
-                logger.debug(
-                    "[stack-var-cprop] mop_d: finished recursion, changed=%s, insn=%s",
-                    changed,
-                    format_mop_t(op),
-                )
-            return changed
-        return changed
+        else:
+            base, off = extract_base_and_offset(ins.d)
+            if base and (base_name := get_stack_var_name(base)):
+                var = f"{base_name}+{off:X}" if off else base_name
+        return (var, (value, size)) if var else None
diff --git a/src/d810/optimizers/microcode/flow/flattening/generic.py b/src/d810/optimizers/microcode/flow/flattening/generic.py
index a5f3722..c06f768 100644
--- a/src/d810/optimizers/microcode/flow/flattening/generic.py
+++ b/src/d810/optimizers/microcode/flow/flattening/generic.py
@@ -203,7 +203,9 @@ class GenericDispatcherInfo(object):
     def emulate_dispatcher_with_father_history(
         self, father_history: MopHistory
     ) -> tuple[mblock_t, list[minsn_t]]:
-        microcode_interpreter = MicroCodeInterpreter()
+        # Use concrete values from tracker - do NOT use symbolic mode here
+        # Symbolic mode would generate fake values instead of using tracked values
+        microcode_interpreter = MicroCodeInterpreter(symbolic_mode=False)
         microcode_environment = MicroCodeEnvironment()
         dispatcher_input_info = []
         # First, we setup the MicroCodeEnvironment with the state variables (self.entry_block.use_before_def_list)
@@ -354,7 +356,13 @@ class GenericDispatcherCollector(minsn_visitor_t):
             return 0
         self.explored_blk_serials.append(self.blk.serial)
         disp_info = self.DISPATCHER_CLASS(self.blk.mba)
-        is_good_candidate = disp_info.explore(self.blk)
+        # Pass entropy thresholds if available
+        kwargs = {}
+        if hasattr(self, "min_entropy"):
+            kwargs["min_entropy"] = self.min_entropy
+        if hasattr(self, "max_entropy"):
+            kwargs["max_entropy"] = self.max_entropy
+        is_good_candidate = disp_info.explore(self.blk, **kwargs)
         if not is_good_candidate:
             return 0
         if not self.specific_checks(disp_info):
diff --git a/src/d810/optimizers/microcode/flow/flattening/unflattener.py b/src/d810/optimizers/microcode/flow/flattening/unflattener.py
index 7abc24a..bc93589 100644
--- a/src/d810/optimizers/microcode/flow/flattening/unflattener.py
+++ b/src/d810/optimizers/microcode/flow/flattening/unflattener.py
@@ -113,7 +113,9 @@ class OllvmDispatcherInfo(GenericDispatcherInfo):
 
         return entropy
 
-    def explore(self, blk: mblock_t) -> bool:  # Detect dispatcher entry blocks
+    def explore(
+        self, blk: mblock_t, min_entropy=None, max_entropy=None
+    ) -> bool:  # Detect dispatcher entry blocks
         unflat_logger.debug(
             "mblock %s: exploring dispatcher (guessed outmost dispatcher %s)",
             blk.serial,
@@ -142,18 +144,26 @@ class OllvmDispatcherInfo(GenericDispatcherInfo):
             self._get_dispatcher_blocks_with_external_father()
         )
         # TODO: I think this can be wrong because we are too permissive in detection of dispatcher blocks
-        # if len(dispatcher_blk_with_external_father) != 0: # All internal blocks (except the entry block) should not have fathers outside the CFF loop
+        # if len(dispatcher_blk_with_external_father) != 0:
+        # All internal blocks (except the entry block) should not have fathers outside the CFF loop
         entropy = self.get_entropy(
             num_mop.size, blk.serial
         )  # additional check by entropy (only effective for O-LLVM)
+
+        # Use passed entropy thresholds or defaults
+        _min_entropy = min_entropy if min_entropy is not None else 0.3
+        _max_entropy = max_entropy if max_entropy is not None else 0.7
+
         if len(dispatcher_blk_with_external_father) != 0 or (
-            entropy < 0.3 or entropy > 0.7
+            entropy < _min_entropy or entropy > _max_entropy
         ):  # validate the comparison value's entropy
             unflat_logger.debug(
-                "mblock %s is excluded as a CFF dispatcher (%s, %f)",
+                "mblock %s is excluded as a CFF dispatcher (%s, entropy=%f not in [%f, %f])",
                 blk.serial,
                 len(dispatcher_blk_with_external_father),
                 entropy,
+                _min_entropy,
+                _max_entropy,
             )
             return False
         unflat_logger.debug(
@@ -251,6 +261,20 @@ class OllvmDispatcherCollector(GenericDispatcherCollector):
     DEFAULT_DISPATCHER_MIN_INTERNAL_BLOCK = 2
     DEFAULT_DISPATCHER_MIN_EXIT_BLOCK = 3
     DEFAULT_DISPATCHER_MIN_COMPARISON_VALUE = 2
+    DEFAULT_MIN_ENTROPY = 0.3
+    DEFAULT_MAX_ENTROPY = 0.7
+
+    def __init__(self):
+        super().__init__()
+        self.min_entropy = self.DEFAULT_MIN_ENTROPY
+        self.max_entropy = self.DEFAULT_MAX_ENTROPY
+
+    def configure(self, kwargs):
+        super().configure(kwargs)
+        if "min_entropy" in kwargs.keys():
+            self.min_entropy = kwargs["min_entropy"]
+        if "max_entropy" in kwargs.keys():
+            self.max_entropy = kwargs["max_entropy"]
 
 
 class Unflattener(GenericDispatcherUnflatteningRule):
diff --git a/src/d810/optimizers/microcode/flow/flattening/unflattener_badwhile_loop.py b/src/d810/optimizers/microcode/flow/flattening/unflattener_badwhile_loop.py
index e749ab2..04c79cc 100644
--- a/src/d810/optimizers/microcode/flow/flattening/unflattener_badwhile_loop.py
+++ b/src/d810/optimizers/microcode/flow/flattening/unflattener_badwhile_loop.py
@@ -1,3 +1,15 @@
+"""
+Unflattener for Bad While Loop
+TODO:
+
+# - Accept m_jz and m_jnz (plus support for == or != with non-zero constants).
+# - Allow the state register to be either l or r operand (code should not assume only l).
+# - Add an optional alias check: for example, if there is 'mov eax, tmp', look back for an earlier 'mov tmp, #CONST'.
+# - If possible, do not rely only on prevb/nextb; make sure selected exits are actual successors using the CFG (succset) to avoid incorrect matches after layout changes.
+# - Keep 'min_constant' and 'max_constant' as configuration options; these are important to filter out irrelevant matches.
+
+"""
+
 from ida_hexrays import *
 
 from d810.conf.loggers import getLogger
@@ -18,7 +30,7 @@ class BadWhileLoopBlockInfo(GenericDispatcherBlockInfo):
 
 
 class BadWhileLoopInfo(GenericDispatcherInfo):
-    def explore(self, blk: mblock_t) -> bool:
+    def explore(self, blk: mblock_t, min_constant=None, max_constant=None) -> bool:
         """
         ; 1WAY-BLOCK 13 [START=0000E1BE END=0000E1D0] STK=48/ARG=250, MAXBSP: 0
         ; - INBOUND: [12, 24, 25, 8] OUTBOUND: [14]
@@ -53,8 +65,16 @@ class BadWhileLoopInfo(GenericDispatcherInfo):
 
 
         """
+        # Use provided values or defaults (Approov obfuscator range)
+        if min_constant is None:
+            min_constant = 0xF6000
+        if max_constant is None:
+            max_constant = 0xF6FFF
+
         self.reset()
-        if not self._is_candidate_for_dispatcher_entry_block(blk):
+        if not self._is_candidate_for_dispatcher_entry_block(
+            blk, min_constant, max_constant
+        ):
             return False
         self.entry_block = BadWhileLoopBlockInfo(blk)
         self.mop_compared = blk.tail.l
@@ -69,16 +89,16 @@ class BadWhileLoopInfo(GenericDispatcherInfo):
             and blk.prevb != None
         ):
             right_cnst = blk.tail.r.signed_value()
-            if right_cnst > 0xF6000 and right_cnst < 0xF6FFF:
+            if right_cnst > min_constant and right_cnst < max_constant:
                 if blk.prevb.tail.opcode == m_mov and blk.prevb.tail.l.t == mop_n:
                     jz0_cnst = blk.prevb.tail.l.signed_value()
                     if blk.nextb.tail.opcode == m_jz and blk.nextb.tail.r.t == mop_n:
                         jz1_cnst = blk.nextb.tail.r.signed_value()
                         if (
-                            jz1_cnst > 0xF6000
-                            and jz1_cnst < 0xF6FFF
-                            and jz0_cnst > 0xF6000
-                            and jz0_cnst < 0xF6FFF
+                            jz1_cnst > min_constant
+                            and jz1_cnst < max_constant
+                            and jz0_cnst > min_constant
+                            and jz0_cnst < max_constant
                         ):
                             exit_block0 = BadWhileLoopBlockInfo(
                                 blk.mba.get_mblock(blk.nextb.tail.d.b), self.entry_block
@@ -99,7 +119,7 @@ class BadWhileLoopInfo(GenericDispatcherInfo):
 
         return True
 
-    def _is_candidate_for_dispatcher_entry_block(self, blk):
+    def _is_candidate_for_dispatcher_entry_block(self, blk, min_constant, max_constant):
         if (
             blk.tail.opcode == m_jz
             and blk.tail.r.t == mop_n
@@ -107,16 +127,16 @@ class BadWhileLoopInfo(GenericDispatcherInfo):
             and blk.prevb != None
         ):
             right_cnst = blk.tail.r.signed_value()
-            if right_cnst > 0xF6000 and right_cnst < 0xF6FFF:
+            if right_cnst > min_constant and right_cnst < max_constant:
                 if blk.prevb.tail.opcode == m_mov and blk.prevb.tail.l.t == mop_n:
                     jz0_cnst = blk.prevb.tail.l.signed_value()
                     if blk.nextb.tail.opcode == m_jz and blk.nextb.tail.r.t == mop_n:
                         jz1_cnst = blk.nextb.tail.r.signed_value()
                         if (
-                            jz1_cnst > 0xF6000
-                            and jz1_cnst < 0xF6FFF
-                            and jz0_cnst > 0xF6000
-                            and jz0_cnst < 0xF6FFF
+                            jz1_cnst > min_constant
+                            and jz1_cnst < max_constant
+                            and jz0_cnst > min_constant
+                            and jz0_cnst < max_constant
                         ):
                             return True
         return False
@@ -133,6 +153,51 @@ class BadWhileLoopCollector(GenericDispatcherCollector):
     DEFAULT_DISPATCHER_MIN_INTERNAL_BLOCK = 1
     DEFAULT_DISPATCHER_MIN_EXIT_BLOCK = 3
     DEFAULT_DISPATCHER_MIN_COMPARISON_VALUE = 3
+    DEFAULT_MIN_CONSTANT = 0xF6000
+    DEFAULT_MAX_CONSTANT = 0xF6FFF
+
+    def __init__(self):
+        super().__init__()
+        self.min_constant = self.DEFAULT_MIN_CONSTANT
+        self.max_constant = self.DEFAULT_MAX_CONSTANT
+
+    def configure(self, kwargs):
+        super().configure(kwargs)
+        if "min_constant" in kwargs:
+            self.min_constant = kwargs["min_constant"]
+            unflat_logger.debug(
+                "BadWhileLoopCollector: min_constant set to 0x%X", self.min_constant
+            )
+        if "max_constant" in kwargs:
+            self.max_constant = kwargs["max_constant"]
+            unflat_logger.debug(
+                "BadWhileLoopCollector: max_constant set to 0x%X", self.max_constant
+            )
+
+    def visit_minsn(self):
+        """Override to pass min/max constant parameters to explore."""
+
+        if self.blk.serial in self.explored_blk_serials:
+            return 0
+        self.explored_blk_serials.append(self.blk.serial)
+        if self.curins.opcode not in FLATTENING_JUMP_OPCODES:
+            return 0
+        disp_info = self.DISPATCHER_CLASS(self.blk.mba)
+
+        # Pass constants as kwargs
+        kwargs = {}
+        if hasattr(self, "min_constant"):
+            kwargs["min_constant"] = self.min_constant
+        if hasattr(self, "max_constant"):
+            kwargs["max_constant"] = self.max_constant
+
+        is_good_candidate = disp_info.explore(self.blk, **kwargs)
+        if not is_good_candidate:
+            return 0
+        if not self.specific_checks(disp_info):
+            return 0
+        self.dispatcher_list.append(disp_info)
+        return 0
 
 
 class BadWhileLoop(GenericDispatcherUnflatteningRule):
@@ -145,3 +210,14 @@ class BadWhileLoop(GenericDispatcherUnflatteningRule):
     def DISPATCHER_COLLECTOR_CLASS(self) -> type[GenericDispatcherCollector]:
         """Return the class of the dispatcher collector."""
         return BadWhileLoopCollector
+
+
+
+"""
+# BadWhileLoop recognizes a very specific "Approov-style" dispatcher head by looking for:
+#   - a jz on a magic constant,
+#   - a previous mov #magic, eax,
+#   - a next jz on another magic constant,
+# and then it collects 3 exits from (next jz target, next fall-through, previous block).
+# The generic unflattening framework then uses those to rewire the CFG and remove the flattened while loop.
+"""
diff --git a/src/d810/optimizers/microcode/flow/flattening/unflattener_cf.py b/src/d810/optimizers/microcode/flow/flattening/unflattener_cf.py
index 9778c26..cc11f0c 100644
--- a/src/d810/optimizers/microcode/flow/flattening/unflattener_cf.py
+++ b/src/d810/optimizers/microcode/flow/flattening/unflattener_cf.py
@@ -14,7 +14,7 @@ import ida_xref
 
 from d810.conf.loggers import getLogger
 from d810.hexrays.cfg_utils import safe_verify
-from d810.hexrays.hexrays_formatters import format_minsn_t
+from d810.hexrays.hexrays_formatters import format_minsn_t, format_mop_t
 from d810.hexrays.hexrays_helpers import MicrocodeHelper, MicroInstruction, MicroOperand
 from d810.optimizers.microcode.flow.handler import FlowOptimizationRule
 
@@ -678,13 +678,14 @@ class switch_state_collector_t(ida_hexrays.minsn_visitor_t):
             for msin in reversed(self.xdu_map):
                 dest = msin.destination_operand
                 src = msin.left_operand
-                logger.debug(
-                    "%s, %s, %s, %s",
-                    msin,
-                    hex(msin.ea),
-                    dest.dstr() if dest else "None",
-                    src.dstr() if src else "None",
-                )
+                if logger.debug_on:
+                    logger.debug(
+                        "%s, %s, %s, %s",
+                        msin,
+                        hex(msin.ea),
+                        dest.dstr() if dest else "None",
+                        src.dstr() if src else "None",
+                    )
                 if dest and dest.equal_mops(idx_mop, ida_hexrays.EQ_IGNSIZE):
                     state_var = src
                     break
@@ -1155,7 +1156,8 @@ class cf_flatten_info_t:
             self.plugin.white_list.append(ea)
 
         op_max = switch_tbl_collector.switches[0].state_var
-        logger.info(f"Comparison variable = {op_max.dstr()}")
+        if op_max and logger.debug_on:
+            logger.debug("Comparison variable = %s", op_max.dstr())
         # op_max is our "comparison" variable used in the control flow switch.
         # if op_max.size < 4:
         #     self.report_error(f"Comparison variable {op_max.dstr()} is too narrow\n")
@@ -1166,7 +1168,7 @@ class cf_flatten_info_t:
         # control flow switch.
         ok, self.mb_first, self.first, self.dispatch = get_first_block(mba)
         if not ok:
-            logger.error(f"Failed determining the first block")
+            logger.error("Failed determining the first block")
             return False
 
         assert self.mb_first
@@ -1189,7 +1191,8 @@ class cf_flatten_info_t:
         # Was the comparison variable assigned a number in the first block?
         found = False
         for sas in fbe.seen_assignments:
-            logger.info(f"sas[0] = {sas[0].dstr()}")
+            if logger.debug_on:
+                logger.debug("sas[0] = %s", sas[0].dstr())
             if sas[0].equal_mops(op_max, ida_hexrays.EQ_IGNSIZE):
                 found = True
                 break
@@ -1265,7 +1268,10 @@ class cf_flatten_info_t:
             ):
                 continue
             assert swi.cases is not None
-            logger.info(f"[+] Found jump-table for {op_max.dstr()}, importing cases")
+            if logger.debug_on:
+                logger.debug(
+                    "[+] Found jump-table for %s, importing cases", op_max.dstr()
+                )
             num_imported = 0
 
             vals: ida_xref.casevec_t = swi.cases.values  # casevec_t of the keys
@@ -1347,9 +1353,12 @@ class assign_searcher_t(ida_hexrays.minsn_visitor_t):
         ida_hexrays.minsn_visitor_t.__init__(self)
         self.op = op
         self.dispatcher_reg = dispatcher_reg
-        logger.info(
-            f"Initiated assign_searcher_t, op = {self.op.dstr()}, dispatcher_reg = {self.dispatcher_reg.dstr()}"
-        )
+        if logger.debug_on:
+            logger.debug(
+                "Initiated assign_searcher_t, op = %s, dispatcher_reg = %s",
+                self.op.dstr(),
+                self.dispatcher_reg.dstr(),
+            )
         self.jz_target_block = -1
         self.hits = []
         self.assign_infos = []
diff --git a/src/d810/optimizers/microcode/flow/flattening/utils.py b/src/d810/optimizers/microcode/flow/flattening/utils.py
index 2735f0d..56ec653 100644
--- a/src/d810/optimizers/microcode/flow/flattening/utils.py
+++ b/src/d810/optimizers/microcode/flow/flattening/utils.py
@@ -2,8 +2,9 @@ import logging
 
 from d810.conf.loggers import getLogger
 
-tracker_logger = getLogger("D810.tracker")
-emulator_logger = getLogger("D810.emulator")
+# TODO: this doesn't belong in this module, move it a different module
+tracker_logger = getLogger("d810.expr.tracker")
+emulator_logger = getLogger("d810.expr.emulator")
 
 
 class UnflatteningException(Exception):
@@ -22,6 +23,7 @@ class NotResolvableFatherException(UnflatteningException):
     pass
 
 
+# TODO: this doesn't belong in this module, move it a different module
 def configure_mop_tracker_log_verbosity(verbose=False):
     tracker_log_level = tracker_logger.getEffectiveLevel()
     emulator_log_level = emulator_logger.getEffectiveLevel()
@@ -36,6 +38,7 @@ def restore_mop_tracker_log_verbosity(tracker_log_level, emulator_log_level):
     emulator_logger.setLevel(emulator_log_level)
 
 
+# TODO: this doesn't belong in this module, move it a different module
 def get_all_possibles_values(mop_histories, searched_mop_list, verbose=False):
     log_levels = configure_mop_tracker_log_verbosity(verbose)
     mop_cst_values_list = []
diff --git a/src/d810/optimizers/microcode/flow/jumps/handler.py b/src/d810/optimizers/microcode/flow/jumps/handler.py
index cabd140..c664e5a 100644
--- a/src/d810/optimizers/microcode/flow/jumps/handler.py
+++ b/src/d810/optimizers/microcode/flow/jumps/handler.py
@@ -116,16 +116,22 @@ class JumpOptimizationRule(Registrant):
     ):
         if instruction.opcode not in self.ORIGINAL_JUMP_OPCODES:
             return None
+        if instruction.d is None or instruction.d.t != mop_b:
+            return None
         self.jump_original_block_serial = instruction.d.b
-        self.direct_block_serial = blk.serial + 1
+        if blk.nextb is None:
+            return None  # or bail gracefully
+        self.direct_block_serial = blk.nextb.serial
         self.jump_replacement_block_serial = None
         valid_candidates = self.get_valid_candidates(
             instruction, left_ast, right_ast, stop_early=True
         )
         if len(valid_candidates) == 0:
             return None
-        if self.jump_original_block_serial is None:
-            self.jump_replacement_block_serial = self.jump_original_block_serial
+        # if self.jump_original_block_serial is None:
+        #     self.jump_replacement_block_serial = self.jump_original_block_serial
+        if self.jump_original_block_serial is None and self.direct_block_serial is None:
+            return None
         left_candidate, right_candidate = valid_candidates[0]
         new_ins = self.get_replacement(instruction, left_candidate, right_candidate)
         return new_ins
@@ -244,10 +250,22 @@ class JumpFixer(FlowOptimizationRule):
                     optimizer_logger.info("  new : {0}".format(format_minsn_t(new_ins)))
                     if new_ins.opcode == m_goto:
                         make_2way_block_goto(blk, new_ins.d.b)
+                        return True
                     else:
-                        change_2way_block_conditional_successor(blk, new_ins.d.b)
-                        blk.make_nop(blk.tail)
-                        blk.insert_into_block(new_ins, blk.tail)
+                        # old:
+                        # change_2way_block_conditional_successor(blk, new_ins.d.b)
+                        # blk.make_nop(blk.tail)
+                        # blk.insert_into_block(new_ins, blk.tail)
+                        # new:
+                        # either:
+                        tail = blk.tail
+                        tail.opcode = new_ins.opcode
+                        tail.l = new_ins.l
+                        tail.r = new_ins.r
+                        tail.d = new_ins.d
+                        # or:
+                        # blk.make_nop(blk.tail)
+                        # blk.insert_into_block(new_ins, blk.tail)
                         return True
             except RuntimeError as e:
                 optimizer_logger.error(
diff --git a/src/d810/optimizers/microcode/flow/jumps/opaque.py b/src/d810/optimizers/microcode/flow/jumps/opaque.py
index dba5024..fe70ce9 100644
--- a/src/d810/optimizers/microcode/flow/jumps/opaque.py
+++ b/src/d810/optimizers/microcode/flow/jumps/opaque.py
@@ -1,9 +1,9 @@
+from ida_hexrays import *
+
 from d810.expr.ast import AstConstant, AstLeaf, AstNode
 from d810.expr.z3_utils import z3_check_mop_equality, z3_check_mop_inequality
 from d810.optimizers.microcode.flow.jumps.handler import JumpOptimizationRule
 
-from ida_hexrays import *
-
 
 class JnzRule1(JumpOptimizationRule):
     ORIGINAL_JUMP_OPCODES = [m_jnz, m_jz]
@@ -123,7 +123,7 @@ class JnzRule7(JumpOptimizationRule):
 
 class JnzRule8(JumpOptimizationRule):
     ORIGINAL_JUMP_OPCODES = [m_jnz, m_jz]
-    PATTERN = AstNode(m_or, AstLeaf("x_0"), AstConstant("c_1"))
+    LEFT_PATTERN = AstNode(m_or, AstLeaf("x_0"), AstConstant("c_1"))
     RIGHT_PATTERN = AstConstant("c_2")
     REPLACEMENT_OPCODE = m_goto
 
@@ -141,7 +141,7 @@ class JnzRule8(JumpOptimizationRule):
 
 class JbRule1(JumpOptimizationRule):
     ORIGINAL_JUMP_OPCODES = [m_jb]
-    PATTERN = AstNode(m_xdu, AstNode(m_and, AstLeaf("x_0"), AstConstant("1", 1)))
+    LEFT_PATTERN = AstNode(m_xdu, AstNode(m_and, AstLeaf("x_0"), AstConstant("1", 1)))
     RIGHT_PATTERN = AstConstant("2", 2)
     REPLACEMENT_OPCODE = m_goto
 
@@ -152,7 +152,7 @@ class JbRule1(JumpOptimizationRule):
 
 class JaeRule1(JumpOptimizationRule):
     ORIGINAL_JUMP_OPCODES = [m_jae]
-    PATTERN = AstNode(m_and, AstLeaf("x_0"), AstConstant("c_1"))
+    LEFT_PATTERN = AstNode(m_and, AstLeaf("x_0"), AstConstant("c_1"))
     RIGHT_PATTERN = AstConstant("c_2")
     REPLACEMENT_OPCODE = m_goto
 
diff --git a/src/d810/optimizers/microcode/flow/loops/__init__.py b/src/d810/optimizers/microcode/flow/loops/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/src/d810/optimizers/microcode/flow/loops/bogus_loops.py b/src/d810/optimizers/microcode/flow/loops/bogus_loops.py
new file mode 100644
index 0000000..11e6a22
--- /dev/null
+++ b/src/d810/optimizers/microcode/flow/loops/bogus_loops.py
@@ -0,0 +1,323 @@
+"""
+Bogus Single-Iteration Loop Removal
+
+This handler detects and removes obfuscation loops that are designed to execute
+exactly once. The pattern is:
+
+    for (i = 0; !i; i = 1) {
+        // body
+    }
+
+Which in microcode becomes:
+    1. Initialize counter to constant C1 (e.g., 0)
+    2. Check if counter != C1, if so exit loop
+    3. Execute loop body
+    4. Set counter to different constant C2 (e.g., 1)
+    5. Jump back to check
+
+This always executes exactly once and can be safely unrolled.
+"""
+
+from ida_hexrays import *
+
+from d810.conf.loggers import getLogger
+from d810.hexrays.cfg_utils import (
+    change_1way_block_successor,
+    make_2way_block_goto,
+    mba_deep_cleaning,
+)
+from d810.hexrays.hexrays_helpers import CONDITIONAL_JUMP_OPCODES
+from d810.optimizers.microcode.flow.handler import FlowOptimizationRule
+
+logger = getLogger("D810.bogus_loop_remover")
+
+
+class BogusLoopInfo:
+    """Information about a detected bogus single-iteration loop."""
+
+    def __init__(self):
+        self.init_block = None  # Block that initializes the counter
+        self.check_block = None  # Block that checks the counter
+        self.body_blocks = []  # Blocks in the loop body
+        self.exit_block = None  # Block after the loop exits
+        self.counter_var = None  # The loop counter variable (mop_t)
+        self.init_value = None  # Initial value of counter
+        self.increment_value = None  # Value counter is set to at end
+        self.is_valid = False
+
+
+def get_mop_from_stack_var(mop: mop_t) -> tuple[int, int] | None:
+    """Extract (offset, size) from a stack variable mop."""
+    if mop.t == mop_S:
+        return (mop.s.off, mop.size)
+    return None
+
+
+def mops_equal(mop1: mop_t, mop2: mop_t) -> bool:
+    """Check if two mops refer to the same location."""
+    if mop1 is None or mop2 is None:
+        return False
+
+    # For stack variables
+    if mop1.t == mop_S and mop2.t == mop_S:
+        return mop1.s.off == mop2.s.off and mop1.size == mop2.size
+
+    # For registers
+    if mop1.t == mop_r and mop2.t == mop_r:
+        return mop1.r == mop2.r and mop1.size == mop2.size
+
+    return False
+
+
+def is_simple_constant_assignment(ins: minsn_t) -> tuple[mop_t | None, int | None]:
+    """
+    Check if instruction is a simple assignment of a constant to a variable.
+    Returns (destination_mop, constant_value) or (None, None).
+    """
+    if ins is None or ins.opcode != m_mov:
+        return None, None
+
+    # Check if right side is a constant
+    if ins.l.t != mop_n:
+        return None, None
+
+    # Check if destination is a stack variable or register
+    if ins.d.t not in [mop_S, mop_r]:
+        return None, None
+
+    return ins.d, ins.l.nnn.value
+
+
+def find_counter_increment_in_block(blk: mblock_t, counter_var: mop_t) -> int | None:
+    """
+    Look for an instruction that sets the counter variable to a constant.
+    Returns the constant value or None.
+    """
+    cur_ins = blk.head
+    while cur_ins is not None:
+        dst, const_val = is_simple_constant_assignment(cur_ins)
+        if dst is not None and mops_equal(dst, counter_var):
+            return const_val
+        cur_ins = cur_ins.next
+    return None
+
+
+def analyze_bogus_loop(blk: mblock_t) -> BogusLoopInfo:
+    """
+    Analyze a block to see if it's part of a bogus single-iteration loop.
+
+    Pattern to detect:
+    1. Current block (check_block) has conditional jump checking counter != init_value
+    2. Loop body sets counter to different value
+    3. Body jumps back to check_block
+
+    Example microcode pattern:
+        Block N:   mov #0, %var_counter      ; init
+        Block N+1: jnz %var_counter, #0, @exit  ; check (this block)
+        Block N+2: <body>                    ; body starts
+        Block N+3: mov #1, %var_counter      ; set to different value
+                   goto @N+1                 ; back edge
+        Block exit: ...                      ; post-loop
+    """
+    info = BogusLoopInfo()
+
+    # Must be a 2-way block (conditional jump)
+    if blk.nsucc() != 2:
+        return info
+
+    if blk.tail is None or blk.tail.opcode not in CONDITIONAL_JUMP_OPCODES:
+        return info
+
+    info.check_block = blk
+
+    # Analyze the conditional jump
+    # Looking for patterns like: jnz %var_1C.4, #0.4, @exit_block
+    # or: jz %var_1C.4, #0.4, @body_block
+
+    jump_ins = blk.tail
+    left_mop = jump_ins.l
+    right_mop = jump_ins.r
+
+    # One operand must be a constant (the comparison value)
+    # The other must be a variable (the counter)
+    counter_var = None
+    compare_value = None
+
+    if left_mop.t == mop_n and right_mop.t in [mop_S, mop_r]:
+        compare_value = left_mop.nnn.value
+        counter_var = right_mop
+    elif right_mop.t == mop_n and left_mop.t in [mop_S, mop_r]:
+        compare_value = right_mop.nnn.value
+        counter_var = left_mop
+    else:
+        return info
+
+    info.counter_var = counter_var
+    info.init_value = compare_value
+
+    # Determine which successor is the exit and which is the body
+    conditional_succ_serial = jump_ins.d.b
+    direct_succ_serial = blk.serial + 1
+
+    # For jnz %var, #0:
+    #   - If var == 0 (initially), condition is false, takes direct successor (fallthrough)
+    #   - If var != 0 (after loop), condition is true, takes conditional successor (jump target)
+    # So: direct_succ is body, conditional_succ is exit
+    #
+    # For jz %var, #0:
+    #   - If var == 0 (initially), condition is true, takes conditional successor (jump target)
+    #   - If var != 0 (after loop), condition is false, takes direct successor (fallthrough)
+    # So: conditional_succ is body, direct_succ is exit
+
+    if jump_ins.opcode == m_jnz:
+        # jnz: jumps when condition is true (!=)
+        # Initially counter == compare_value, so doesn't jump → body is fallthrough
+        body_serial = direct_succ_serial
+        exit_serial = conditional_succ_serial
+    elif jump_ins.opcode == m_jz:
+        # jz: jumps when condition is true (==)
+        # Initially counter == compare_value, so jumps → body is jump target
+        body_serial = conditional_succ_serial
+        exit_serial = direct_succ_serial
+    else:
+        # Other conditional jumps - could extend for jae, jb, etc.
+        return info
+
+    mba = blk.mba
+
+    # Validate that successors exist
+    if body_serial >= mba.qty or exit_serial >= mba.qty:
+        return info
+
+    body_block = mba.get_mblock(body_serial)
+    exit_block = mba.get_mblock(exit_serial)
+
+    # Now trace through the body to find where counter is set and where it loops back
+    visited = set()
+    current = body_block
+    body_blocks = []
+
+    # Follow the body blocks until we find one that:
+    # 1. Sets the counter to a different value
+    # 2. Jumps back to check_block
+
+    max_depth = 10  # Prevent infinite loops
+    depth = 0
+    found_increment = False
+    increment_value = None
+
+    while current and depth < max_depth:
+        if current.serial in visited:
+            break
+        visited.add(current.serial)
+        body_blocks.append(current)
+        depth += 1
+
+        # Look for counter increment in this block
+        inc_val = find_counter_increment_in_block(current, counter_var)
+        if inc_val is not None and inc_val != compare_value:
+            found_increment = True
+            increment_value = inc_val
+
+        # Check successors
+        if current.nsucc() == 1:
+            next_serial = current.succset[0]
+            if next_serial == blk.serial:
+                # Found the back edge to check block!
+                if found_increment:
+                    info.body_blocks = body_blocks
+                    info.exit_block = exit_block
+                    info.increment_value = increment_value
+                    info.is_valid = True
+                break
+            # Continue to next block in body
+            if next_serial < mba.qty:
+                current = mba.get_mblock(next_serial)
+            else:
+                break
+        elif current.nsucc() == 0:
+            # End of body without looping back
+            break
+        else:
+            # Body has conditional jump - more complex pattern
+            # For now, we don't handle this
+            break
+
+    return info
+
+
+class BogusLoopRemover(FlowOptimizationRule):
+    """
+    Removes bogus single-iteration loops used for obfuscation.
+
+    These loops have the pattern:
+        for (i = C1; i == C1; i = C2) { body }
+
+    Which always executes exactly once and can be safely unrolled.
+    """
+
+    def __init__(self):
+        super().__init__()
+        self.nb_changes = 0
+
+    def optimize(self, blk: mblock_t) -> int:
+        """
+        Analyze and optimize the given block.
+
+        Returns the number of changes made.
+        """
+        info = analyze_bogus_loop(blk)
+
+        if not info.is_valid:
+            return 0
+
+        logger.info(
+            "Found bogus single-iteration loop at block %d (counter: init=%d, inc=%d)",
+            info.check_block.serial,
+            info.init_value,
+            info.increment_value,
+        )
+
+        # Remove the loop by:
+        # 1. Change check_block to jump directly to first body block (remove the conditional)
+        # 2. Change last body block to jump to exit_block instead of back to check_block
+
+        mba = blk.mba
+
+        # Find the first body block (it's the one that the check block should enter)
+        if len(info.body_blocks) == 0:
+            logger.warning(
+                "No body blocks found for bogus loop at block %d", blk.serial
+            )
+            return 0
+
+        first_body_serial = info.body_blocks[0].serial
+        exit_serial = info.exit_block.serial
+
+        # Change the check block from conditional jump to unconditional goto to body
+        logger.info(
+            "  Changing block %d from conditional to goto %d (body)",
+            info.check_block.serial,
+            first_body_serial,
+        )
+        make_2way_block_goto(info.check_block, first_body_serial)
+
+        # Change the last body block to jump to exit instead of looping back
+        last_body_block = info.body_blocks[-1]
+        if (
+            last_body_block.nsucc() == 1
+            and last_body_block.succset[0] == info.check_block.serial
+        ):
+            logger.info(
+                "  Changing block %d to goto %d (exit) instead of %d (check)",
+                last_body_block.serial,
+                exit_serial,
+                info.check_block.serial,
+            )
+            change_1way_block_successor(last_body_block, exit_serial)
+
+        # Clean up the graph
+        mba_deep_cleaning(mba, call_mba_combine_block=True)
+
+        self.nb_changes += 1
+        return 1
diff --git a/src/d810/optimizers/microcode/instructions/chain/chain_rules.py b/src/d810/optimizers/microcode/instructions/chain/chain_rules.py
index f2e8f05..dbc0cdf 100644
--- a/src/d810/optimizers/microcode/instructions/chain/chain_rules.py
+++ b/src/d810/optimizers/microcode/instructions/chain/chain_rules.py
@@ -16,12 +16,13 @@ rules_chain_logger = getLogger("D810.rules.chain")
 
 
 class ChainSimplification(object):
-    def __init__(self, opcode):
+    def __init__(self, opcode, func_entry_ea: int = 0):
         self.opcode = opcode
         self.formatted_ins = ""
         self.non_cst_mop_list = []
         self.cst_mop_list = []
         self._is_instruction_simplified = False
+        self.func_entry_ea = func_entry_ea
 
     def add_mop(self, mop):
         if (mop.t == mop_d) and (mop.d.opcode == self.opcode):
@@ -68,60 +69,115 @@ class ChainSimplification(object):
             return [final_cst_mop]
 
     def get_simplified_non_constant(self):
+        # Fast path
         if len(self.non_cst_mop_list) == 0:
             return []
-        elif len(self.non_cst_mop_list) == 1:
+        if len(self.non_cst_mop_list) == 1:
             return self.non_cst_mop_list
-        else:
-            is_always_0 = False
-            index_removed = []
-            for i in range(len(self.non_cst_mop_list)):
-                for j in range(i + 1, len(self.non_cst_mop_list)):
-                    if (i not in index_removed) and (j not in index_removed):
-                        if equal_mops_ignore_size(
-                            self.non_cst_mop_list[i], self.non_cst_mop_list[j]
-                        ):
-                            if self.opcode == m_xor:
-                                # x ^ x == 0
-                                rules_chain_logger.debug(
-                                    "Doing non cst simplification (xor): {0}, {1} in {2}".format(
-                                        i, j, self.formatted_ins
-                                    )
-                                )
-                                index_removed += [i, j]
-                            elif self.opcode == m_and:
-                                # x & x == x
-                                rules_chain_logger.debug(
-                                    "Doing non cst simplification (and): {0}, {1} in {2}".format(
-                                        i, j, self.formatted_ins
-                                    )
-                                )
-                                index_removed += [j]
-                            elif self.opcode == m_or:
-                                # x | x == x
-                                rules_chain_logger.debug(
-                                    "Doing non cst simplification (or): {0}, {1} in {2}".format(
-                                        i, j, self.formatted_ins
-                                    )
-                                )
-                                index_removed += [j]
-                        elif equal_bnot_mop(
-                            self.non_cst_mop_list[i], self.non_cst_mop_list[j]
-                        ):
-                            if self.opcode == m_and:
-                                is_always_0 = True
 
-            if len(index_removed) == 0 and not is_always_0:
-                return self.non_cst_mop_list
-            final_mop_list = []
-            self._is_instruction_simplified = True
-            if is_always_0:
-                final_mop_list.append(self.create_cst_mop(0, self.res_mop_size))
-                return final_mop_list
-            for i in range(len(self.non_cst_mop_list)):
-                if i not in index_removed:
-                    final_mop_list.append(self.non_cst_mop_list[i])
+        # Local memo for equality to avoid recomputing within one instruction
+        eq_memo: dict[tuple[int, int], bool] = {}
+
+        def _eq(a, b) -> bool:
+            ka, kb = id(a), id(b)
+            key = (ka, kb) if ka <= kb else (kb, ka)
+            cached = eq_memo.get(key)
+            if cached is not None:
+                return cached
+            res = equal_mops_ignore_size(a, b)
+            eq_memo[key] = res
+            return res
+
+        from d810.hexrays.hexrays_helpers import structural_mop_hash
+
+        # Bucket first by structural hash and build per-bucket unique reps
+        buckets: dict[int, list[mop_t]] = {}
+        bucket_reps: dict[int, list[mop_t]] = {}
+        unique: list[mop_t] = []
+
+        for m in self.non_cst_mop_list:
+            try:
+                k = structural_mop_hash(m, self.func_entry_ea)
+            except Exception:
+                k = m.t
+            reps = bucket_reps.setdefault(k, [])
+            for rep in reps:
+                if _eq(m, rep):
+                    break
+            else:
+                reps.append(m)
+                unique.append(m)
+            buckets.setdefault(k, []).append(m)
+
+        # Index mapping for removals over the flattened unique list
+        idx_of = {id(m): i for i, m in enumerate(unique)}
+        index_removed: set[int] = set()
+        is_always_0 = False
+
+        # Within-bucket reduction without O(n^2): group by equality
+        for key, members in bucket_reps.items():
+            if len(members) < 2:
+                continue
+            # Map representative id -> all indices in 'unique' that are equal to it
+            groups: list[mop_t] = []
+            group_indices: dict[int, list[int]] = {}
+
+            # For each original member occurrence in this bucket, assign to a group
+            for m in buckets[key]:
+                # Find representative for this member
+                for rep in members:
+                    if _eq(m, rep):
+                        rid = id(rep)
+                        groups.append(rep) if rid not in group_indices else None
+                        group_indices.setdefault(rid, []).append(idx_of[id(m)])
+                        break
+
+            # Now apply opcode-specific consolidation
+            if self.opcode == m_xor:
+                for rid, idxs in group_indices.items():
+                    if len(idxs) % 2 == 0:
+                        index_removed.update(idxs)
+                    else:
+                        # keep one, drop the rest
+                        index_removed.update(idxs[1:])
+            elif self.opcode in (m_and, m_or):
+                for rid, idxs in group_indices.items():
+                    # keep first occurrence only
+                    index_removed.update(idxs[1:])
+
+        # Cross-bucket bnot only when needed for AND → zero
+        if self.opcode == m_and and not is_always_0:
+            keys = list(bucket_reps.keys())
+            for i in range(len(keys)):
+                for j in range(i + 1, len(keys)):
+                    for mi in bucket_reps[keys[i]]:
+                        ii = idx_of[id(mi)]
+                        if ii in index_removed:
+                            continue
+                        for mj in bucket_reps[keys[j]]:
+                            jj = idx_of[id(mj)]
+                            if jj in index_removed:
+                                continue
+                            if equal_bnot_mop(mi, mj):
+                                is_always_0 = True
+                                break
+                        if is_always_0:
+                            break
+                if is_always_0:
+                    break
+
+        if len(index_removed) == 0 and not is_always_0:
+            return unique
+
+        final_mop_list: list[mop_t] = []
+        self._is_instruction_simplified = True
+        if is_always_0:
+            final_mop_list.append(self.create_cst_mop(0, self.res_mop_size))
             return final_mop_list
+        for idx, mop in enumerate(unique):
+            if idx not in index_removed:
+                final_mop_list.append(mop)
+        return final_mop_list
 
     def simplify(self, ins):
         self.res_mop_size = ins.d.size
@@ -249,7 +305,10 @@ class ArithmeticChainSimplification(object):
 
     def get_simplified_non_constant(self):
         if len(self.add_non_cst_mop_list) == 0 and len(self.sub_non_cst_mop_list) == 0:
-            return [[], []]
+            # Return an explicit zero-constant mop so callers can safely inspect .nnn
+            zero = mop_t()
+            zero.make_number(0, 1)
+            return [], [], zero
         final_add_list = self.add_non_cst_mop_list
         final_sub_list = self.sub_non_cst_mop_list
         index_add_removed = []
@@ -392,7 +451,8 @@ class XorChain(ChainSimplificationRule):
     )
 
     def check_and_replace(self, blk, ins):
-        xor_simplifier = ChainSimplification(m_xor)
+        func_ea = getattr(getattr(blk, "mba", None), "entry_ea", 0)
+        xor_simplifier = ChainSimplification(m_xor, func_entry_ea=func_ea)
         new_ins = xor_simplifier.simplify(ins)
         return new_ins
 
@@ -403,7 +463,8 @@ class AndChain(ChainSimplificationRule):
     )
 
     def check_and_replace(self, blk, ins):
-        and_simplifier = ChainSimplification(m_and)
+        func_ea = getattr(getattr(blk, "mba", None), "entry_ea", 0)
+        and_simplifier = ChainSimplification(m_and, func_entry_ea=func_ea)
         new_ins = and_simplifier.simplify(ins)
         return new_ins
 
@@ -414,7 +475,8 @@ class OrChain(ChainSimplificationRule):
     )
 
     def check_and_replace(self, blk, ins):
-        or_simplifier = ChainSimplification(m_or)
+        func_ea = getattr(getattr(blk, "mba", None), "entry_ea", 0)
+        or_simplifier = ChainSimplification(m_or, func_entry_ea=func_ea)
         new_ins = or_simplifier.simplify(ins)
         return new_ins
 
diff --git a/src/d810/optimizers/microcode/instructions/chain/handler.py b/src/d810/optimizers/microcode/instructions/chain/handler.py
index ecebea9..2384d5d 100644
--- a/src/d810/optimizers/microcode/instructions/chain/handler.py
+++ b/src/d810/optimizers/microcode/instructions/chain/handler.py
@@ -15,3 +15,10 @@ class ChainSimplificationRule(InstructionOptimizationRule):
 
 class ChainOptimizer(InstructionOptimizer):
     RULE_CLASSES = [ChainSimplificationRule]
+
+    def __init__(self, maturities, stats, log_dir=None):
+        super().__init__(maturities, stats, log_dir)
+        # Only consider binary associative ops chains
+        from ida_hexrays import m_add, m_and, m_or, m_sub, m_xor
+
+        self._allowed_root_opcodes = {m_xor, m_and, m_or, m_add, m_sub}
diff --git a/src/d810/optimizers/microcode/instructions/early/mem_read.py b/src/d810/optimizers/microcode/instructions/early/mem_read.py
index 95018b2..f6c539d 100644
--- a/src/d810/optimizers/microcode/instructions/early/mem_read.py
+++ b/src/d810/optimizers/microcode/instructions/early/mem_read.py
@@ -1,3 +1,5 @@
+from typing import Any, Optional
+
 from ida_hexrays import *
 from idaapi import (
     SEGPERM_READ,
@@ -14,6 +16,27 @@ from d810.expr.ast import AstConstant, AstLeaf, AstNode
 from d810.optimizers.microcode.instructions.early.handler import EarlyRule
 
 
+def segment_is_read_only(addr: int) -> bool:
+    s: segment_t = getseg(addr)
+    if s is None:
+        return False
+    return (s.perm & SEGPERM_READ) != 0 and (s.perm & SEGPERM_WRITE) == 0
+
+
+def is_read_only_inited_var(address: int) -> bool:
+    if not segment_is_read_only(address):
+        return False
+    if is_loaded(address):
+        return False
+    ref_finder = xrefblk_t()
+    is_ok = ref_finder.first_to(address, XREF_DATA)
+    while is_ok:
+        if ref_finder.type == dr_W:
+            return False
+        is_ok = ref_finder.next_to()
+    return True
+
+
 class SetGlobalVariablesToZero(EarlyRule):
     DESCRIPTION = "This rule can be used to patch memory read"
 
@@ -43,13 +66,21 @@ class SetGlobalVariablesToZero(EarlyRule):
     def check_candidate(self, candidate):
         if (self.ro_dword_min_ea is None) or (self.ro_dword_max_ea is None):
             return False
-        if candidate["ro_dword"].mop.t != mop_v:
+        leaf = candidate["ro_dword"]
+        if leaf is None:
+            return False
+        mop = leaf.mop
+        if mop is None:
+            return False
+        if getattr(mop, "t", None) != mop_v:
+            return False
+        mem_read_address = getattr(mop, "g", None)
+        if mem_read_address is None:
             return False
-        mem_read_address = candidate["ro_dword"].mop.g
         if not (self.ro_dword_min_ea <= mem_read_address <= self.ro_dword_max_ea):
             return False
 
-        candidate.add_constant_leaf("val_res", 0, candidate["ro_dword"].mop.size)
+        candidate.add_constant_leaf("val_res", 0, mop.size)
         return True
 
 
@@ -75,34 +106,82 @@ class SetGlobalVariablesToZeroIfDetectedReadOnly(EarlyRule):
         # Thus, we explicitly specify the MMAT_PREOPTIMIZED maturity.
         self.maturities = [MMAT_PREOPTIMIZED]
 
-    def is_read_only_inited_var(self, address):
-        s: segment_t = getseg(address)
-        if s is None:
-            return False
-        if s.perm != (SEGPERM_READ | SEGPERM_WRITE):
-            return False
-        if is_loaded(address):
-            return False
-        ref_finder = xrefblk_t()
-        is_ok = ref_finder.first_to(address, XREF_DATA)
-        while is_ok:
-            if ref_finder.type == dr_W:
-                return False
-            is_ok = ref_finder.next_to()
-        return True
-
     def check_candidate(self, candidate):
-        mem_read_address = None
-        if candidate["ro_dword"].mop.t == mop_v:
-            mem_read_address = candidate["ro_dword"].mop.g
-        elif candidate["ro_dword"].mop.t == mop_a:
-            if candidate["ro_dword"].mop.a.t == mop_v:
-                mem_read_address = candidate["ro_dword"].mop.a.g
+        leaf = candidate["ro_dword"]
+        if leaf is None:
+            return False
+        mop = leaf.mop
+        if mop is None:
+            return False
+        mem_read_address: Optional[int] = None
+        if mop.t == mop_v:
+            mem_read_address = mop.g
+        elif mop.t == mop_a and mop.a is not None:
+            inner = mop.a
+            if inner.t == mop_v:
+                mem_read_address = inner.g
 
         if mem_read_address is None:
             return False
 
-        if not self.is_read_only_inited_var(mem_read_address):
+        if not is_read_only_inited_var(mem_read_address):
             return False
-        candidate.add_constant_leaf("val_res", 0, candidate["ro_dword"].mop.size)
+        candidate.add_constant_leaf("val_res", 0, mop.size)
+        return True
+
+
+class ReplaceReadonlyAddressOfWithImmediate(EarlyRule):
+    DESCRIPTION = (
+        "Replace mov &($sym[+off]), dst with immediate addr if in .rdata/.rodata"
+    )
+
+    @property
+    def PATTERN(self) -> AstNode:
+        return AstNode(m_mov, AstLeaf("ro_addr"))
+
+    @property
+    def REPLACEMENT_PATTERN(self) -> AstNode:
+        return AstNode(m_mov, AstConstant("val_res"))
+
+    def __init__(self) -> None:
+        super().__init__()
+        # Run early to avoid creating bogus MEMORY[0] later when addresses fold late
+        self.maturities = [MMAT_PREOPTIMIZED]
+
+    def _resolve_address_from_mop(self, mop_obj: mop_t | None) -> int | None:
+        if mop_obj is None:
+            return None
+        t = mop_obj.t
+        if t == mop_a:
+            inner = mop_obj.a
+            if inner is None:
+                return None
+            it = inner.t
+            if it == mop_v:
+                return inner.g
+            if it == mop_S:
+                # Prefer concrete address in `off`, fallback to `start_ea`
+                return getattr(inner.s, "off", None) or getattr(
+                    inner.s, "start_ea", None
+                )
+        elif t == mop_v:
+            return mop_obj.g
+        return None
+
+    def check_candidate(self, candidate):
+        leaf = candidate["ro_addr"]
+        if leaf is None:
+            return False
+        mop_obj: mop_t | None = leaf.mop
+        if mop_obj is None:
+            return False
+        addr = self._resolve_address_from_mop(mop_obj)
+        if addr is None:
+            return False
+        if not segment_is_read_only(addr):
+            return False
+        size = mop_obj.size or 0
+        if size == 0:
+            return False
+        candidate.add_constant_leaf("val_res", addr, size)
         return True
diff --git a/src/d810/optimizers/microcode/instructions/egraph/handler.py b/src/d810/optimizers/microcode/instructions/egraph/handler.py
new file mode 100644
index 0000000..ad54a6b
--- /dev/null
+++ b/src/d810/optimizers/microcode/instructions/egraph/handler.py
@@ -0,0 +1,856 @@
+"""
+This module replaces the original pattern matching optimizer with a
+canonicalisation and egraph‑style matcher.  The original implementation
+generated a potentially enormous number of fuzzy pattern permutations in
+order to handle commutative and associative operations.  This new
+implementation avoids that combinatorial explosion by canonicalising
+both the pattern and the candidate microcode AST before matching them.
+
+The canonicalisation step flattens and sorts operands of commutative
+and associative operators (addition, multiplication, bitwise‐or, bitwise‐and
+and bitwise‐xor).  Subtraction is rewritten into addition of a negated
+operand.  Negation is simplified where possible.  Once canonicalised
+both the pattern and the candidate instruction have a unique shape
+regardless of operand ordering, allowing a single match to succeed
+instead of having to enumerate all permutations.
+
+When a rule matches, we construct a fresh AST in the shape of the
+original pattern and populate it with the matched operands.  We then
+delegate to the existing rule logic to copy microcode (mops), perform
+additional candidate checks and build the replacement instruction.
+
+This approach is analogous to equality saturation (egraphs) in that
+equivalent trees collapse to a canonical representation.  It yields
+dramatically fewer pattern candidates to consider and therefore avoids
+the exponential blow‑up of the previous scheme.
+"""
+
+from __future__ import annotations
+
+import copy
+import dataclasses
+import typing
+
+from ida_hexrays import *  # noqa: F401,F403
+
+from d810.conf.loggers import getLogger
+from d810.expr.ast import AstBase, AstConstant, AstLeaf, AstNode, minsn_to_ast
+from d810.hexrays.hexrays_formatters import format_minsn_t
+
+# Additional helpers for some rules.  These functions and tables are
+# imported from the original helpers module.  They are used in the
+# candidate checks below.  If these symbols are not available at
+# runtime, importing them will raise which allows the caller to see
+# that the helpers are missing.
+from d810.hexrays.hexrays_helpers import (
+    AND_TABLE,
+    equal_bnot_mop,
+    equal_mops_ignore_size,
+)
+from d810.optimizers.microcode.instructions.handler import (
+    GenericPatternRule,
+    InstructionOptimizationRule,
+    InstructionOptimizer,
+)
+
+if typing.TYPE_CHECKING:
+    from d810.stats import OptimizationStatistics
+
+# Reuse the same loggers as the original implementation.
+optimizer_logger = getLogger("D810.optimizer")
+pattern_search_logger = getLogger("D810.pattern_search")
+
+
+COMMUTATIVE_OPCODES = {m_add, m_mul, m_or, m_and, m_xor}
+
+
+def _flatten_operands(node: AstNode, opcode: int) -> list[AstBase]:
+    """Recursively collect all operands of a commutative/associative operator.
+
+    If the node's opcode matches the operator, its children are
+    flattened into a single list.  Otherwise the node itself is
+    returned as the sole element.  Leaves and constants are returned
+    unchanged.
+
+    Args:
+        node: The AST node to flatten.
+        opcode: The opcode of the operator being flattened.
+    Returns:
+        A list of operand ASTs.
+    """
+    if not isinstance(node, AstBase) or not node.is_node():
+        return [node]
+    node = typing.cast(AstNode, node)
+    if node.opcode != opcode:
+        return [node]
+    # recurse into children collecting operands of the same opcode
+    operands: list[AstBase] = []
+    if node.left is not None:
+        operands += _flatten_operands(node.left, opcode)
+    if node.right is not None:
+        operands += _flatten_operands(node.right, opcode)
+    return operands
+
+
+def _build_balanced_tree(opcode: int, operands: list[AstBase]) -> AstBase:
+    """Reconstruct a binary tree from a list of operands.
+
+    Given a list of operands and an opcode, this helper builds
+    a left‑associative binary tree.  The ordering of the operands
+    should already be normalised (e.g. sorted) by the caller.
+
+    Args:
+        opcode: The opcode for the internal nodes.
+        operands: A list of operands after flattening and sorting.
+    Returns:
+        An AST with the operands combined using the given opcode.
+    """
+    assert len(operands) > 0
+    if len(operands) == 1:
+        return operands[0]
+    cur: AstBase = operands[0]
+    for operand in operands[1:]:
+        # Ensure both sides are AstBase when constructing an AstNode
+        left = typing.cast(AstBase, cur)
+        right = typing.cast(AstBase, operand)
+        cur = AstNode(opcode, left, right)
+    return cur
+
+
+def canonicalize_ast(node: AstBase | None) -> AstBase | None:
+    """Return a canonicalised copy of the given AST.
+
+    The canonicalisation performs the following transforms:
+    * For addition, multiplication, bitwise or, bitwise and and bitwise xor,
+      all nested occurrences of the operator are flattened into a list of
+      operands, each operand is canonicalised recursively, the list is sorted
+      by the textual pattern of each operand and finally rebuilt into a
+      left‑associative binary tree.
+    * Subtraction is rewritten into addition of the left operand and the
+      negation of the right operand.  This allows x − y to be matched
+      against x + (−y) regardless of which representation appears in the
+      candidate.
+    * Negation of a negation collapses: −(−x) becomes x.
+    * All other nodes are canonicalised recursively on their children.
+
+    Leaves (AstLeaf) and constants (AstConstant) are returned unchanged.
+
+    Args:
+        node: The AST to canonicalise.
+    Returns:
+        A new canonicalised AST or None if the input was None.
+    """
+    if node is None:
+        return None
+    # For leaves and constants we simply return a shallow copy to avoid
+    # mutating the original AST.  AstLeaf and AstConstant inherit from
+    # AstBase but are not nodes (is_node() returns False) so they
+    # bypass the canonicalisation rules below.
+    if not node.is_node():
+        return copy.deepcopy(node)
+    # Work with a deep copy of the node to avoid mutating the original
+    ast = typing.cast(AstNode, copy.deepcopy(node))
+    opcode = ast.opcode
+    # Handle commutative and associative operators
+    if opcode in COMMUTATIVE_OPCODES:
+        # Flatten nested operators and canonicalise all operands
+        operands: list[AstBase] = []
+        for op in _flatten_operands(ast, opcode):
+            operands.append(canonicalize_ast(op))
+        # Sort operands by their textual pattern to produce a deterministic
+        # ordering.  get_pattern() returns a pattern string identifying
+        # the shape of the subexpression; it is safe to use for sorting.
+        operands.sort(
+            key=lambda n: (n.get_pattern() if hasattr(n, "get_pattern") else str(n))
+        )
+        return _build_balanced_tree(opcode, operands)
+    # Rewrite subtraction into addition of a negated operand
+    if opcode == m_sub:
+        # canonicalise children first
+        left = canonicalize_ast(ast.left) if ast.left is not None else None
+        right = canonicalize_ast(ast.right) if ast.right is not None else None
+        # Convert x - y into x + (-y)
+        neg_right: AstBase
+        if (
+            isinstance(right, AstBase)
+            and right.is_node()
+            and typing.cast(AstNode, right).opcode == m_neg
+        ):
+            # -(y) negated again becomes y
+            inner = typing.cast(AstNode, right)
+            neg_right = canonicalize_ast(inner.left)
+        else:
+            neg_right = AstNode(m_neg, right)
+        # Recursively canonicalise the resulting addition
+        return canonicalize_ast(AstNode(m_add, left, neg_right))
+    # Simplify double negation
+    if opcode == m_neg:
+        sub = canonicalize_ast(ast.left) if ast.left is not None else None
+        # If the child is itself a negation, collapse them
+        if (
+            isinstance(sub, AstBase)
+            and sub.is_node()
+            and typing.cast(AstNode, sub).opcode == m_neg
+        ):
+            inner = typing.cast(AstNode, sub)
+            return canonicalize_ast(inner.left)
+        return AstNode(m_neg, sub)
+    # Default case: recursively canonicalise left and right children
+    left = canonicalize_ast(ast.left) if ast.left is not None else None
+    right = canonicalize_ast(ast.right) if ast.right is not None else None
+    return AstNode(opcode, left, right)
+
+
+def _ast_equal(a: AstBase, b: AstBase) -> bool:
+    """Return True if two ASTs have the same pattern.
+
+    This helper compares two ASTs using their pattern strings if
+    available; otherwise it falls back to structural equality via
+    `get_pattern()`.  This is used to ensure consistency when
+    matching variables that may appear multiple times in a pattern.
+    """
+    # Leaves and constants implement get_pattern() as part of AstBase
+    if hasattr(a, "get_pattern") and hasattr(b, "get_pattern"):
+        return a.get_pattern() == b.get_pattern()
+    # Fallback structural comparison
+    return str(a) == str(b)
+
+
+def match_pattern(
+    pattern: AstBase | None, candidate: AstBase | None, mapping: dict[str, AstBase]
+) -> bool:
+    """Attempt to match a canonicalised pattern against a canonicalised candidate.
+
+    The matching algorithm walks both trees simultaneously.  When a
+    pattern leaf (AstLeaf) is encountered, the corresponding candidate
+    subtree is recorded in the mapping.  Subsequent uses of the same
+    pattern variable must match identically.  Pattern constants
+    (AstConstant) with a `None` value are treated as variables and are
+    similarly recorded; constants with a concrete value must match
+    exactly against constants in the candidate.
+
+    For commutative/associative operations the canonicaliser has
+    already sorted and flattened operands, so simple left/right
+    matching suffices.
+
+    Args:
+        pattern: The canonicalised pattern AST.
+        candidate: The canonicalised candidate AST.
+        mapping: A dict recording assignments of pattern variables to
+          candidate subtrees.  This dict is updated in place.
+    Returns:
+        True if the pattern matches the candidate, False otherwise.
+    """
+    # Both None: match
+    if pattern is None and candidate is None:
+        return True
+    # One is None and the other is not: mismatch
+    if pattern is None or candidate is None:
+        return False
+    # Pattern leaf: bind variable
+    if isinstance(pattern, AstLeaf):
+        var_name = pattern.name
+        if var_name in mapping:
+            return _ast_equal(mapping[var_name], candidate)
+        # Record the first occurrence of this variable
+        mapping[var_name] = candidate
+        return True
+    # Pattern constant: capture or literal
+    if isinstance(pattern, AstConstant):
+        if pattern.value is None:
+            # Capturing constant variable
+            var_name = pattern.name
+            if var_name in mapping:
+                # Ensure the previously bound constant matches the current candidate
+                bound = mapping[var_name]
+                # Both must be constants and have the same value
+                return (
+                    isinstance(bound, AstConstant)
+                    and isinstance(candidate, AstConstant)
+                    and bound.value == typing.cast(AstConstant, candidate).value
+                )
+            if not isinstance(candidate, AstConstant):
+                return False
+            mapping[var_name] = candidate
+            return True
+        # Literal constant: require exact match of the numeric value
+        if not isinstance(candidate, AstConstant):
+            return False
+        return pattern.value == typing.cast(AstConstant, candidate).value
+    # Pattern is an AST node: candidate must be a node with the same opcode
+    if not isinstance(candidate, AstBase) or not candidate.is_node():
+        return False
+    pat_node = typing.cast(AstNode, pattern)
+    cand_node = typing.cast(AstNode, candidate)
+    if pat_node.opcode != cand_node.opcode:
+        return False
+    # Recursively match left and right
+    left_match = match_pattern(pat_node.left, cand_node.left, mapping)
+    if not left_match:
+        return False
+    return match_pattern(pat_node.right, cand_node.right, mapping)
+
+
+def substitute_pattern(
+    pattern: AstBase | None, mapping: dict[str, AstBase]
+) -> AstBase | None:
+    """Instantiate a pattern AST using a mapping from variable names to ASTs.
+
+    This helper reconstructs an AST in the shape of the given pattern by
+    replacing each AstLeaf and capturing AstConstant with the
+    corresponding subtree recorded in `mapping`.  Literal constants are
+    copied verbatim.  Operator nodes are rebuilt recursively.
+
+    Args:
+        pattern: The pattern AST whose variables will be substituted.
+        mapping: The mapping produced by `match_pattern` assigning
+            pattern variables to candidate subtrees.
+    Returns:
+        A new AST where pattern variables and capturing constants are
+        replaced with the corresponding candidate subtrees.
+    """
+    if pattern is None:
+        return None
+    # Leaf: return the bound candidate
+    if isinstance(pattern, AstLeaf):
+        var_name = pattern.name
+        return copy.deepcopy(mapping[var_name])
+    # Capturing constant: substitute the bound constant
+    if isinstance(pattern, AstConstant):
+        if pattern.value is None:
+            var_name = pattern.name
+            return copy.deepcopy(mapping[var_name])
+        # Literal constant: return a copy
+        return copy.deepcopy(pattern)
+    # Node: rebuild with substituted children
+    assert isinstance(pattern, AstNode)
+    new_left = substitute_pattern(pattern.left, mapping)
+    new_right = substitute_pattern(pattern.right, mapping)
+    return AstNode(
+        pattern.opcode, typing.cast(AstBase, new_left), typing.cast(AstBase, new_right)
+    )
+
+
+class CanonicalPatternRule(GenericPatternRule):
+    """A Pattern rule that uses canonicalisation for matching.
+
+    Subclasses should override the PATTERN and REPLACEMENT_PATTERN
+    properties as usual.  This base class caches a canonicalised copy
+    of the pattern to avoid repeated work.  The canonical pattern is
+    used solely for structural matching; the original pattern is still
+    used for copying microcode (mops) and building the replacement
+    instruction.
+    """
+
+    FUZZ_PATTERN: bool = (
+        False  # Fuzzy pattern generation is unused with canonical matching
+    )
+
+    def __init__(self) -> None:
+        super().__init__()
+        # Cache a canonicalised version of the pattern for matching.  The
+        # canonical pattern is computed once in the constructor.  Note
+        # that we do not canonicalise the replacement here because it
+        # will be instantiated using the mapping and then passed back
+        # through the rule logic which expects the original shape.
+        self._canonical_pattern: AstBase | None = None
+
+    @property
+    def canonical_pattern(self) -> AstBase:
+        # Lazily compute and cache the canonical pattern
+        if self._canonical_pattern is None:
+            if self.PATTERN is None:
+                raise ValueError("Pattern must not be None for canonical matching")
+            self._canonical_pattern = canonicalize_ast(self.PATTERN)
+        return self._canonical_pattern
+
+    def configure(self, fuzz_pattern: bool | None = None, **kwargs) -> None:  # type: ignore[override]
+        """Override configure to bypass fuzzy pattern generation.
+
+        The original implementation could generate many fuzzy patterns
+        representing different operand orderings.  With canonical
+        matching there is no need to generate such variations.  This
+        method still accepts a `fuzz_pattern` argument for API
+        compatibility but ignores it.
+        """
+        # Do not generate any fuzzy patterns – the canonical pattern suffices
+        super().configure(kwargs)
+        self.fuzz_pattern = self.FUZZ_PATTERN
+        # Canonical pattern may depend on configuration; recompute
+        self._canonical_pattern = canonicalize_ast(self.PATTERN)
+
+
+@dataclasses.dataclass
+class RuleMatchInfo:
+    """Holds a rule together with the mapping that matched the rule."""
+
+    rule: CanonicalPatternRule
+    mapping: dict[str, AstBase]
+
+
+class PatternOptimizer2(InstructionOptimizer):  # type: ignore[misc]
+    """An optimizer that uses canonical pattern matching instead of fuzzy enumeration.
+
+    This optimizer keeps a list of canonical pattern rules.  For each
+    instruction it converts the microcode instruction into an AST,
+    canonicalises it, and attempts to match it against each rule's
+    canonical pattern.  Upon a successful match, it reconstructs the
+    candidate in the shape of the original pattern and delegates to
+    the rule to perform any additional checks and construct the
+    replacement instruction.
+    """
+
+    RULE_CLASSES = [CanonicalPatternRule]
+
+    def __init__(
+        self,
+        maturities: list[int],
+        stats: OptimizationStatistics,
+        log_dir: str | None = None,
+    ) -> None:
+        super().__init__(maturities, stats, log_dir=log_dir)
+        # Keep rules in a simple list; no PatternStorage is needed
+        self.rules: list[CanonicalPatternRule] = []
+
+    def add_rule(self, rule: CanonicalPatternRule) -> bool:  # type: ignore[override]
+        # Let the superclass decide whether the rule should be added
+        is_ok = super().add_rule(rule)
+        if not is_ok:
+            return False
+        # Cache the canonical pattern now to detect errors early
+        _ = rule.canonical_pattern
+        self.rules.append(rule)
+        return True
+
+    def get_optimized_instruction(self, blk: mblock_t | None, ins: minsn_t) -> minsn_t | None:  # type: ignore[override]
+        # Respect the current maturity as in the original implementation
+        if blk is not None:
+            self.cur_maturity = blk.mba.maturity
+        if self.cur_maturity not in self.maturities:
+            return None
+        # If no rules are configured, skip conversion altogether
+        if len(self.rules) == 0:
+            if optimizer_logger.debug_on:
+                optimizer_logger.debug(
+                    "[PatternOptimizer] No canonical rules configured, skipping"
+                )
+            return None
+        # Convert the instruction to an AST
+        tmp_ast = minsn_to_ast(ins)
+        if tmp_ast is None:
+            if optimizer_logger.debug_on:
+                optimizer_logger.debug(
+                    "[PatternOptimizer] minsn_to_ast failed, skipping"
+                )
+            return None
+        # Canonicalise the candidate once
+        canonical_candidate = canonicalize_ast(tmp_ast)
+        # Try each rule in order
+        for rule in self.rules:
+            canonical_pattern = rule.canonical_pattern
+            mapping: dict[str, AstBase] = {}
+            # Attempt to match the canonical pattern against the canonical candidate
+            if not match_pattern(canonical_pattern, canonical_candidate, mapping):
+                continue
+            # Reconstruct the candidate AST in the shape of the original pattern
+            candidate_ast_for_rule = substitute_pattern(rule.PATTERN, mapping)
+            if candidate_ast_for_rule is None:
+                continue
+            # We need a fresh copy of the pattern AST for mops copying
+            # Deepcopy is used because rule.PATTERN is reused across matches
+            candidate_pattern = copy.deepcopy(rule.PATTERN)
+            candidate_pattern.reset_mops()
+            # Copy the mops from the reconstructed candidate into the pattern
+            try:
+                if not candidate_pattern.check_pattern_and_copy_mops(
+                    candidate_ast_for_rule
+                ):
+                    continue
+            except Exception:
+                # In case the check fails unexpectedly, skip this match
+                continue
+            # Allow the rule to perform custom candidate checks (e.g. equal_mops_ignore_size)
+            try:
+                if not rule.check_candidate(candidate_pattern):
+                    continue
+            except Exception:
+                # If the candidate check raises, log and skip
+                optimizer_logger.error(
+                    "Error during candidate check for rule %s",
+                    rule,
+                    exc_info=True,
+                )
+                continue
+            # Build the replacement instruction via the rule's replacement pattern
+            try:
+                new_instruction = rule.get_replacement(candidate_pattern)
+            except Exception:
+                optimizer_logger.error(
+                    "Error during replacement construction for rule %s",
+                    rule,
+                    exc_info=True,
+                )
+                continue
+            if new_instruction is not None:
+                # Update usage statistics as in the original implementation
+                self.rules_usage_info[rule.name] += 1
+                if optimizer_logger.info_on:
+                    optimizer_logger.info(
+                        "Rule %s matched in maturity %s:",
+                        rule.name,
+                        self.cur_maturity,
+                    )
+                    optimizer_logger.info("  orig: %s", format_minsn_t(ins))
+                    optimizer_logger.info(
+                        "  new : %s",
+                        format_minsn_t(new_instruction),
+                    )
+                return new_instruction
+        # No rule matched
+        return None
+
+
+# === Example canonical rules ===
+#
+# The following classes illustrate how to port existing pattern rules to
+# the canonical matcher.  They inherit from CanonicalPatternRule
+# instead of the original PatternMatchingRule.  The PATTERN and
+# REPLACEMENT_PATTERN definitions are unchanged.  Any custom
+# check_candidate() methods are preserved.  When these classes are
+# registered with the PatternOptimizer above, operand orderings are
+# handled automatically by canonicalisation.
+
+
+class Add_HackersDelightRule_1(CanonicalPatternRule):
+    @property
+    def PATTERN(self) -> AstNode:
+        # x - (~y + 1) → x + y
+        return AstNode(
+            m_sub,
+            AstLeaf("x_0"),
+            AstNode(m_add, AstNode(m_bnot, AstLeaf("x_1")), AstConstant("1", 1)),
+        )
+
+    @property
+    def REPLACEMENT_PATTERN(self) -> AstNode:
+        return AstNode(m_add, AstLeaf("x_0"), AstLeaf("x_1"))
+
+
+class Add_HackersDelightRule_2(CanonicalPatternRule):
+    @property
+    def PATTERN(self) -> AstNode:
+        # (x XOR y) + 2*(x & y) → x + y
+        return AstNode(
+            m_add,
+            AstNode(m_xor, AstLeaf("x_0"), AstLeaf("x_1")),
+            AstNode(
+                m_mul,
+                AstConstant("2", 2),
+                AstNode(m_and, AstLeaf("x_0"), AstLeaf("x_1")),
+            ),
+        )
+
+    @property
+    def REPLACEMENT_PATTERN(self) -> AstNode:
+        return AstNode(m_add, AstLeaf("x_0"), AstLeaf("x_1"))
+
+
+class Add_HackersDelightRule_3(CanonicalPatternRule):
+    @property
+    def PATTERN(self) -> AstNode:
+        # (x OR y) + (x & y) → x + y
+        return AstNode(
+            m_add,
+            AstNode(m_or, AstLeaf("x_0"), AstLeaf("x_1")),
+            AstNode(m_and, AstLeaf("x_0"), AstLeaf("x_1")),
+        )
+
+    @property
+    def REPLACEMENT_PATTERN(self) -> AstNode:
+        return AstNode(m_add, AstLeaf("x_0"), AstLeaf("x_1"))
+
+
+class Add_HackersDelightRule_4(CanonicalPatternRule):
+    @property
+    def PATTERN(self) -> AstNode:
+        # 2*(x OR y) - (x XOR y) → x + y
+        return AstNode(
+            m_sub,
+            AstNode(
+                m_mul,
+                AstConstant("2", 2),
+                AstNode(m_or, AstLeaf("x_0"), AstLeaf("x_1")),
+            ),
+            AstNode(m_xor, AstLeaf("x_0"), AstLeaf("x_1")),
+        )
+
+    @property
+    def REPLACEMENT_PATTERN(self) -> AstNode:
+        return AstNode(m_add, AstLeaf("x_0"), AstLeaf("x_1"))
+
+
+class Add_HackersDelightRule_5(CanonicalPatternRule):
+    @property
+    def PATTERN(self) -> AstNode:
+        # 2*( (x OR y) OR z ) - (x XOR (y OR z)) → x + (y OR z)
+        return AstNode(
+            m_sub,
+            AstNode(
+                m_mul,
+                AstConstant("2", 2),
+                AstNode(
+                    m_or,
+                    AstNode(m_or, AstLeaf("x_0"), AstLeaf("x_1")),
+                    AstLeaf("x_2"),
+                ),
+            ),
+            AstNode(
+                m_xor, AstLeaf("x_0"), AstNode(m_or, AstLeaf("x_1"), AstLeaf("x_2"))
+            ),
+        )
+
+    @property
+    def REPLACEMENT_PATTERN(self) -> AstNode:
+        return AstNode(
+            m_add, AstLeaf("x_0"), AstNode(m_or, AstLeaf("x_1"), AstLeaf("x_2"))
+        )
+
+
+class Add_SpecialConstantRule_1(CanonicalPatternRule):
+    def check_candidate(self, candidate):
+        # c1 and c2 must have the same mop ignoring size
+        return equal_mops_ignore_size(candidate["c_1"].mop, candidate["c_2"].mop)
+
+    @property
+    def PATTERN(self) -> AstNode:
+        # (x XOR c1) + 2*(x & c2) → x + c1  where c1 == c2 ignoring size
+        return AstNode(
+            m_add,
+            AstNode(m_xor, AstLeaf("x_0"), AstConstant("c_1")),
+            AstNode(
+                m_mul,
+                AstConstant("2", 2),
+                AstNode(m_and, AstLeaf("x_0"), AstConstant("c_2")),
+            ),
+        )
+
+    @property
+    def REPLACEMENT_PATTERN(self) -> AstNode:
+        return AstNode(m_add, AstLeaf("x_0"), AstConstant("c_1"))
+
+
+class Add_SpecialConstantRule_2(CanonicalPatternRule):
+    def check_candidate(self, candidate):
+        # This rule is intentionally conservative: c1 & 0xFF == c2
+        return (candidate["c_1"].value & 0xFF) == candidate["c_2"].value
+
+    @property
+    def PATTERN(self) -> AstNode:
+        # ((x & 0xFF) XOR c1) + 2*(x & c2) → (x & 0xFF) + c1  with constraint above
+        return AstNode(
+            m_add,
+            AstNode(
+                m_xor,
+                AstNode(m_and, AstLeaf("x_0"), AstConstant("val_ff", 0xFF)),
+                AstConstant("c_1"),
+            ),
+            AstNode(
+                m_mul,
+                AstConstant("2", 2),
+                AstNode(m_and, AstLeaf("x_0"), AstConstant("c_2")),
+            ),
+        )
+
+    @property
+    def REPLACEMENT_PATTERN(self) -> AstNode:
+        return AstNode(
+            m_add,
+            AstNode(m_and, AstLeaf("x_0"), AstConstant("val_ff", 0xFF)),
+            AstConstant("c_1"),
+        )
+
+
+class Add_SpecialConstantRule_3(CanonicalPatternRule):
+    def check_candidate(self, candidate):
+        # c1 == ~c2; introduce val_res = c2 - 1
+        if not equal_bnot_mop(candidate["c_1"].mop, candidate["c_2"].mop):
+            return False
+        candidate.add_constant_leaf(
+            "val_res",
+            candidate["c_2"].value - 1,
+            candidate["x_0"].size,
+        )
+        return True
+
+    @property
+    def PATTERN(self) -> AstNode:
+        # (x XOR c1) + 2*(x OR c2) → x + (c2 - 1)  when c1 == ~c2
+        return AstNode(
+            m_add,
+            AstNode(m_xor, AstLeaf("x_0"), AstConstant("c_1")),
+            AstNode(
+                m_mul,
+                AstConstant("2", 2),
+                AstNode(m_or, AstLeaf("x_0"), AstConstant("c_2")),
+            ),
+        )
+
+    @property
+    def REPLACEMENT_PATTERN(self) -> AstNode:
+        return AstNode(m_add, AstLeaf("x_0"), AstConstant("val_res"))
+
+
+class Add_OllvmRule_1(CanonicalPatternRule):
+    def check_candidate(self, candidate):
+        # Introduce val_1 = 1 (same size as the operands)
+        candidate.add_constant_leaf("val_1", 1, candidate.size)
+        return True
+
+    @property
+    def PATTERN(self) -> AstNode:
+        # ~(x XOR y) + 2*(x OR y) → (x + y) - 1
+        return AstNode(
+            m_add,
+            AstNode(m_bnot, AstNode(m_xor, AstLeaf("x_0"), AstLeaf("x_1"))),
+            AstNode(
+                m_mul,
+                AstConstant("2", 2),
+                AstNode(m_or, AstLeaf("x_1"), AstLeaf("x_0")),
+            ),
+        )
+
+    @property
+    def REPLACEMENT_PATTERN(self) -> AstNode:
+        return AstNode(
+            m_sub,
+            AstNode(m_add, AstLeaf("x_0"), AstLeaf("x_1")),
+            AstConstant("val_1"),
+        )
+
+
+class Add_OllvmRule_2(CanonicalPatternRule):
+    def check_candidate(self, candidate):
+        # Only valid when (val_fe + 2) & AND_TABLE[size] == 0
+        if (candidate["val_fe"].value + 2) & AND_TABLE[candidate["val_fe"].size] != 0:
+            return False
+        candidate.add_constant_leaf("val_1", 1, candidate.size)
+        return True
+
+    @property
+    def PATTERN(self) -> AstNode:
+        # ~(x XOR y) - val_fe*(x OR y) → (x + y) - 1  when constraint holds
+        return AstNode(
+            m_sub,
+            AstNode(m_bnot, AstNode(m_xor, AstLeaf("x_0"), AstLeaf("x_1"))),
+            AstNode(
+                m_mul,
+                AstConstant("val_fe"),
+                AstNode(m_or, AstLeaf("x_0"), AstLeaf("x_1")),
+            ),
+        )
+
+    @property
+    def REPLACEMENT_PATTERN(self) -> AstNode:
+        return AstNode(
+            m_sub,
+            AstNode(m_add, AstLeaf("x_0"), AstLeaf("x_1")),
+            AstConstant("val_1"),
+        )
+
+
+class Add_OllvmRule_3(CanonicalPatternRule):
+    @property
+    def PATTERN(self) -> AstNode:
+        # (x XOR y) + 2*(x & y) → x + y
+        return AstNode(
+            m_add,
+            AstNode(m_xor, AstLeaf("x_0"), AstLeaf("x_1")),
+            AstNode(
+                m_mul,
+                AstConstant("2", 2),
+                AstNode(m_and, AstLeaf("x_0"), AstLeaf("x_1")),
+            ),
+        )
+
+    @property
+    def REPLACEMENT_PATTERN(self) -> AstNode:
+        return AstNode(m_add, AstLeaf("x_0"), AstLeaf("x_1"))
+
+
+class Add_OllvmRule_4(CanonicalPatternRule):
+    @property
+    def PATTERN(self) -> AstNode:
+        # (x XOR y) - val_fe*(x & y) → x + y
+        return AstNode(
+            m_sub,
+            AstNode(m_xor, AstLeaf("x_0"), AstLeaf("x_1")),
+            AstNode(
+                m_mul,
+                AstConstant("val_fe"),
+                AstNode(m_and, AstLeaf("x_0"), AstLeaf("x_1")),
+            ),
+        )
+
+    @property
+    def REPLACEMENT_PATTERN(self) -> AstNode:
+        return AstNode(m_add, AstLeaf("x_0"), AstLeaf("x_1"))
+
+
+class AddXor_Rule_1(CanonicalPatternRule):
+    def check_candidate(self, candidate):
+        # x1 == ~bnot_x1
+        if not equal_bnot_mop(candidate["x_1"].mop, candidate["bnot_x_1"].mop):
+            return False
+        candidate.add_constant_leaf("val_2", 2, candidate["x_0"].size)
+        return True
+
+    @property
+    def PATTERN(self) -> AstNode:
+        # (x0 - x1) - 2*(x0 OR ~x1) → (x0 XOR x1) + 2
+        return AstNode(
+            m_sub,
+            AstNode(m_sub, AstLeaf("x_0"), AstLeaf("x_1")),
+            AstNode(
+                m_mul,
+                AstConstant("2", 2),
+                AstNode(m_or, AstLeaf("x_0"), AstLeaf("bnot_x_1")),
+            ),
+        )
+
+    @property
+    def REPLACEMENT_PATTERN(self) -> AstNode:
+        return AstNode(
+            m_add,
+            AstNode(m_xor, AstLeaf("x_0"), AstLeaf("x_1")),
+            AstConstant("val_2"),
+        )
+
+
+class AddXor_Rule_2(CanonicalPatternRule):
+    def check_candidate(self, candidate):
+        # x0 == ~bnot_x0
+        if not equal_bnot_mop(candidate["x_0"].mop, candidate["bnot_x_0"].mop):
+            return False
+        candidate.add_constant_leaf("val_2", 2, candidate["x_0"].size)
+        return True
+
+    @property
+    def PATTERN(self) -> AstNode:
+        # (x0 - x1) - 2*~( (~x0) & x1 ) → (x0 XOR x1) + 2
+        return AstNode(
+            m_sub,
+            AstNode(m_sub, AstLeaf("x_0"), AstLeaf("x_1")),
+            AstNode(
+                m_mul,
+                AstConstant("2", 2),
+                AstNode(m_bnot, AstNode(m_and, AstLeaf("bnot_x_0"), AstLeaf("x_1"))),
+            ),
+        )
+
+    @property
+    def REPLACEMENT_PATTERN(self) -> AstNode:
+        return AstNode(
+            m_add,
+            AstNode(m_xor, AstLeaf("x_0"), AstLeaf("x_1")),
+            AstLeaf("val_2"),
+        )
diff --git a/src/d810/optimizers/microcode/instructions/handler.py b/src/d810/optimizers/microcode/instructions/handler.py
index 3b3a5ae..74d98d8 100644
--- a/src/d810/optimizers/microcode/instructions/handler.py
+++ b/src/d810/optimizers/microcode/instructions/handler.py
@@ -13,6 +13,9 @@ from d810.hexrays.hexrays_formatters import format_minsn_t, maturity_to_string
 from d810.optimizers.microcode.handler import OptimizationRule
 from d810.registry import Registrant
 
+if typing.TYPE_CHECKING:
+    from d810.stats import OptimizationStatistics
+
 d810_logger = getLogger("D810")
 optimizer_logger = getLogger("D810.optimizer")
 
@@ -145,12 +148,15 @@ class InstructionOptimizer(Registrant, typing.Generic[T_Rule]):
     RULE_CLASSES: list[typing.Type[T_Rule]] = []
     NAME = None
 
-    def __init__(self, maturities: list[int], log_dir=None):
+    def __init__(
+        self, maturities: list[int], stats: OptimizationStatistics, log_dir=None
+    ):
         self.rules: set[T_Rule] = set()
-        self.rules_usage_info: dict[str, int] = {}
         self.maturities = maturities
         self.log_dir = log_dir
         self.cur_maturity = ida_hexrays.MMAT_PREOPTIMIZED
+        # Centralized statistics collector injected by the manager
+        self.stats = stats
 
     def add_rule(self, rule: T_Rule) -> bool:
         is_valid_rule_class = False
@@ -165,26 +171,20 @@ class InstructionOptimizer(Registrant, typing.Generic[T_Rule]):
         if len(rule.maturities) == 0:
             rule.maturities = self.maturities
         self.rules.add(rule)
-        self.rules_usage_info[rule.name] = 0
         return True
 
-    def reset_rule_usage_statistic(self):
-        self.rules_usage_info = {}
-        for rule in self.rules:
-            self.rules_usage_info[rule.name] = 0
-
-    def show_rule_usage_statistic(self):
-        for rule_name, rule_nb_match in self.rules_usage_info.items():
-            if rule_nb_match > 0:
-                d810_logger.info(
-                    "Instruction Rule '%s' has been used %d times",
-                    rule_name,
-                    rule_nb_match,
-                )
-
     def get_optimized_instruction(
         self, blk: ida_hexrays.mblock_t, ins: ida_hexrays.minsn_t
     ) -> ida_hexrays.minsn_t | None:
+        # Fast opcode gate for chain rules to avoid work on unrelated instructions
+        # Only applies to optimizers whose rules expose a "TARGET_OPCODES" set.
+        try:
+            target_opcodes = getattr(self, "_allowed_root_opcodes", None)
+            if target_opcodes:
+                if ins.opcode not in target_opcodes:
+                    return None
+        except Exception:
+            pass
         if blk is not None:
             self.cur_maturity = blk.mba.maturity
         # This was commented out in the original code,
@@ -199,7 +199,8 @@ class InstructionOptimizer(Registrant, typing.Generic[T_Rule]):
             try:
                 new_ins = rule.check_and_replace(blk, ins)
                 if new_ins is not None:
-                    self.rules_usage_info[rule.name] += 1
+                    if self.stats is not None:
+                        self.stats.record_instruction_rule_match(rule_name=rule.name)
                     optimizer_logger.info(
                         "Rule %s matched in maturity %s:",
                         rule.name,
diff --git a/src/d810/optimizers/microcode/instructions/pattern_matching/handler.py b/src/d810/optimizers/microcode/instructions/pattern_matching/handler.py
index 42c7aaf..363aabd 100644
--- a/src/d810/optimizers/microcode/instructions/pattern_matching/handler.py
+++ b/src/d810/optimizers/microcode/instructions/pattern_matching/handler.py
@@ -17,6 +17,9 @@ from d810.optimizers.microcode.instructions.handler import (
 optimizer_logger = getLogger("D810.optimizer")
 pattern_search_logger = getLogger("D810.pattern_search")
 
+if typing.TYPE_CHECKING:
+    from d810.stats import OptimizationStatistics
+
 
 class PatternMatchingRule(GenericPatternRule):
     FUZZ_PATTERN: bool = True
@@ -56,7 +59,7 @@ class PatternMatchingRule(GenericPatternRule):
         else:
             self.pattern_candidates = ast_generator(self.PATTERN)
         if self.PATTERNS is not None:
-            self.pattern_candidates += [x for x in self.PATTERNS]
+            self.pattern_candidates += list(self.PATTERNS)
 
     def check_pattern_and_replace(self, candidate_pattern: AstNode, test_ast: AstNode):
         if optimizer_logger.debug_on:
@@ -97,7 +100,7 @@ class PatternMatchingRule(GenericPatternRule):
 @dataclasses.dataclass
 class RulePatternInfo:
     rule: InstructionOptimizationRule
-    pattern: AstNode
+    pattern: AstBase
 
 
 def signature_generator(ref_sig):
@@ -119,7 +122,7 @@ class PatternStorage(object):
         self.next_layer_patterns: dict[str, tuple[list[str], PatternStorage]] = {}
         self.rule_resolved = []
 
-    def add_pattern_for_rule(self, pattern: AstNode, rule: InstructionOptimizationRule):
+    def add_pattern_for_rule(self, pattern: AstBase, rule: InstructionOptimizationRule):
         layer_signature = self.layer_signature_to_key(
             pattern.get_depth_signature(self.depth)
         )
@@ -246,9 +249,14 @@ class PatternOptimizer(InstructionOptimizer):
 
     RULE_CLASSES = [PatternMatchingRule]
 
-    def __init__(self, maturities, log_dir=None):
-        super().__init__(maturities, log_dir=log_dir)
+    def __init__(self, maturities, stats: "OptimizationStatistics", log_dir=None):
+        super().__init__(maturities, stats, log_dir=log_dir)
         self.pattern_storage = PatternStorage(depth=1)
+        # Fast-path filter: restrict AST building to instructions whose opcode
+        # appears as the root opcode of at least one registered rule pattern.
+        # This avoids paying the cost of minsn_to_ast() for obviously
+        # incompatible instructions.
+        self._allowed_root_opcodes: set[int] = set()
 
     def add_rule(self, rule: PatternMatchingRule):
         is_ok = super().add_rule(rule)
@@ -261,6 +269,15 @@ class PatternOptimizer(InstructionOptimizer):
                     str(pattern),
                 )
             self.pattern_storage.add_pattern_for_rule(pattern, rule)
+            # Collect root opcode for quick opcode pre-filtering
+            try:
+                # Only AstNode instances have an opcode. Guarding with isinstance
+                # keeps static type-checkers happy.
+                if isinstance(pattern, AstNode) and pattern.opcode is not None:
+                    self._allowed_root_opcodes.add(int(pattern.opcode))
+            except Exception:
+                # Be permissive: failure to extract opcode just means we won't pre-filter it
+                pass
         return True
 
     def get_optimized_instruction(self, blk: mblock_t, ins: minsn_t) -> minsn_t | None:
@@ -278,6 +295,16 @@ class PatternOptimizer(InstructionOptimizer):
                 )
             return None
 
+        # Opcode pre-filter: if the instruction opcode is never used as a root
+        # by any registered rule, there is no point in attempting an AST build.
+        # This shortcut has a substantial impact on hot paths.
+        try:
+            if ins.opcode not in self._allowed_root_opcodes:
+                return None
+        except Exception:
+            # If anything goes wrong while reading opcode, fall through to safe path
+            pass
+
         tmp = minsn_to_ast(ins)
         if tmp is None:
             if optimizer_logger.debug_on:
@@ -301,7 +328,6 @@ class PatternOptimizer(InstructionOptimizer):
                     rule_pattern_info.pattern, tmp
                 )
                 if new_ins is not None:
-                    self.rules_usage_info[rule_pattern_info.rule.name] += 1
                     if optimizer_logger.info_on:
                         optimizer_logger.info(
                             "Rule %s matched in maturity %s:",
@@ -313,6 +339,11 @@ class PatternOptimizer(InstructionOptimizer):
                             "  new : %s",
                             format_minsn_t(new_ins),
                         )
+                    if self.stats is not None:
+                        self.stats.record_instruction_rule_match(
+                            rule_name=rule_pattern_info.rule.name
+                        )
+
                     return new_ins
             except RuntimeError as e:
                 optimizer_logger.error(
@@ -391,7 +422,7 @@ def get_opcode_operands(ref_opcode: int, ast_node: AstBase) -> list[AstBase]:
     if not isinstance(ast_node, AstBase) or not ast_node.is_node():
         return [ast_node]
     ast_node = typing.cast(AstNode, ast_node)
-    if ast_node.opcode == ref_opcode:
+    if ast_node.opcode is not None and ast_node.opcode == ref_opcode:
         left = (
             get_opcode_operands(ref_opcode, ast_node.left)
             if ast_node.left is not None
@@ -408,6 +439,8 @@ def get_opcode_operands(ref_opcode: int, ast_node: AstBase) -> list[AstBase]:
 
 
 def get_similar_opcode_operands(ast_node: AstNode) -> list[AstNode]:
+    if ast_node.opcode is None:
+        return [ast_node]
     if ast_node.opcode in [m_add, m_sub]:
         add_elts = get_addition_operands(ast_node)
         all_add_ordering = get_all_binary_tree_representation(add_elts)
@@ -416,7 +449,7 @@ def get_similar_opcode_operands(ast_node: AstNode) -> list[AstNode]:
             ast_res.append(generate_ast(m_add, leaf_ordering))
         return ast_res
     elif ast_node.opcode in [m_xor, m_or, m_and, m_mul]:
-        same_elts = get_opcode_operands(ast_node.opcode, ast_node)
+        same_elts = get_opcode_operands(int(ast_node.opcode), ast_node)
         all_same_ordering = get_all_binary_tree_representation(same_elts)
         ast_res = []
         for leaf_ordering in all_same_ordering:
@@ -484,9 +517,10 @@ def ast_generator(ast_node: AstBase | None, excluded_opcodes=None) -> list[AstBa
                     for sub_ast_right in sub_ast_right_list:
                         sub_ast_left = typing.cast(AstNode, sub_ast_left)
                         sub_ast_right = typing.cast(AstNode, sub_ast_right)
-                        res_ast += get_ast_variations_with_add_sub(
-                            ast_node.opcode, sub_ast_left, sub_ast_right
-                        )
+                        if ast_node.opcode is not None:
+                            res_ast += get_ast_variations_with_add_sub(
+                                int(ast_node.opcode), sub_ast_left, sub_ast_right
+                            )
             return res_ast
     if ast_node.opcode not in [m_add, m_sub, m_or, m_and, m_mul]:
         excluded_opcodes = []
@@ -511,8 +545,9 @@ def ast_generator(ast_node: AstBase | None, excluded_opcodes=None) -> list[AstBa
             for sub_ast_right in sub_ast_right_list:
                 sub_ast_left = typing.cast(AstNode, sub_ast_left)
                 sub_ast_right = typing.cast(AstNode, sub_ast_right)
-                res_ast += get_ast_variations_with_add_sub(
-                    ast_node.opcode, sub_ast_left, sub_ast_right
-                )
+                if ast_node.opcode is not None:
+                    res_ast += get_ast_variations_with_add_sub(
+                        int(ast_node.opcode), sub_ast_left, sub_ast_right
+                    )
         return res_ast
     return []
diff --git a/src/d810/optimizers/microcode/instructions/peephole/constant_call.py b/src/d810/optimizers/microcode/instructions/peephole/constant_call.py
index b384b70..3e4b39f 100644
--- a/src/d810/optimizers/microcode/instructions/peephole/constant_call.py
+++ b/src/d810/optimizers/microcode/instructions/peephole/constant_call.py
@@ -39,7 +39,11 @@ class ConstantCallResultFoldRule(PeepholeSimplificationRule):
 
     def __init__(self, *args: typing.Any, **kwargs: typing.Any) -> None:
         super().__init__(*args, **kwargs)
-        self.maturities = [ida_hexrays.MMAT_LOCOPT, ida_hexrays.MMAT_CALLS]
+        self.maturities = [
+            ida_hexrays.MMAT_LOCOPT,
+            ida_hexrays.MMAT_CALLS,
+            ida_hexrays.MMAT_GLBOPT1,
+        ]
 
     @example(
         "opcode=call l=<mop_t type=mop_h size=-1 dstr=!__ROL8__> r=<mop_t type=mop_z size=-1 dstr=> d=<mop_t type=mop_f size=8 dstr=<fast:_QWORD #0x33637E66.8,char #4.1>.8>"
diff --git a/src/d810/optimizers/microcode/instructions/peephole/fold_readonlydata.py b/src/d810/optimizers/microcode/instructions/peephole/fold_readonlydata.py
index 820cb29..147aaf6 100644
--- a/src/d810/optimizers/microcode/instructions/peephole/fold_readonlydata.py
+++ b/src/d810/optimizers/microcode/instructions/peephole/fold_readonlydata.py
@@ -51,6 +51,20 @@ def _ea_from_indirect_load(
         return None
     base_ea = def_ins.l.s.start_ea
     return base_ea + disp
+    
+error:
+
+.text:00000001800066D5 C38 48 8B 05 0E 36 06 00                  mov     rax, cs:_qword_1802D2C99+2                                  ; jumptable 0000000180004869 case 134
+.text:00000001800066DC C38 48 89 84 24 00 06 00 00               mov     [rsp+0C38h+var_638], rax
+.text:0000000180006B2B C38 48 8B 84 24 00 06 00 00               mov     rax, [rsp+0C38h+var_638] 
+.text:0000000180006B66 C38 48 89 84 24 A8 05 00 00               mov     [rsp+0C38h+var_690], rax
+.text:0000000180006B7E C38 48 8B 84 24 A8 05 00 00               mov     rax, [rsp+0C38h+var_690]
+.text:0000000180006B8E C38 48 89 84 24 A0 05 00 00               mov     [rsp+0C38h+var_698], rax
+.text:0000000180006BA6 C38 48 8B 84 24 A0 05 00 00               mov     rax, [rsp+0C38h+var_698] 
+.text:0000000180006BAE C38 48 8B 80 20 03 00 00                  mov     rax, [rax+320h]
+.text:0000000180006BB5 C38 48 89 84 24 98 05 00 00               mov     [rsp+0C38h+var_6A0], rax
+
+this becomes: `(__ROL8__(MEMORY[0xB10000007FFE03FD]...)` which is obviously wrong.
 """
 
 
@@ -62,6 +76,7 @@ import idaapi
 
 import d810._compat as _compat
 from d810.conf.loggers import getLogger
+from d810.hexrays.hexrays_helpers import extract_literal_from_mop
 from d810.optimizers.microcode.instructions.peephole.handler import (
     PeepholeSimplificationRule,
 )
@@ -79,7 +94,12 @@ class FoldReadonlyDataRule(PeepholeSimplificationRule):
 
     def __init__(self, *args: typing.Any, **kwargs: typing.Any) -> None:
         super().__init__(*args, **kwargs)
-        self.maturities = [ida_hexrays.MMAT_LOCOPT, ida_hexrays.MMAT_CALLS]
+        # Run where the IR is stable enough to rewrite `ldx` forms safely.
+        self.maturities = [
+            ida_hexrays.MMAT_LOCOPT,
+            ida_hexrays.MMAT_CALLS,
+            getattr(ida_hexrays, "MMAT_GLBOPT1", ida_hexrays.MMAT_CALLS),
+        ]
 
     # --------------------------------------------------------------------- #
     # Helper functions                                                      #
@@ -90,10 +110,12 @@ class FoldReadonlyDataRule(PeepholeSimplificationRule):
         seg = ida_segment.getseg(addr)
         if seg is None:
             return False
-        # A read-only segment has READ perm but no WRITE perm.
-        return (seg.perm & idaapi.SEGPERM_READ) and not (
-            seg.perm & idaapi.SEGPERM_WRITE
-        )
+        # Treat as readonly DATA: has READ, no WRITE, and not EXECUTE.
+        perms = seg.perm
+        has_read = bool(perms & idaapi.SEGPERM_READ)
+        has_write = bool(perms & idaapi.SEGPERM_WRITE)
+        has_exec = bool(perms & idaapi.SEGPERM_EXEC)
+        return has_read and not has_write and not has_exec
 
     @staticmethod
     def _fetch_constant(addr: int, size: int) -> Optional[int]:
@@ -123,27 +145,42 @@ class FoldReadonlyDataRule(PeepholeSimplificationRule):
 
         # Attempt the **direct displacement** form first ------------------ #
         ea = self._ea_from_direct_load(ins)
+        folded_from_mov = False
+        if ea is None:
+            # Try folding readonly globals used as plain values inside
+            # expression trees (e.g., nested under mop_d). Do NOT fold top-level
+            # mov of addresses (e.g., function pointers / IAT entries) into
+            # immediates – that breaks call-site rendering.
+            expr_folded = self._fold_readonly_operands_in_expr(ins)
+            return expr_folded
+
+        # We have an effective address for a memory load.  Is it really read-only?
         if ea is None:
             return None
-
-        # We have an effective address.  Is it really read-only?
         if not self._segment_is_read_only(ea):
             return None
 
-        # Fetch the literal constant.
-        value = self._fetch_constant(ea, ins.d.size)
+        # Compute the immediate value from memory contents at the EA.
+        # Use the destination size when available, otherwise try source size.
+        load_size = ins.d.size if (ins.d and ins.d.size) else ins.l.size
+        if not load_size:
+            return None
+        value = self._fetch_constant(ea, load_size)
         if value is None:
             return None
 
         # ------------------------------------------------------------------
-        # Build a brand-new `ldc` instruction so that we do not mutate *ins* in
-        # place (Hex-Rays validator dislikes opcode changes in-situ).
+        # Build the replacement instruction. For true `ldx` loads, use
+        # `ldc #imm, dst`.
         # ------------------------------------------------------------------
         new_ins = ida_hexrays.minsn_t(ins.ea)
         new_ins.opcode = ida_hexrays.m_ldc
 
         cst = ida_hexrays.mop_t()
-        cst.make_number(value, ins.d.size)
+        # Use unsigned constant form to avoid unexpected sign-extension.
+        if load_size in (1, 2, 4, 8) and value < 0:
+            value &= (1 << (load_size * 8)) - 1
+        cst.make_number(value, load_size)
         new_ins.l = cst
 
         # Keep original destination when it is a legal l-value, otherwise erase.
@@ -157,12 +194,12 @@ class FoldReadonlyDataRule(PeepholeSimplificationRule):
             new_ins.d.assign(ins.d)
         else:
             new_ins.d.erase()
-        new_ins.d.size = ins.d.size
+        new_ins.d.size = load_size
 
-        # r operand is empty for ldc
+        # r operand must be empty
         new_ins.r = ida_hexrays.mop_t()
         new_ins.r.erase()
-        new_ins.r.size = ins.d.size
+        # do not set size on an empty operand
 
         return new_ins
 
@@ -224,3 +261,192 @@ class FoldReadonlyDataRule(PeepholeSimplificationRule):
             return base + off
 
         return None
+
+    @staticmethod
+    def _ea_from_simple_mov_load(ins: ida_hexrays.minsn_t) -> Optional[int]:
+        """Resolve EA for early `mov`-based memory loads.
+
+        Pattern handled (typically seen at pre-optimized maturity)::
+
+            mov  $_qword_xxx@?.8, rX.8
+
+        Where the left operand is a direct reference to a global/readonly
+        location (represented as `mop_v` or `mop_S`).
+        """
+
+        if ins.opcode != ida_hexrays.m_mov:
+            return None
+
+        # Source must be a direct global/addr operand; destination is ignored
+        # here (the caller will validate size and build the `ldc`).
+        src = ins.l
+        if src is None:
+            return None
+
+        if src.t == ida_hexrays.mop_v:
+            return src.g
+        if src.t == ida_hexrays.mop_S:
+            # Only accept true address-bearing symbols. In pre-optimized IR,
+            # `mop_S` can also denote stack variables (`stkvar_ref_t`) which do
+            # not have an EA and must be ignored.
+            start_ea = getattr(src.s, "start_ea", None)
+            if start_ea is not None:
+                return start_ea
+            return None
+
+        return None
+
+    # ------------------------------------------------------------------ #
+    # Expression tree folding                                            #
+    # ------------------------------------------------------------------ #
+    def _fold_readonly_operands_in_expr(
+        self, ins: ida_hexrays.minsn_t
+    ) -> ida_hexrays.minsn_t | None:
+        """Return a copy of `ins` with any `mop_v`/`mop_S` operands that
+        reside in a read-only segment replaced by numeric immediates.
+
+        Only folds values used as r-values; addresses (`mop_a`) are not touched.
+        """
+
+        # Clone the instruction shallowly via operand assignment
+        new_ins = ida_hexrays.minsn_t(ins.ea)
+        new_ins.opcode = ins.opcode
+        new_ins.l = ida_hexrays.mop_t()
+        new_ins.l.assign(ins.l)
+        new_ins.r = ida_hexrays.mop_t()
+        new_ins.r.assign(ins.r)
+        new_ins.d = ida_hexrays.mop_t()
+        new_ins.d.assign(ins.d)
+
+        changed = False
+        changed |= self._fold_readonly_inplace(new_ins.l)
+        # Avoid folding the call target of call-like instructions. Folding a
+        # function pointer (e.g., IAT/vtable) into an immediate can confuse the
+        # decompiler into emitting spurious MEMORY[ea](...) calls.
+        m_icall = getattr(ida_hexrays, "m_icall", None)
+        if new_ins.opcode in (ida_hexrays.m_call, m_icall):
+            pass
+        else:
+            changed |= self._fold_readonly_inplace(new_ins.r)
+        # do not touch destination
+
+        return new_ins if changed else None
+
+    def _fold_readonly_inplace(self, op: ida_hexrays.mop_t) -> bool:
+        """Recursively fold `op` if it references a readonly global.
+
+        Returns True if the operand (or any nested operand) was modified.
+        """
+        if not op:
+            return False
+
+        # Nested expression: recurse into its operands
+        if op.t == ida_hexrays.mop_d:
+            inner: ida_hexrays.minsn_t = op.d
+            # Handle zero/sign-extend of a memory byte/word into an immediate when
+            # the effective address can be resolved to &sym + const in a RO segment.
+            if inner.opcode in (ida_hexrays.m_xdu, ida_hexrays.m_xds):
+                src = inner.l
+                if src and getattr(src, "t", None) == ida_hexrays.mop_b:
+                    ea = self._ea_from_mop_b(src)
+                    if ea is not None and self._segment_is_read_only(ea):
+                        mem_size = getattr(src, "size", 0) or 0
+                        out_size = (
+                            getattr(op, "size", 0) or getattr(inner, "size", 0) or 0
+                        )
+                        if mem_size in (1, 2, 4, 8) and out_size in (1, 2, 4, 8):
+                            val = self._fetch_constant(ea, mem_size)
+                            if val is not None:
+                                # Apply sign/zero extension
+                                if inner.opcode == ida_hexrays.m_xds:
+                                    sign_bit = 1 << (mem_size * 8 - 1)
+                                    if val & sign_bit:
+                                        val = val - (1 << (mem_size * 8))
+                                mask = (1 << (out_size * 8)) - 1
+                                folded = val & mask
+                                op.make_number(folded, out_size)
+                                return True
+            # Otherwise, recurse into sub-operands of the inner instruction
+            return self._fold_readonly_inplace(inner.l) or self._fold_readonly_inplace(
+                inner.r
+            )
+
+        # Address-of or pointer-like forms are not folded here
+        if op.t in {ida_hexrays.mop_a, ida_hexrays.mop_b}:
+            return False
+
+        size = op.size if getattr(op, "size", 0) else 0
+
+        # Do NOT fold plain symbolic addresses (mop_v/mop_S) into immediates.
+        # Those represent address values, not memory contents. Actual memory
+        # reads go through mop_b (handled above via xdu/xds) or ldx paths.
+        if op.t in (ida_hexrays.mop_v, ida_hexrays.mop_S):
+            return False
+
+        return False
+
+    # ------------------------------------------------------------------ #
+    # mop_b EA reconstruction                                            #
+    # ------------------------------------------------------------------ #
+    def _ea_from_mop_b(self, mop_b: ida_hexrays.mop_t) -> Optional[int]:
+        """Try to reconstruct EA from a memory operand (mop_b).
+
+        Handles patterns like [ds:( &sym + const )] or when the base is an add()
+        expression that combines an address-of operand with a constant.
+        """
+        try:
+            b = mop_b.b
+            i = mop_b.i
+        except Exception:
+            return None
+
+        def _addr_from_mop_a(mop_a: ida_hexrays.mop_t) -> Optional[int]:
+            inner = mop_a.a
+            if inner is None:
+                return None
+            if inner.t == ida_hexrays.mop_v:
+                return inner.g
+            if inner.t == ida_hexrays.mop_S:
+                return getattr(inner.s, "start_ea", None)
+            return None
+
+        def _const_from_mop(m: ida_hexrays.mop_t) -> Optional[int]:
+            if m is None:
+                return 0
+            if m.t == ida_hexrays.mop_n:
+                return m.nnn.value
+            lits = extract_literal_from_mop(m)
+            if lits and len(lits) == 1:
+                return lits[0][0]
+            return None
+
+        # Case 1: base is address-of symbol, optional constant in index
+        if b and b.t == ida_hexrays.mop_a:
+            base = _addr_from_mop_a(b)
+            if base is None:
+                return None
+            off = _const_from_mop(i)
+            if off is None:
+                return None
+            return base + off
+
+        # Case 2: base is an add() expression combining &sym and const
+        if b and b.t == ida_hexrays.mop_d and b.d and b.d.opcode == ida_hexrays.m_add:
+            add_ins = b.d
+            left, right = add_ins.l, add_ins.r
+            # Try both orders to find (&sym, const)
+            if left and left.t == ida_hexrays.mop_a:
+                base = _addr_from_mop_a(left)
+                if base is not None:
+                    off = _const_from_mop(right)
+                    if off is not None:
+                        return base + off
+            if right and right.t == ida_hexrays.mop_a:
+                base = _addr_from_mop_a(right)
+                if base is not None:
+                    off = _const_from_mop(left)
+                    if off is not None:
+                        return base + off
+
+        # Not a supported form
+        return None
diff --git a/src/d810/optimizers/microcode/instructions/z3/cst.py b/src/d810/optimizers/microcode/instructions/z3/cst.py
index ab8dc2b..5a1f24a 100644
--- a/src/d810/optimizers/microcode/instructions/z3/cst.py
+++ b/src/d810/optimizers/microcode/instructions/z3/cst.py
@@ -75,7 +75,7 @@ class Z3ConstantOptimization(Z3Rule):
             #   new_instruction = self.get_replacement(typing.cast(AstNode, tmp))
             #   return new_instruction
             c_res_mop = mop_t()
-            c_res_mop.make_number(val_0, tmp.mop.size)
+            c_res_mop.make_number(val_0, tmp.mop.size or 1)
             if z3_check_mop_equality(tmp.mop, c_res_mop):
                 if logger.debug_on:
                     logger.debug("  z3_check_mop_equality is equal")
diff --git a/src/d810/optimizers/microcode/instructions/z3/handler.py b/src/d810/optimizers/microcode/instructions/z3/handler.py
index 1f1bef2..f275754 100644
--- a/src/d810/optimizers/microcode/instructions/z3/handler.py
+++ b/src/d810/optimizers/microcode/instructions/z3/handler.py
@@ -1,5 +1,7 @@
 import abc
 
+import ida_hexrays
+
 from d810.expr.ast import AstNode
 from d810.optimizers.microcode.instructions.handler import (
     GenericPatternRule,
@@ -22,3 +24,31 @@ class Z3Rule(GenericPatternRule):
 
 class Z3Optimizer(InstructionOptimizer):
     RULE_CLASSES = [Z3Rule]
+
+    def __init__(self, maturities, stats, log_dir=None):
+        super().__init__(maturities, stats, log_dir)
+        self._allowed_root_opcodes: set[int] = set()
+
+    def add_rule(self, rule: Z3Rule) -> bool:  # type: ignore[override]
+        ok = super().add_rule(rule)
+        if not ok:
+            return False
+        try:
+            pat = rule.PATTERN
+            if isinstance(pat, AstNode) and pat.opcode is not None:
+                self._allowed_root_opcodes.add(int(pat.opcode))
+        except Exception:
+            pass
+        return True
+
+    def get_optimized_instruction(self, blk: ida_hexrays.mblock_t, ins: ida_hexrays.minsn_t):  # type: ignore[override]
+        # Cheap opcode pre-filter: if no Z3 rule targets this opcode, skip.
+        try:
+            if (
+                self._allowed_root_opcodes
+                and ins.opcode not in self._allowed_root_opcodes
+            ):
+                return None
+        except Exception:
+            pass
+        return super().get_optimized_instruction(blk, ins)
diff --git a/src/d810/optimizers/microcode/instructions/z3/predicates.py b/src/d810/optimizers/microcode/instructions/z3/predicates.py
index 4d9efea..dc34de8 100644
--- a/src/d810/optimizers/microcode/instructions/z3/predicates.py
+++ b/src/d810/optimizers/microcode/instructions/z3/predicates.py
@@ -64,9 +64,9 @@ class Z3lnotRuleGeneric(Z3Rule):
         return AstNode(m_mov, AstConstant("val_res"))
 
     def check_candidate(self, candidate):
-        val_0_mop = mop_t()
-        val_0_mop.make_number(0, candidate["x_0"].size)
         res_size = candidate["x_0"].size or 1
+        val_0_mop = mop_t()
+        val_0_mop.make_number(0, res_size)
         if z3_check_mop_equality(candidate["x_0"].mop, val_0_mop):
             candidate.add_constant_leaf("val_res", 1, res_size)
             return True
diff --git a/src/d810/registry.py b/src/d810/registry.py
index 6fb97d7..9143593 100644
--- a/src/d810/registry.py
+++ b/src/d810/registry.py
@@ -2,9 +2,11 @@ import collections
 import dataclasses
 import functools
 import importlib
+import sys
 from abc import ABCMeta
 from functools import cache, wraps
 from types import GenericAlias, MappingProxyType
+from collections.abc import MutableMapping
 from typing import (
     Annotated,
     Any,
@@ -70,6 +72,99 @@ DeferTypeRef: TypeAlias = Defer[type] | TypeRef
 """A typelike reference which can be wrapped to be resolved later."""
 
 
+def survives_reload(cls=None, *, reload_key: str = ""):
+    """
+    Class decorator (optionally parameterized) that enables a class to survive reloads by storing a shared instance
+    on the module object, keyed by `reload_key` (or `_SHARED_<ClassName>`).
+
+    Usage:
+        @survives_reload
+        class MyClass: ...
+
+        @survives_reload()
+        class MyClass: ...
+
+        BLAH = survives_reload(MyClass, reload_key="FOO")
+    """
+    def decorator(inner_cls):
+        _reload_key = reload_key or f"_SHARED_{inner_cls.__name__}"
+        _module = sys.modules[inner_cls.__module__]
+
+        @functools.wraps(inner_cls)
+        def get_shared_instance(*args, **kwargs):
+            existing = getattr(_module, _reload_key, None)
+            if existing is not None:
+                return existing
+            inst = inner_cls.__new__(inner_cls, *args, **kwargs)
+            inner_cls.__init__(inst, *args, **kwargs)
+            setattr(_module, _reload_key, inst)
+            return inst
+
+        return get_shared_instance
+
+    # Handle all three cases:
+    # 1. @survives_reload
+    # 2. @survives_reload()
+    # 3. survives_reload(SomeClass, reload_key=...)
+    if cls is not None and callable(cls):
+        return decorator(cls)
+    return decorator
+
+
+class CombineMeta:
+    def __prepare__(
+        self,
+        name: str,
+        bases: tuple[type, ...],
+        **kwargs: Any
+    ) -> MutableMapping[str, Any]:
+        namespace: MutableMapping[str, Any] = {}
+        for metaclass in self._get_most_derived_metaclasses(bases):
+            ns = metaclass.__prepare__(name, bases, **kwargs)
+            if type(ns) in (dict, type(namespace)):
+                namespace.update(ns)
+            else:
+                if type(namespace) is not dict:
+                    raise TypeError(
+                        "metaclass conflict: " "multiple custom namespaces defined."
+                    )
+                ns.update(namespace)
+                namespace = ns
+        return namespace
+
+    def __call__(
+        self,
+        name: str,
+        bases: tuple[type, ...],
+        namespace: MutableMapping[str, Any],
+        **kwargs: Any
+    ) -> type:
+        metaclasses = self._get_most_derived_metaclasses(bases)
+        if len(metaclasses) > 1:
+            merged_name = "__".join(meta.__name__ for meta in metaclasses)
+            ns = self.__prepare__(merged_name, tuple(metaclasses))
+            metaclass = self(merged_name, tuple(metaclasses), ns, **kwargs)
+        else:
+            (metaclass,) = metaclasses or (type,)
+        return metaclass(name, bases, dict(namespace), **kwargs)
+
+    @staticmethod
+    def _get_most_derived_metaclasses(
+        bases: tuple[type, ...]
+    ) -> list[type]:
+        metaclasses: list[type] = []
+        for metaclass in map(type, bases):
+            if metaclass is not type:
+                metaclasses = [
+                    other for other in metaclasses if not issubclass(metaclass, other)
+                ]
+                if not any(issubclass(other, metaclass) for other in metaclasses):
+                    metaclasses.append(metaclass)
+        return metaclasses
+
+
+combine_meta = CombineMeta()
+
 def async_await(
     fn: Callable[..., Coroutine[Any, Any, T]],
 ) -> Callable[..., Generator[Any, None, T]]:
diff --git a/src/d810/stats.py b/src/d810/stats.py
new file mode 100644
index 0000000..a46515e
--- /dev/null
+++ b/src/d810/stats.py
@@ -0,0 +1,89 @@
+from __future__ import annotations
+
+import dataclasses
+from collections import defaultdict
+from typing import Dict, List
+
+from d810.conf.loggers import getLogger
+
+logger = getLogger("D810")
+
+
+@dataclasses.dataclass
+class OptimizationStatistics:
+    """Centralized statistics for optimizers & rules.
+
+    Tracks usage across instruction optimizers, individual instruction rules,
+    and block (CFG) rules. Provides reset/report and query helpers.
+    """
+
+    # instruction optimizer usage (e.g., PatternOptimizer, ChainOptimizer...)
+    instruction_optimizer_usage: Dict[str, int] = dataclasses.field(
+        default_factory=lambda: defaultdict(int)
+    )
+
+    # instruction rule usage by name
+    instruction_rule_usage: Dict[str, int] = dataclasses.field(
+        default_factory=lambda: defaultdict(int)
+    )
+
+    # block/CFG rule usage; we store list of patch counts per use
+    cfg_rule_usages: Dict[str, List[int]] = dataclasses.field(
+        default_factory=lambda: defaultdict(list)
+    )
+
+    def reset(self) -> None:
+        self.instruction_optimizer_usage.clear()
+        self.instruction_rule_usage.clear()
+        self.cfg_rule_usages.clear()
+
+    # Recording APIs
+    def record_optimizer_match(self, optimizer_name: str) -> None:
+        self.instruction_optimizer_usage[optimizer_name] += 1
+
+    def record_instruction_rule_match(self, rule_name: str) -> None:
+        self.instruction_rule_usage[rule_name] += 1
+
+    def record_cfg_rule_patches(self, rule_name: str, nb_patches: int) -> None:
+        self.cfg_rule_usages[rule_name].append(nb_patches)
+
+    # Query APIs
+    def get_optimizer_match_count(self, optimizer_name: str) -> int:
+        return int(self.instruction_optimizer_usage.get(optimizer_name, 0))
+
+    def get_instruction_rule_match_count(self, rule_name: str) -> int:
+        return int(self.instruction_rule_usage.get(rule_name, 0))
+
+    def get_cfg_rule_patch_counts(self, rule_name: str) -> List[int]:
+        return list(self.cfg_rule_usages.get(rule_name, []))
+
+    # Reporting APIs
+    def report(self) -> None:
+        # Optimizers
+        for optimizer_name, nb_match in self.instruction_optimizer_usage.items():
+            if nb_match > 0:
+                logger.info(
+                    "Instruction optimizer '%s' has been used %d times",
+                    optimizer_name,
+                    nb_match,
+                )
+
+        # Instruction rules
+        for rule_name, nb_match in self.instruction_rule_usage.items():
+            if nb_match > 0:
+                logger.info(
+                    "Instruction Rule '%s' has been used %d times",
+                    rule_name,
+                    nb_match,
+                )
+
+        # CFG rules
+        for rule_name, patch_list in self.cfg_rule_usages.items():
+            nb_use = len(patch_list)
+            if nb_use > 0:
+                logger.info(
+                    "BlkRule '%s' has been used %d times for a total of %d patches",
+                    rule_name,
+                    nb_use,
+                    sum(patch_list),
+                )
diff --git a/src/d810/ui/ida_ui.py b/src/d810/ui/ida_ui.py
index 1c2753e..31f70d7 100644
--- a/src/d810/ui/ida_ui.py
+++ b/src/d810/ui/ida_ui.py
@@ -650,12 +650,12 @@ class D810ConfigForm_t(ida_kernwin.PluginForm):
         btn_split.addWidget(self.btn_stop)
 
         # --- Profiling buttons ---
-        self.btn_start_profiling = QtWidgets.QPushButton("Start Profiling")
-        self.btn_start_profiling.clicked.connect(self._start_profiling)
+        self.btn_start_profiling = QtWidgets.QPushButton("Enable Profiling")
+        self.btn_start_profiling.clicked.connect(self._enable_profiling)
         btn_split.addWidget(self.btn_start_profiling)
 
-        self.btn_stop_profiling = QtWidgets.QPushButton("Stop Profiling")
-        self.btn_stop_profiling.clicked.connect(self._stop_profiling)
+        self.btn_stop_profiling = QtWidgets.QPushButton("Disable Profiling")
+        self.btn_stop_profiling.clicked.connect(self._disable_profiling)
         btn_split.addWidget(self.btn_stop_profiling)
 
         if TestRunnerForm is not None:
@@ -877,18 +877,18 @@ class D810ConfigForm_t(ida_kernwin.PluginForm):
         )
         return
 
-    def _start_profiling(self):
-        logger.debug("Calling _start_profiling")
-        if hasattr(self.state, "manager") and self.state.manager:
-            self.state.manager.start_profiling()
-            logger.info("Profiling started.")
+    def _enable_profiling(self):
+        logger.debug("Calling _enable_profiling")
+        if self.state.manager:
+            self.state.manager.enable_profiling()
+            logger.info("Profiling enabled.")
         else:
-            logger.warning("D810 manager not initialized; cannot start profiling.")
+            logger.warning("D810 manager not initialized; cannot enable profiling.")
 
-    def _stop_profiling(self):
-        logger.debug("Calling _stop_profiling")
-        if hasattr(self.state, "manager") and self.state.manager:
-            output_path = self.state.manager.stop_profiling()
+    def _disable_profiling(self):
+        logger.debug("Calling _disable_profiling")
+        if self.state.manager:
+            output_path = self.state.manager.disable_profiling()
             if output_path:
                 logger.info("Profiling stopped. Report saved to: %s", output_path)
                 QtWidgets.QMessageBox.information(
@@ -897,7 +897,7 @@ class D810ConfigForm_t(ida_kernwin.PluginForm):
                     f"Profiling report saved to:\n{str(output_path)}",
                 )
         else:
-            logger.warning("D810 manager not initialized; cannot stop profiling.")
+            logger.warning("D810 manager not initialized; cannot disable profiling.")
 
     def _show_test_runner(self):
         if self.test_runner is None:
diff --git a/tests/system/optimizers/microcode/flow/__init__.py b/tests/system/optimizers/microcode/flow/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/tests/system/test_libdeobfuscated.py b/tests/system/test_libdeobfuscated.py
index 6f3db38..cce5a9c 100644
--- a/tests/system/test_libdeobfuscated.py
+++ b/tests/system/test_libdeobfuscated.py
@@ -60,7 +60,7 @@ class TestLibDeobfuscated(unittest.TestCase):
             }"""
         )
 
-        deobfuscated = textwrap.dedent(
+        deobfuscated_without_constant_propagation = textwrap.dedent(
             """\
             __int64 __fastcall test_cst_simplification(int *a1)
             {
@@ -74,6 +74,19 @@ class TestLibDeobfuscated(unittest.TestCase):
                 return (unsigned int)(v2 - 0x86D41AD);
             }"""
         )
+        deobfuscated_with_constant_propagation = textwrap.dedent(
+            """\
+            __int64 __fastcall test_cst_simplification(int *a1)
+            {
+                // [COLLAPSED LOCAL DECLARATIONS. PRESS NUMPAD "+" TO EXPAND]
+
+                a1[1] = 0x222E69C0;
+                a1[2] = 0xD32B5931;
+                a1[3] = 0x238FB62;
+                a1[4] = 0xA29;
+                return 0xF792C87CLL;
+            }"""
+        )
 
     def test_deobfuscate_opaque_predicate(self):
         obfuscated = textwrap.dedent(
diff --git a/tests/unit/test_cache.py b/tests/unit/test_cache.py
index be9c4a1..fa2aa36 100644
--- a/tests/unit/test_cache.py
+++ b/tests/unit/test_cache.py
@@ -10,6 +10,8 @@ from .tutils import load_conf_classes
 with load_conf_classes():
     from d810.cache import LFU, CacheImpl, OverweightError, cache, lru_cache
 
+from d810.registry import survives_reload
+
 
 class FixedClock:
     def __init__(self):
@@ -32,16 +34,16 @@ class TestCacheImpl(unittest.TestCase):
 
     def test_stats_hits_misses(self):
         c = CacheImpl(max_size=10, clock=FixedClock())
-        stats = c.stats
+        stats = c.stats()
         self.assertEqual(stats.hits, 0)
         self.assertEqual(stats.misses, 0)
         with self.assertRaises(KeyError):
             _ = c["missing"]
-        stats = c.stats
+        stats = c.stats()
         self.assertEqual(stats.misses, 1)
         c["x"] = 99
         _ = c["x"]
-        stats = c.stats
+        stats = c.stats()
         self.assertEqual(stats.hits, 1)
 
     def test_lru_eviction(self):
@@ -236,7 +238,7 @@ class TestCacheDecorator(unittest.TestCase):
         self.assertEqual(f(10), 20)
         self.assertEqual(calls, [10])
         # stats via wrapper.cache
-        stats = f.cache.stats
+        stats = f.cache.stats()
         self.assertEqual(stats.misses, 1)
         self.assertEqual(stats.hits, 1)
 
@@ -278,7 +280,7 @@ class TestCPythonCacheBehavior(unittest.TestCase):
         for i in range(5):
             f(i)
         # Unlimited caching: cache grows without eviction ([stackoverflow.com](https://stackoverflow.com/questions/78875431/how-to-disable-functools-lru-cache-when-developing-locally?utm_source=chatgpt.com))
-        self.assertEqual(f.cache.stats.size, 5)
+        self.assertEqual(f.cache.stats().size, 5)
 
     def test_cache_info_and_clear(self):
         @cache
@@ -287,12 +289,12 @@ class TestCPythonCacheBehavior(unittest.TestCase):
 
         f(1)
         f(1)
-        stats = f.cache.stats
+        stats = f.cache.stats()
         self.assertEqual(stats.hits, 1)
         self.assertEqual(stats.misses, 1)
         # clear removes entries but does not reset stats ([docs.python.org](https://docs.python.org/3/library/functools.html?utm_source=chatgpt.com))
         f.cache.clear()
-        self.assertEqual(f.cache.stats.size, 0)
+        self.assertEqual(f.cache.stats().size, 0)
 
     def test_unhashable_argument(self):
         @lru_cache(max_size=10)
@@ -310,7 +312,7 @@ class TestCPythonCacheBehavior(unittest.TestCase):
 
         for i in range(3):
             f(i)
-        self.assertEqual(f.cache.stats.size, 3)
+        self.assertEqual(f.cache.stats().size, 3)
 
     def test_wrapper_attributes(self):
         @cache
@@ -321,12 +323,15 @@ class TestCPythonCacheBehavior(unittest.TestCase):
         self.assertTrue(hasattr(f, "__wrapped__"))
 
 
+SURVIVES_RELOAD_CACHE = survives_reload(CacheImpl)
+
+
 class TestSurviveReload(unittest.TestCase):
     def test_same_key_returns_singleton(self):
         from d810.cache import CacheImpl
 
-        c1 = CacheImpl(max_size=5, survive_reload=True, reload_key="FOO")
-        c2 = CacheImpl(max_size=10, survive_reload=True, reload_key="FOO")
+        c1 = survives_reload(CacheImpl, reload_key="FOO")(max_size=5)
+        c2 = survives_reload(CacheImpl, reload_key="FOO")(max_size=10)
         self.assertIs(c1, c2)
         # initial config sticks:
         self.assertEqual(c1._max_size, 5)
@@ -334,18 +339,18 @@ class TestSurviveReload(unittest.TestCase):
     def test_different_keys_are_distinct(self):
         from d810.cache import CacheImpl
 
-        c1 = CacheImpl(max_size=5, survive_reload=True, reload_key="A")
-        c2 = CacheImpl(max_size=5, survive_reload=True, reload_key="B")
+        c1 = survives_reload(CacheImpl, reload_key="A")(max_size=5)
+        c2 = survives_reload(CacheImpl, reload_key="B")(max_size=5)
         self.assertIsNot(c1, c2)
 
     def test_default_shared_key(self):
         from d810.cache import CacheImpl
 
-        c1 = CacheImpl(max_size=3, survive_reload=True)
-        c2 = CacheImpl(max_size=4, survive_reload=True)
+        c1 = survives_reload(CacheImpl)(max_size=3)
+        c2 = survives_reload(CacheImpl)(max_size=4)
         self.assertIs(c1, c2)
         # And a non-surviving instance is new:
-        c3 = CacheImpl(max_size=3, survive_reload=False)
+        c3 = CacheImpl(max_size=3)
         self.assertIsNot(c1, c3)
 
     def test_no_survive_reload_creates_new_each_time(self):
@@ -357,13 +362,13 @@ class TestSurviveReload(unittest.TestCase):
 
     def test_survive_reload_meta(self):
         # load twice to simulate a reload()
-        mod = importlib.reload(sys.modules["d810.cache"])
-        c1 = mod.CacheImpl(max_size=3, survive_reload=True, reload_key="X")
+        mod = importlib.reload(sys.modules[__name__])
+        c1 = mod.SURVIVES_RELOAD_CACHE(max_size=3)
         # simulate reload by reimporting
-        importlib.reload(sys.modules["d810.cache"])
-        c2 = mod.CacheImpl(max_size=99, survive_reload=True, reload_key="X")
+        importlib.reload(sys.modules[__name__])
+        c2 = mod.SURVIVES_RELOAD_CACHE(max_size=99)
         self.assertIs(c1, c2)
-        self.assertEqual(c1._max_size, 3)  # original config sticks
+        self.assertEqual(c2._max_size, 3)  # original config sticks
 
     def test_no_survive_different(self):
         from d810.cache import CacheImpl
diff --git a/tests/unit/test_utils.py b/tests/unit/test_utils.py
new file mode 100644
index 0000000..60fc1b2
--- /dev/null
+++ b/tests/unit/test_utils.py
@@ -0,0 +1,179 @@
+import unittest
+from unittest.mock import patch, MagicMock
+
+from .tutils import load_conf_classes
+
+# Mock MSB_TABLE to avoid importing ida_hexrays
+MSB_TABLE_MOCK = {
+    1: 0x80,
+    2: 0x8000,
+    4: 0x80000000,
+    8: 0x8000000000000000,
+    16: 0x80000000000000000000000000000000,
+}
+
+# Mock the entire hexrays_helpers module
+hexrays_helpers_mock = MagicMock()
+hexrays_helpers_mock.MSB_TABLE = MSB_TABLE_MOCK
+
+# Use load_conf_classes and patch the module
+with load_conf_classes():
+    with patch.dict('sys.modules', {'d810.hexrays.hexrays_helpers': hexrays_helpers_mock}):
+        from d810.expr.utils import signed_to_unsigned, unsigned_to_signed, get_parity_flag
+
+
+class TestUtils(unittest.TestCase):
+    """Test utility functions for type conversions and bit operations."""
+
+    def test_signed_to_unsigned_small_sizes(self):
+        """Test signed_to_unsigned for 1, 2, 4, 8 byte sizes."""
+        # Test positive values
+        self.assertEqual(signed_to_unsigned(42, 1), 42)
+        self.assertEqual(signed_to_unsigned(1000, 2), 1000)
+        self.assertEqual(signed_to_unsigned(123456, 4), 123456)
+        self.assertEqual(signed_to_unsigned(9876543210, 8), 9876543210)
+
+        # Test negative values (should wrap around)
+        self.assertEqual(signed_to_unsigned(-1, 1), 255)  # 0xFF
+        self.assertEqual(signed_to_unsigned(-1, 2), 65535)  # 0xFFFF
+        self.assertEqual(signed_to_unsigned(-1, 4), 4294967295)  # 0xFFFFFFFF
+        self.assertEqual(signed_to_unsigned(-1, 8), 18446744073709551615)  # 0xFFFFFFFFFFFFFFFF
+
+        # Test edge cases
+        self.assertEqual(signed_to_unsigned(0, 1), 0)
+        self.assertEqual(signed_to_unsigned(127, 1), 127)  # Max positive for int8
+        self.assertEqual(signed_to_unsigned(-128, 1), 128)  # Min negative for int8
+
+    def test_unsigned_to_signed_small_sizes(self):
+        """Test unsigned_to_signed for 1, 2, 4, 8 byte sizes."""
+        # Test positive values
+        self.assertEqual(unsigned_to_signed(42, 1), 42)
+        self.assertEqual(unsigned_to_signed(1000, 2), 1000)
+        self.assertEqual(unsigned_to_signed(123456, 4), 123456)
+        self.assertEqual(unsigned_to_signed(9876543210, 8), 9876543210)
+
+        # Test values that should be interpreted as negative
+        self.assertEqual(unsigned_to_signed(255, 1), -1)  # 0xFF as signed int8
+        self.assertEqual(unsigned_to_signed(65535, 2), -1)  # 0xFFFF as signed int16
+        self.assertEqual(unsigned_to_signed(4294967295, 4), -1)  # 0xFFFFFFFF as signed int32
+        self.assertEqual(unsigned_to_signed(18446744073709551615, 8), -1)  # 0xFFFFFFFFFFFFFFFF as signed int64
+
+        # Test edge cases
+        self.assertEqual(unsigned_to_signed(0, 1), 0)
+        self.assertEqual(unsigned_to_signed(127, 1), 127)  # Max positive for int8
+        self.assertEqual(unsigned_to_signed(128, 1), -128)  # Min negative for int8
+
+    def test_signed_to_unsigned_16_bytes(self):
+        """Test signed_to_unsigned for 16-byte (128-bit) values."""
+        # Test positive values
+        test_val = 123456789012345678901234567890123456789
+        self.assertEqual(signed_to_unsigned(test_val, 16), test_val)
+
+        # Test negative values (should be treated as unsigned 128-bit)
+        negative_val = -1
+        expected = (1 << 128) - 1  # All bits set for 128-bit unsigned
+        self.assertEqual(signed_to_unsigned(negative_val, 16), expected)
+
+        # Test zero
+        self.assertEqual(signed_to_unsigned(0, 16), 0)
+
+        # Test large positive value
+        large_val = (1 << 127) - 1  # Max positive for signed 128-bit
+        self.assertEqual(signed_to_unsigned(large_val, 16), large_val)
+
+    def test_unsigned_to_signed_16_bytes(self):
+        """Test unsigned_to_signed for 16-byte (128-bit) values."""
+        # Test positive values
+        test_val = 123456789012345678901234567890123456789
+        self.assertEqual(unsigned_to_signed(test_val, 16), test_val)
+
+        # Test values that should be interpreted as negative (MSB set)
+        msb_set = 1 << 127  # Only MSB set in 128-bit value
+        expected = msb_set - (1 << 128)  # Sign extension
+        self.assertEqual(unsigned_to_signed(msb_set, 16), expected)
+
+        # Test all bits set (should be -1)
+        all_bits_set = (1 << 128) - 1
+        self.assertEqual(unsigned_to_signed(all_bits_set, 16), -1)
+
+        # Test zero
+        self.assertEqual(unsigned_to_signed(0, 16), 0)
+
+    def test_roundtrip_conversion(self):
+        """Test that signed->unsigned->signed and unsigned->signed->unsigned roundtrips work."""
+        # Test signed -> unsigned -> signed roundtrips
+        signed_test_values = [0, 1, -1, 42, -42, 127, -128, -1, 42, -100]
+
+        for val in signed_test_values:
+            for size in [1, 2, 4, 8]:
+                # signed -> unsigned -> signed
+                unsigned = signed_to_unsigned(val, size)
+                back_to_signed = unsigned_to_signed(unsigned, size)
+                self.assertEqual(back_to_signed, val,
+                               f"Signed roundtrip failed for {val} at size {size}: {val} -> {unsigned} -> {back_to_signed}")
+
+        # Test unsigned -> signed -> unsigned roundtrips
+        unsigned_test_cases = [
+            (0, [1, 2, 4, 8]),
+            (1, [1, 2, 4, 8]),
+            (42, [1, 2, 4, 8]),
+            (127, [1, 2, 4, 8]),
+            (255, [1, 2, 4, 8]),  # Max for 1 byte
+            (65535, [2, 4, 8]),   # Max for 2 bytes
+            (4294967295, [4, 8]), # Max for 4 bytes
+        ]
+
+        for val, sizes in unsigned_test_cases:
+            for size in sizes:
+                # unsigned -> signed -> unsigned
+                signed = unsigned_to_signed(val, size)
+                back_to_unsigned = signed_to_unsigned(signed, size)
+                self.assertEqual(back_to_unsigned, val,
+                               f"Unsigned roundtrip failed for {val} at size {size}: {val} -> {signed} -> {back_to_unsigned}")
+
+    def test_get_parity_flag(self):
+        """Test get_parity_flag function."""
+        # The function returns 1 when number of 1 bits is even, 0 when odd
+        # Test cases with even number of 1s (should return 1)
+        self.assertEqual(get_parity_flag(1, 2, 4), 1)  # 1-2 = -1, 32 ones (even)
+        self.assertEqual(get_parity_flag(4, 4, 4), 1)  # 4-4 = 0, 0 ones (even)
+        self.assertEqual(get_parity_flag(3, 0, 1), 1)  # 3 in 8 bits: 11 (2 ones, even)
+
+        # Test cases with odd number of 1s (should return 0)
+        self.assertEqual(get_parity_flag(1, 0, 4), 0)  # 1-0 = 1, 1 one (odd)
+        self.assertEqual(get_parity_flag(7, 2, 4), 1)  # 7-2 = 5, binary: 101 (2 ones, even)
+        self.assertEqual(get_parity_flag(1, 0, 1), 0)  # 1 in 8 bits: 1 one (odd)
+
+        # Test 16-byte case
+        self.assertEqual(get_parity_flag(1, 0, 16), 0)  # 1 in 128 bits: odd parity
+        self.assertEqual(get_parity_flag(3, 0, 16), 1)  # 3 in 128 bits: even parity
+
+    def test_large_values_16_bytes(self):
+        """Test handling of very large values for 16-byte operations."""
+        # Test maximum 128-bit unsigned value
+        max_u128 = (1 << 128) - 1
+        self.assertEqual(signed_to_unsigned(max_u128, 16), max_u128)
+        self.assertEqual(unsigned_to_signed(max_u128, 16), -1)  # All bits set = -1
+
+        # Test maximum 128-bit signed value
+        max_s128 = (1 << 127) - 1
+        self.assertEqual(unsigned_to_signed(max_s128, 16), max_s128)
+
+        # Test minimum 128-bit signed value
+        min_s128 = -(1 << 127)
+        self.assertEqual(signed_to_unsigned(min_s128, 16), 1 << 127)
+
+    def test_invalid_sizes(self):
+        """Test behavior with unsupported byte sizes."""
+        with self.assertRaises(KeyError):
+            signed_to_unsigned(42, 3)  # 3 bytes not supported
+
+        with self.assertRaises(KeyError):
+            unsigned_to_signed(42, 32)  # 32 bytes not supported
+
+        with self.assertRaises(KeyError):
+            get_parity_flag(1, 2, 64)  # 64 bytes not supported
+
+
+if __name__ == "__main__":
+    unittest.main()
